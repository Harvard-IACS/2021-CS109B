{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title\n",
    "\n",
    "Generative Adversarial Networks\n",
    "- Generate 1D Gaussian Distribution from Uniform Noise \n",
    "\n",
    "## Description\n",
    "\n",
    "In this exercise, we are going to generate 1-D Gaussian distribution from a n-D uniform distribution. This is a toy exercise in order to understand the ability of GANs (generators) to generate arbitrary distributions from random noise.\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#train_on_batch\" target=\"_blank\">tf.keras.Model.train_on_batch()</a> tf.keras.Model.train_on_batch()\n",
    "\n",
    "Runs a single gradient update on a single batch of data.\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.ones.html\" target=\"_blank\">np.ones</a> np.ones\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\" target=\"_blank\">np.zeros</a> np.zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ðŸŽ¨ Generative Models ðŸ–¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 1-D Gaussian distribution from a n-D uniform distribution\n",
    "  \n",
    "  <hr>\n",
    "In this exercise, we are going to generate 1-D Gaussian distribution from a n-D uniform distribution.  This is a toy exercise in order to understand the ability of GANs (generators) to generate arbitrary distributions from random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "%matplotlib inline\n",
    "\n",
    "tf.random.set_seed(109)\n",
    "np.random.seed(109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our real dataset (samples from stanard normal distribution )\n",
    "def generate_data(n_samples = 10000,n_dim=1):\n",
    "    np.random.seed(109)\n",
    "    return np.random.randn(n_samples, n_dim) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general function to define feedforward architecture.\n",
    "def make_model(input_dim, \n",
    "               output_dim, \n",
    "               hidden_dim=64,\n",
    "               n_layers = 1,\n",
    "               activation='tanh',\n",
    "               optimizer='adam',\n",
    "               loss = 'binary_crossentropy'):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_dim,input_dim=input_dim,activation=activation))\n",
    "    \n",
    "    for _ in range(n_layers-1):\n",
    "        model.add(Dense(hidden_dim),activation=activation)\n",
    "    model.add(Dense(output_dim))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "NOISE_DIM = 100\n",
    "DATA_DIM = 1   #this is equivalent to image size e.g. 32*32*3\n",
    "G_LAYERS = 1\n",
    "D_LAYERS = 1\n",
    "generator = make_model(NOISE_DIM, DATA_DIM, n_layers=G_LAYERS)  \n",
    "discriminator = make_model(DATA_DIM, 1, n_layers= D_LAYERS, activation='sigmoid')  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(random_dim, optimizer = 'adam'):\n",
    "    \n",
    "    #This ensures that when we combine our networks we only train the Generator.\n",
    "    #While generator training we do not want discriminator weights to be adjusted. \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    #this model will be used to train generator\n",
    "    gan = tf.keras.Model(inputs = gan_input,outputs=gan_output)\n",
    "    gan.compile( loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(epochs=15,batch_size=128):\n",
    "    #Loads the real data \n",
    "    x_train = generate_data(n_samples=12800,n_dim=DATA_DIM)  \n",
    "    gan = get_gan_network(NOISE_DIM, 'adam') # Get GAN model \n",
    "\n",
    "    for e in range(1,epochs+1):   \n",
    "        np.random.seed(109 + e)\n",
    "        # noise from a uniform distribution \n",
    "        noise = np.random.rand(batch_size,NOISE_DIM) \n",
    "        # generate a batch of fake data/images)\n",
    "        generated_values = generator.predict(noise)\n",
    "        \n",
    "        #Gets a batch of real data (images)\n",
    "        true_batch = x_train[np.random.choice(x_train.shape[0], batch_size, replace=False), :]  \n",
    "\n",
    "        # Train discriminator on real data, use train_on_batch\n",
    "        # real data has label of all 1s\n",
    "        disc_history_true =  ___ \n",
    "        # Train discriminator on generated values, use train_on_batch\n",
    "        # fake data has label of all 0s\n",
    "        disc_history_noise = ___\n",
    "\n",
    "        # Train generator/GAN\n",
    "        noise = np.random.rand(batch_size,NOISE_DIM)\n",
    "        y_gen = np.ones(batch_size)      \n",
    "        # Train gan with noise, with label all 1s. \n",
    "        gan_loss = ___\n",
    "\n",
    "        \n",
    "    return generator, discriminator, gan_loss, disc_history_true, disc_history_noise\n",
    "generator, discriminator, gan_loss, disc_history_true, disc_history_noise = train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_check) ###\n",
    "print( gan_loss, disc_history_true, disc_history_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order moment -  True :  0.0 , GAN : [0.]\n",
      "2nd order moment -  True :  1.0016611800894593 , GAN : [0.11815298]\n",
      "3rd order moment -  True :  0.003238467835095741 , GAN : [-0.00209988]\n",
      "4th order moment -  True :  3.0480390897072773 , GAN : [0.04024393]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQkUlEQVR4nO3dbYxcV33H8e+PENIKkJrIm2AcU6fUrUiQCGjlpopUpYUSK63kIBFkXlCrSmVaJSpIvKhDpUJfWApVAbUVoTIlwpWAYAnSWBAeQgRCSJCwjkKIEyJc4iaLrXh5KA9qldbm3xd7Qwd71zO7M7Ozc/b7kVZz58y9s//ra//2+NwzZ1NVSJLa8rxJFyBJGj3DXZIaZLhLUoMMd0lqkOEuSQ16/qQLANi0aVNt27Zt0mVI0lQ5cuTI96tqZqnX1kW4b9u2jbm5uUmXIUlTJcl/LPeawzKS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgdfEJVWnabdv3mV9sH7/9jyZYibTIcJfGyNDXpDgsI0kN6hvuSX4lyYNJvpnkaJK/7dovSXJfku90jxf3HHNbkmNJnkhy/ThPQJJ0rkF67s8Cf1BVrwKuBnYmuQbYB9xfVduB+7vnJLkS2A1cBewE7khywRhqlyQto2+416KfdU8v7L4K2AUc7NoPAjd227uAu6rq2ap6EjgG7Bhl0ZKk8xvohmrX8z4C/Cbwgap6IMllVXUSoKpOJrm0230L8PWew+e7trPfcy+wF+BlL3vZ6s9AWiFvcmojGOiGalWdqaqrgcuBHUleeZ7ds9RbLPGeB6pqtqpmZ2aW/EUikqRVWtFUyKr6zyRfZnEs/Zkkm7te+2bgVLfbPLC157DLgROjKFaaNHv9mhZ9wz3JDPC/XbD/KvA64D3AYWAPcHv3eE93yGHgY0neB7wU2A48OIbapTXRG+ijeh9/MGjcBum5bwYOduPuzwMOVdWnk3wNOJTkZuAp4CaAqjqa5BDwGHAauKWqzoynfGn9GdUPA2kYfcO9qh4BXr1E+w+A1y5zzH5g/9DVSZJWxeUHpI7DJmqJyw9IUoPsuWtDm9T4uP9L0LjZc5ekBhnuktQgw12SGuSYu7QE56pr2tlzl6QG2XOXVsnevdYze+6S1CDDXZIaZLhLUoMcc1eTzh4P91Og2mjsuUtSgwx3SWqQ4S5JDXLMXRuCc9K10dhzl6QGGe6S1CDDXZIaZLhLUoO8oSpNmL9yT+Ngz12SGmS4S1KDHJaR1hGHaDQqfXvuSbYm+VKSx5McTfK2rv3dSb6X5OHu64aeY25LcizJE0muH+cJSJLONUjP/TTwjqp6KMmLgSNJ7utee39V/X3vzkmuBHYDVwEvBb6Y5Leq6swoC5ckLa9vz72qTlbVQ932T4HHgS3nOWQXcFdVPVtVTwLHgB2jKFaSNJgV3VBNsg14NfBA13RrkkeS3Jnk4q5tC/B0z2HzLPHDIMneJHNJ5hYWFlZeuSRpWQOHe5IXAZ8E3l5VPwE+CLwcuBo4Cbz3uV2XOLzOaag6UFWzVTU7MzOz0rolSecx0GyZJBeyGOwfrapPAVTVMz2vfwj4dPd0Htjac/jlwImRVCudhys/Sv9vkNkyAT4MPF5V7+tp39yz2xuAR7vtw8DuJBcluQLYDjw4upIlSf0M0nO/FngL8K0kD3dt7wTenORqFodcjgNvBaiqo0kOAY+xONPmFmfKSNLa6hvuVfVVlh5Hv/c8x+wH9g9RlyRpCH5CVVPNcXZpaa4tI0kNMtwlqUGGuyQ1yHCXpAYZ7pLUIGfLSOuUa7trGPbcJalBhrskNchwl6QGGe6S1CDDXZIa5GwZTR3Xk5H6s+cuSQ0y3CWpQYa7JDXIcJekBnlDVZoyLkugQRju0hRwhpBWymEZSWqQPXdpijlEo+XYc5ekBhnuktQgw12SGtQ33JNsTfKlJI8nOZrkbV37JUnuS/Kd7vHinmNuS3IsyRNJrh/nCUiSzjVIz/008I6qegVwDXBLkiuBfcD9VbUduL97TvfabuAqYCdwR5ILxlG8JGlpfcO9qk5W1UPd9k+Bx4EtwC7gYLfbQeDGbnsXcFdVPVtVTwLHgB0jrluSdB4rGnNPsg14NfAAcFlVnYTFHwDApd1uW4Cnew6b79rOfq+9SeaSzC0sLKyidEnScgYO9yQvAj4JvL2qfnK+XZdoq3Maqg5U1WxVzc7MzAxahiRpAAOFe5ILWQz2j1bVp7rmZ5Js7l7fDJzq2ueBrT2HXw6cGE25kqRBDDJbJsCHgcer6n09Lx0G9nTbe4B7etp3J7koyRXAduDB0ZUsSepnkOUHrgXeAnwrycNd2zuB24FDSW4GngJuAqiqo0kOAY+xONPmlqo6M+rCtbG4cJa0Mn3Dvaq+ytLj6ACvXeaY/cD+IeqSJA3BhcOkRriImHq5/IAkNchwl6QGOSwjNcghGhnuWlcMJWk0HJaRpAYZ7pLUIMNdkhpkuEtSg7yhqnXLJQek1TPcNXGG+Hg5A2ljclhGkhpkuEtSgwx3SWqQ4S5JDfKGqtaMN/aktWPPXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgvuGe5M4kp5I82tP27iTfS/Jw93VDz2u3JTmW5Ikk14+rcEnS8gbpuX8E2LlE+/ur6uru616AJFcCu4GrumPuSHLBqIqVJA2m7/IDVfWVJNsGfL9dwF1V9SzwZJJjwA7ga6svUS1yDffJOPvP3WUg2jXMmPutSR7phm0u7tq2AE/37DPftUmS1tBqw/2DwMuBq4GTwHu79iyxby31Bkn2JplLMrewsLDKMiRJS1lVuFfVM1V1pqp+DnyIxaEXWOypb+3Z9XLgxDLvcaCqZqtqdmZmZjVlSJKWsapwT7K55+kbgOdm0hwGdie5KMkVwHbgweFKlCStVN8bqkk+DlwHbEoyD7wLuC7J1SwOuRwH3gpQVUeTHAIeA04Dt1TVmbFULkla1iCzZd68RPOHz7P/fmD/MEVJkobjJ1QlqUH+mj1JgL8GsTX23CWpQfbcpQ3MTwq3y567JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNcp67Rs5POkqTZ89dkhpkuEtSgwx3SWqQY+6SzuF9k+lnz12SGmTPXSPh6oLS+mK4a9UMdGn9clhGkhpkuEtSgwx3SWqQ4S5JDfKGqqTzcs77dLLnLkkN6ttzT3In8MfAqap6Zdd2CfAJYBtwHHhTVf2oe+024GbgDPCXVfX5sVSuqeB0SWkyBum5fwTYeVbbPuD+qtoO3N89J8mVwG7gqu6YO5JcMLJqJUkD6RvuVfUV4IdnNe8CDnbbB4Ebe9rvqqpnq+pJ4BiwYzSlSpIGtdox98uq6iRA93hp174FeLpnv/mu7RxJ9iaZSzK3sLCwyjIkSUsZ9Q3VLNFWS+1YVQeqaraqZmdmZkZchiRtbKsN92eSbAboHk917fPA1p79LgdOrL48SdJqrDbcDwN7uu09wD097buTXJTkCmA78OBwJUqSVmqQqZAfB64DNiWZB94F3A4cSnIz8BRwE0BVHU1yCHgMOA3cUlVnxlS7JGkZfcO9qt68zEuvXWb//cD+YYqSJA3HT6hKUoMMd0lqkOEuSQ0y3CWpQYa7JDXI9dy1Iq7yKE0He+6S1CDDXZIa5LCMpIH5K/emhz13SWqQ4S5JDXJYRn05Q0b9OFyz/thzl6QGGe6S1CDDXZIa5Ji7pFXxXsz6Zs9dkhpkuEtSgwx3SWqQ4S5JDTLcJalBzpbRkpwJIU03w12/YKBL7XBYRpIaNFTPPclx4KfAGeB0Vc0muQT4BLANOA68qap+NFyZkqaFi4itD6MYlvn9qvp+z/N9wP1VdXuSfd3zvxrB99EYOBQjtWkcY+67gOu67YPAlzHcpQ3JXvzkDDvmXsAXkhxJsrdru6yqTgJ0j5cudWCSvUnmkswtLCwMWYYkqdewPfdrq+pEkkuB+5J8e9ADq+oAcABgdna2hqxDktRjqJ57VZ3oHk8BdwM7gGeSbAboHk8NW6QkaWVWHe5JXpjkxc9tA68HHgUOA3u63fYA9wxbpCRpZYYZlrkMuDvJc+/zsar6XJJvAIeS3Aw8Bdw0fJkaJWfISO1bdbhX1XeBVy3R/gPgtcMUJUkajssPbBD21qWNxeUHJKlBhrskNchwl6QGGe6S1CDDXZIa5GwZSWvCRcTWlj13SWqQPXdJa85e/PgZ7pLWDUN/dAz3hvmpVGnjMtwbY6BLAsNd0oTZIRkPZ8tIUoMMd0lqkOEuSQ0y3CWpQd5QXcec8yst8t/CyhnuU8jZBdoI/Hs+HMN9SvgXXVpkL34whrukqWXQL88bqpLUIHvuE7LcMIu9D0mjYLhLaoJDNL8sVTXpGpidna25ublJlzF23hSV1o8WfgAkOVJVs0u9Nraee5KdwD8AFwD/UlW3j+t7rWcGuqRJGEu4J7kA+ADwh8A88I0kh6vqsXF8v7X879ggY+UGurT+DfrvdFp7+OPque8AjlXVdwGS3AXsAsYS7oNY6Q+AlQa0gS5tXMN0+sb1w2MsY+5J3gjsrKo/656/Bfidqrq1Z5+9wN7u6W8DT4y8kP42Ad+fwPddSxvhHGFjnKfn2I5RneevV9XMUi+Mq+eeJdp+6adIVR0ADozp+w8kydxyNyNasRHOETbGeXqO7ViL8xzXh5jmga09zy8HTozpe0mSzjKucP8GsD3JFUleAOwGDo/pe0mSzjKWYZmqOp3kVuDzLE6FvLOqjo7jew1posNCa2QjnCNsjPP0HNsx9vNcFx9ikiSNlguHSVKDDHdJatCGCvckNyU5muTnSZadhpRkZ5InkhxLsm8taxxWkkuS3JfkO93jxcvsdzzJt5I8nGQqFvbpd12y6B+71x9J8ppJ1DmsAc7zuiQ/7q7dw0n+ZhJ1rlaSO5OcSvLoMq+3ch37ned4r2NVbZgv4BUsfmDqy8DsMvtcAPw78BvAC4BvAldOuvYVnOPfAfu67X3Ae5bZ7ziwadL1ruC8+l4X4Abgsyx+zuIa4IFJ1z2m87wO+PSkax3iHH8PeA3w6DKvT/11HPA8x3odN1TPvaoer6p+n4T9xdIJVfU/wHNLJ0yLXcDBbvsgcOPkShmpQa7LLuBfa9HXgV9LsnmtCx3StP/966uqvgL88Dy7tHAdBznPsdpQ4T6gLcDTPc/nu7ZpcVlVnQToHi9dZr8CvpDkSLcUxHo3yHWZ9msHg5/D7yb5ZpLPJrlqbUpbMy1cx0GN7To298s6knwReMkSL/11Vd0zyFss0bau5oue7xxX8DbXVtWJJJcC9yX5dtfTWK8GuS7r/toNYJBzeIjFNUV+luQG4N+A7eMubA21cB0HMdbr2Fy4V9XrhnyLdb90wvnOMckzSTZX1cnuv7KnlnmPE93jqSR3szgcsJ7DfZDrsu6v3QD6nkNV/aRn+94kdyTZVFWtLLjVwnXsa9zX0WGZc0370gmHgT3d9h7gnP+tJHlhkhc/tw28Hljyjv46Msh1OQz8STfb4hrgx88NUU2RvueZ5CVJ0m3vYPHf8Q/WvNLxaeE69jXu69hcz/18krwB+CdgBvhMkoer6vokL2Xxt0XdUNOzdMJybgcOJbkZeAq4CaD3HIHLgLu7v1fPBz5WVZ+bUL0DWe66JPnz7vV/Bu5lcabFMeC/gD+dVL2rNeB5vhH4iySngf8Gdlc3/WIaJPk4izNFNiWZB94FXAjtXEcY6DzHeh1dfkCSGuSwjCQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDfo/zRbjAA3rh/oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = np.random.rand(10000,NOISE_DIM)\n",
    "generated_values = generator.predict(noise)\n",
    "plt.hist(generated_values,bins=100)\n",
    "\n",
    "true_gaussian = [np.random.randn() for x in range(10000)]\n",
    "\n",
    "print('1st order moment - ', 'True : ', scipy.stats.moment(true_gaussian, 1) , ', GAN :', scipy.stats.moment(generated_values,1))\n",
    "print('2nd order moment - ', 'True : ', scipy.stats.moment(true_gaussian, 2) , ', GAN :', scipy.stats.moment(generated_values,2))\n",
    "print('3rd order moment - ', 'True : ', scipy.stats.moment(true_gaussian, 3) , ', GAN :', scipy.stats.moment(generated_values,3))\n",
    "print('4th order moment - ', 'True : ', scipy.stats.moment(true_gaussian, 4) , ', GAN :', scipy.stats.moment(generated_values,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h3>CONCLUSIONS </h3>     <br/>\n",
    "\n",
    "1. GANs are able to learn a generative model from general noise distributions. <br/>\n",
    "\n",
    "2. Traditional GANs do not learn the higher-order moments well. Possible issues : Number of samples, approximating higher moments is hard. Usually known to under-predict higher order variances. For people interested in learning why, read more about divergence measures between distributions (particularly about Wasserstein etc.)\n",
    "    </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- GANs in Action (Jakub Langr; Vladimir Bok) \n",
    "- https://machinelearningmastery.com/generative-adversarial-network-loss-functions/ \n",
    "- https://arxiv.org/pdf/1406.2661.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
