{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "ðŸ†“ Exercise: Q-Learning using DQN\n",
    "\n",
    "## Description\n",
    "The aim of this exercise is to implement Deep Q Networks for a pre-defined reinforcement learning environment. For this, we will be using a pre-defined environment by OpenAI Gym. We will be using an environment called FrozenLake-v0. This is the same as what was used for the previous session (Refer to it to get information about the environment).\n",
    "\n",
    "<img src=\"../fig/fig.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "## Instructions\n",
    "- Initialize an environment using a pre-defined environment from OpenAI Gym.\n",
    "- Get the number of possible actions in the environment. \n",
    "- Define a simple feed-forward neural network with your choice of hidden layers and nodes.\n",
    "- Define the action sampling policy. We will be using the Epsilon Greedy policy.\n",
    "- Initialize sequential memory to store the data, which is the input to the DQN.\n",
    "- Define the DQN and compile it with Adam optimizer.\n",
    "- Fit and test the DQN model.\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "<a href=\"https://gym.openai.com/docs/#environments\" target=\"_blank\">gym.make(environment-name)</a> Access a pre-defined environment\n",
    "\n",
    "Env.action_space.n : Returns the number of discrete actions\n",
    "\n",
    "Env.observation_space.n : Returns the number of discrete states\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\" target=\"_blank\">Dense()</a> A regular densely-connected NN layer\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\" target=\"_blank\">Flatten()</a> Flattens the input. \n",
    "\n",
    "<a href=\"https://keras-rl.readthedocs.io/en/latest/agents/dqn/\" target=\"_blank\">Adam()</a> Optimizer that implements the Adam algorithm\n",
    "\n",
    "<a href=\"https://keras-rl.readthedocs.io/en/latest/agents/dqn/\" target=\"_blank\">DQNAgent()</a> Initializes the DQN Agent\n",
    "\n",
    "SequentialMemory()\n",
    "\n",
    "Keras-RL provides a class called rl.memory.SequentialMemory that provides a fast and efficient data structure that we can store the agentâ€™s experiences in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once and then comment it to ensure it does not run multiple times\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessay libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an environment using a pre-defined environment from OpenAI Gym \n",
    "# The environment used here is 'FrozenLake-v0'\n",
    "env = ___\n",
    "\n",
    "# Get the number of actions within the environment\n",
    "nb_actions = ___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Feed-Forward Neural Network \n",
    "\n",
    "# Initialize a keras sequential model\n",
    "model = ___\n",
    "\n",
    "# Flatten the input to have an input shape of (1,) + \n",
    "# shape of the environment state space i.e. the observation space\n",
    "model.add(Flatten(___))\n",
    "\n",
    "# Add Dense layers with Relu activation\n",
    "# The number of hidden layers and number of nodes in each layer is your choice\n",
    "\n",
    "___\n",
    "\n",
    "# Add an output layer with number of nodes as the number of actions\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy to sample the actions\n",
    "# We will be using the Epsilon-Greedy algorithm\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# To store our data initialize Sequential Memory with limit=500000 and window_length of 1\n",
    "memory = SequentialMemory(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN AGENT**\n",
    "\n",
    "<img src=\"./images/dqn.png\" alt=\"DQN Agent\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DQNAgent with the neural network model, nb_actions as the number of actions in the environment, \n",
    "# set the memory as the sequential memory defined above, nb_steps_warmup as 100, policy as the epsilon greedy policy defined above\n",
    "# and set the target_model_update as 1e-2\n",
    "dqn = DQNAgent(___)\n",
    "\n",
    "# Compile the DQN with Adam optimizer with learning rate of 1e-3 and metric as mse\n",
    "dqn.compile(___)\n",
    "\n",
    "# Fit the DQN by passing with environment with nb_steps as 5000\n",
    "# You have an option to visualize the output, which is done by implicitly calling the render function of the environment\n",
    "# However, this will slow down the training process and is not recommended for EdStem\n",
    "# To see the complete training details, set verbose as 2\n",
    "dqn.fit(___);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your model by passing the environment and running for 10 episodes\n",
    "dqn.test(___)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
