var tipuesearch = {
    "pages": [{"title": "Calendar", "text": "", "tags": "pages", "url": "pages/calendar.html"}, {
        "title": "FAQ",
        "text": "General Does the individual HW mean I have to submit on my own but can I still work with my HW partner? An individual CS109B HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with partner. You are allowed to use OHs to ask for clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at libraries documentation. Do I have access to the video recorded materials if I am not an Extension School student? Yes. All CS109B students have access to all video captured materials. If you have any issues accessing the video content, please send an email to our helpline with your name and HUID. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109b2021@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, you are welcome to audit this course. Send an email to cs109b2021@gmail.com to request full Canvas access. All auditors must agree to abide by the following rules: All auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Auditors may not attend lectures or section during the live stream. Students are randomly assigned to small groups for coding exercises and mixing auditors and students in this way is not ideal. We are investigating methods that would allow auditors to join, but this is the current policy. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, Jupyter Hub, and office hours. Auditors must have an active HUID number. Quizzes & Exercises When are quizzes and exercises due? Quizzes and exercises are due before the next lecture. I missed a quiz/exercise. Can I make it up? No. We will drop your lowest 25% of quizzes and your lowest 25% of exercises. This policy is to reduce stress and is in place so that missing a quiz or exercise on occasion should not affect your final grade.",
        "tags": "pages",
        "url": "pages/faq.html"
    }, {
        "title": "Modules",
        "text": "Final Project Guidelines A. Ancient Texts B. Astrophysics [Canceled] C. Law Citations D. Design E. Privacy F. Genetic Sequencing G. Greek Pottery H. Public Health",
        "tags": "pages",
        "url": "pages/modules.html"
    }, {
        "title": "Schedule",
        "text": "Date (Mon) Lecture (Mon) Lecture (Wed) Lecture (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 25-Jan Lecture 1: Splines Smoothers and GAMs (part 1) Lecture 2: Splines Smoothers and GAMs (part 2) Lecture 3: Setup and Review of statsmodels 1-Feb Lecture 4: Splines Smoothers and GAMs (part 3) Lecture 5: Unsupervised learning cluster analysis (part 1) Lecture 5_5: Smoothers pyGAM csaps R:HW1 8-Feb Lecture 6: Unsupervised learning cluster analysis (part 2) Lecture 7: Bayesian statistics (part 1) Lecture 8: Clustering in Python (Lab) R:HW2 - D:HW1 15-Feb No Lecture (Holiday) Lecture 9: Bayesian statistics (part 2) Lecture 10: Bayes PyMC3 R:HW3 - D:HW2 22-Feb Lecture 11: Bayesian statistics (part 3) Lecture 12: Bayesian statistics (part 4) Lecture 13: Hierarchical Models (Lab) 1-Mar No Lecture (Wellness Day) Lecture 14: Œ† CNNs basics Lecture 15: ‚ç∫ CNNs Pooling and CNNs Structure R: HW4 - D: HW3 8-Mar Lecture 16: œç Backprop max pooling Receptive Fields and feature map viz Lecture 17: Œª Saliency maps Lecture 18: ùóà State of the art models (SOTA) and Transfer Learning Advanced Section 1: Segmentation Techniques YOLO Unet 15-Mar Lecture 19: œÇ RNNs Lecture 20: Œ† GRUs Lecture 21: ‚ç¥ LSTMs Advanced Section 2: Recurrent Neural Networks and Reservoir Computing R:HW5 - D:HW4 22-Mar Lecture 22: üí¨ Language Modelling NLP 1/4 Lecture 23: üî¢ Language Representations NLP 2/4 Lecture 24: üß† Attention (Transformers I) NLP 3/4 Advanced Section 3: Word Embeddings R:HW6 - D:HW5 29-Mar Lecture 25: ü§ñ (Transformers II) NLP 4/4 No Lecture (Wellness Day) Lecture 26: Autoencoder 5-Apr Lecture 27: Variational Autoencoder 1/2 Lecture 28: Variational Autoencoder 2/2 Lecture 29: GANS 1/2 Advanced Section 4: Inference in NN R:HW7 - D:HW6 12-Apr Lecture 30: GANS 2/2 Lecture 31: Reinforcement Learning - Basics 1 Lecture 32: Reinforcement Learning - Basics 2 Advanced Section 5: GANS 19-Apr Lecture 33: Deep Reinforcement Learning Module: Lecture Domain Module: Problem Background Advanced Section:6 Reinforcement Learning D:HW7 26-Apr 3-May Reading Period 10-May Module: Final Presentations Finals Week",
        "tags": "pages",
        "url": "pages/schedule.html"
    }, {
        "title": "Syllabus",
        "text": "Draft Syllabus Subject to Change Advanced Topics in Data Science (Spring 2021) CS 109b, AC 209b, Stat 121b, or CSCI E-109b Instructors Pavlos Protopapas (SEAS), Mark Glickman (Statistics), & Chris Tanner (SEAS) Lectures: Mon, Wed, & Fri 9‚Äê10:15am Sections: Fri 10:30am (starting 3/5) Advanced Sections: Wed 12pm (starting 3/10) Office Hours: TBD Prerequisites: CS 109a, AC 209a, Stat 121a, or CSCI E-109a or the equivalent. Course description Tentative Course Topics Course Objectives Course Components Lectures In-class Quizzes In-class Exercises Sections Exams Projects Homework Assignments Course Resources Online Materials Recommended Textbooks Getting Help Course Policies and Expectations Grading Collaboration Policy Late or Wrongly Submitted Assignments Re-grade Requests Auditing the Class Academic Integrity Accommodations for students with disabilities Diversity and Inclusion Statement Course Description Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, language models, transformers, autoencoders, and generative models as well as basic Bayesian methods, nonlinear statistical models, and unsupervised learning. The programming language will be Python. Tentative Course Topics Smoothing and Additive Models Unsupervised Learning, Clustering Bayesian Modeling Convolutional Neural Networks Autoencoders Recurrent Neural Networks NLP / Text Analysis Transformers Variational AutoEncoders & Generative Models Generative Adversarial Networks (Deep) Reinforcement Learning Course Objectives Upon successful completion of this course, you should feel comfortable with the material mentioned above, and you will have gained experience working with others on real-world problems. The content knowledge, the project, and teamwork will prepare you for the professional world or further studies. Course Components Lectures, sections, and advanced sections will be live-streamed and can be accessed through the Zoom section on Canvas. Video recordings of the live stream will be made available within 24 hours after the event, and will be accessible from the Lecture Video section on Canvas. Lectures The class meets, virtually, three days a week for lectures (M, W, F). Mondays and Wednesdays will be mostly lecture content with some hands-on coding, whereas Fridays will be the inverse (mostly hands-on coding). Attending and participating in lectures is a crucial component of learning the material presented in this course. What to expect A lecture will have the following pedagogy layout which will be repeated: Asynchronous pre-class exercises of approxmately 30 min. This will include, reading from the textbooks or other sources, watching videos to prepare you for the class. Approx. 10 minutes of Q&A regarding the pre-class exercises and/or review of homework and quiz questions. Live online instruction followed by a short Q/A session Hands-on exercises, on the ED platform. Sessions will help students develop the intuition for the core concepts, provide the necessary mathematical background, and provide guidance on technical details. Sessions will be accompanied by relevant examples to clarify key concepts and techniques. Quizzes At the end of each lecture, there will be a short, graded quiz that will cover the pre-class and in-class material; there will be no AC209b content in the quizzes. The quizzes will be available until the next lecture. 25% of the quizzes will be dropped from your grade. Exercises Lectures will include one or more coding exercises focused on the newly introduced material; there will be no AC209b content in the exercises. The exercises are short enough to be completed during the time allotted in lecture but they will remain available until the beginning of the following lecture to accomodate those who cannot attend in real time. 25% of the exercises will be dropped from your grade. Exercises will only be factored into your grade if the would improve your final grade. Sections Lectures are supplemented by sections led by teaching fellows. There are two types of sections: Standard Sections : This will be a mix of review of material and practice problems similar to the HW. Advanced Sections The course will include advanced sections for 209b students and will cover a different topic per week. These are 75 min lectures and they will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and lab and extensions of those methods. The material covered in the advanced sections is required for all AC209b students. Tentative topics are: Segmentation Techniques, YOLO, Unet and M-RCNN RNN, Echo State Transformers / NLP Neural Network Inference GANS, Cycle GANS, etc. Deep Reinforcement Learning Note: Sections are not held every week. Consult the course calendar for exact dates. Exams There are no exams in this course. Projects During the final four (4) weeks of the course, students will be divided in break-out thematic sections where they will study topics. These topics are tentative at the moment but may include medicine, law, astronomy, e-commerce, and government. Each section will include lectures by Harvard faculty, experts on the field, followed by project work also led by that faculty. You will get to present your projects in the SEAS Design Fair at the end of the semester. Homework Assignments There will be eight graded homework assignments. Some of them will be due one week after being assigned and some will be due two weeks after being assigned. For six assignments, you have the option to work and submit in pairs, the two remaining are to be completed individually. Course Resources Online Materials All course materials, including lecture notes, lab notes, and section notes will be published on the course GitHub repo as well as the public site's Materials section . Note: Lecture content for weeks 1-3 is only available to registered students through the Materals section. Assignments will only be posted on Canvas. Working Environment You will be working in Jupyter Notebooks which you can run in your own machine or in the SEAS JupyterHub. Recommended Textbooks ISLR: An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani (Springer: New York, 2013) DL: Deep Learning by Goodfellow, Bengio and Courville. (The MIT Press: Cambridge, 2016) Glassner: Deep Learning, Vol. 1 & 2 by Andrew Glassner SLP Speech and Language Processing by Jurafsky and Martin (3rd Edition Draft) INLP Introduction to Natural Language Processing by Jacob Eisenstein (The MIT Press: Cambridge, 2019) Free electronic versions are available ( ISLR , DL , SLP , INLP ) or hard copy through Amazon ( ISLR , DL , Glassner , SLP , INLP ). Getting Help For questions about homework, course content, package installation, the process is: try to troubleshoot yourself by reading the lecture, lab, and section notes, and looking up online resources. go to office hours this is the best way to get help. post on the class Ed forum; we want you and your peers to engage in helping each other. TFs also monitor Ed and will respond within 24 hours. Note that Ed questions are visible to everyone. If you are citing homework solution code you must post privately so that only the staff sees your message. watch for official announcements via Canvas so make sure you have your Canvas notifications turned on. Ed should always be your first resource for seeking answers to your content questions. send an email to the Helpline cs109b2020@gmail.com for administrative issues, regrade requests, and non-content specific questions. for personal matters that you do not feel comfortable sharing with the TFs, you may send an email to either or both of the instructors. Course Policies and Expectations Grading for CS109b, STAT121b, and CS209b (tentative): Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Paired Homework (6) 47% Individual Homework (2) 21% Quizzes 8% Ed Exercises 4% Project 20% Total 100% Note: Regular homework (for everyone) counts as 5 points. 209b extra homework counts as 1 point. Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit different papers, include the name of each other in the designated area of the submission paper. if you work with a fellow student and want to submit the same paper you need to form a group prior to the submission. Details in the assignment. Not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator you are welcome to take ideas from code presented in labs, lecture, or sections but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Wrongly Submitted Assignments There are no late days in homework submission. We will accept late submissions only for medical reasons and if accompanied by a doctor's note. To submit after Canvas has closed or to ask for an extension , send an email to the Helpline with subject line \"Submit HW1: Reason=the flu\" replacing 'HW1' with the name of the current assignment and \"the flu\" with your reason. You need to attach the note from your medical provider otherwise we will not accept the request. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading . The points we take off are based on a grading rubric that is being applied uniformly to all assignments. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release. Auditing the Class You are welcome to audit this course. To request access, send an email to cs109b2021@gmail.com with you HUID (required) and a statment of agreement to the terms below. All auditors must agree to abide by the following rules: All auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Auditors may not attend lectures or section during the live stream. Students are randomly assigned to small groups for coding exercises and mixing auditors and students in this way is not ideal. We are investigating methods that would allow auditors to join, but this is the current policy. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, Jupyter Hub, and office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109b we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. You are responsible for understanding Harvard Extension School policies on academic integrity https://www.extension.harvard.edu/resources-policies/student-conduct/academic-integrity and how to use sources responsibly. Stated most broadly, academic integrity means that all course work submitted, whether a draft or a final version of a paper, project, take-home exam, online exam, computer program, oral presentation, or lab report, must be your own words and ideas, or the sources must be clearly acknowledged. The potential outcomes for violations of academic integrity are serious and ordinarily include all of the following: required withdrawal (RQ), which means a failing grade in the course (with no refund), the suspension of registration privileges, and a notation on your transcript. Using sources responsibly https://www.extension.harvard.edu/resources-policies/resources/avoiding-plagiarism is an essential part of your Harvard education. We provide additional information about our expectations regarding academic integrity on our website. We invite you to review that information and to check your understanding of academic citation rules by completing two free online 15-minute tutorials that are also available on our site. (The tutorials are anonymous open-learning tools.) Accommodations for students with disabilities Harvard students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Harvard Extension School is committed to providing an inclusive, accessible academic community for students with disabilities and chronic health conditions. The Accessibility Services Office (ASO) https://www.extension.harvard.edu/resources-policies/accessibility-services-office-aso offers accommodations and supports to students with documented disabilities. If you have a need for accommodations or adjustments in your course, please contact the Accessibility Services Office by email at accessibility@extension.harvard.edu or by phone at 617-998-9640. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve.",
        "tags": "pages",
        "url": "pages/syllabus.html"
    }, {
        "title": "Lecture 33: œÇ Deep Q-Learning",
        "text": "Slides Lecture 33: Deep Q-Learning (PDF) Exercises Lecture 33: Q-Learning using DQN (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture33/"
    }, {
        "title": "Lecture 33: œÇ Deep Q-Learning",
        "text": "Title üÜì Exercise: Q-Learning using DQN Description The aim of this exercise is to implement Deep Q Networks for a pre-defined reinforcement learning environment. For this, we will be using a pre-defined environment by OpenAI Gym. We will be using an environment called FrozenLake-v0. This is the same as what was used for the previous session (Refer to it to get information about the environment). Instructions Initialize an environment using a pre-defined environment from OpenAI Gym. Get the number of possible actions in the environment. Define a simple feed-forward neural network with your choice of hidden layers and nodes. Define the action sampling policy. We will be using the Epsilon Greedy policy. Initialize sequential memory to store the data, which is the input to the DQN. Define the DQN and compile it with Adam optimizer. Fit and test the DQN model. Hints gym.make(environment-name) Access a pre-defined environment Env.action_space.n : Returns the number of discrete actions Env.observation_space.n : Returns the number of discrete states Dense() A regular densely-connected NN layer Flatten() Flattens the input. Adam() Optimizer that implements the Adam algorithm DQNAgent() Initializes the DQN Agent SequentialMemory() Keras-RL provides a class called rl.memory.SequentialMemory that provides a fast and efficient data structure that we can store the agent's experiences in. In [2]: # Run this once and then comment it to ensure it does not run multiple times # !pip install keras-rl2 In [6]: # Import necessay libraries import numpy as np import gym from keras.models import Sequential from keras.layers import Dense , Activation , Flatten from keras.optimizers import Adam from rl.agents.dqn import DQNAgent from rl.policy import EpsGreedyQPolicy from rl.memory import SequentialMemory In [15]: # Initializing an environment using a pre-defined environment from OpenAI Gym # The environment used here is 'FrozenLake-v0' env = ___ # Get the number of actions within the environment nb_actions = ___ In [16]: # Define a Feed-Forward Neural Network # Initialize a keras sequential model model = ___ # Flatten the input to have an input shape of (1,) + # shape of the environment state space i.e. the observation space model . add ( Flatten ( ___ )) # Add Dense layers with Relu activation # The number of hidden layers and number of nodes in each layer is your choice ___ # Add an output layer with number of nodes as the number of actions ___ In [0]: # Define the policy to sample the actions # We will be using the Epsilon-Greedy algorithm policy = EpsGreedyQPolicy () # To store our data initialize Sequential Memory with limit=500000 and window_length of 1 memory = SequentialMemory ( ___ ) DQN AGENT In [0]: # Initialize the DQNAgent with the neural network model, nb_actions as the number of actions in the environment, # set the memory as the sequential memory defined above, nb_steps_warmup as 100, policy as the epsilon greedy policy defined above # and set the target_model_update as 1e-2 dqn = DQNAgent ( ___ ) # Compile the DQN with Adam optimizer with learning rate of 1e-3 and metric as mse dqn . compile ( ___ ) # Fit the DQN by passing with environment with nb_steps as 5000 # You have an option to visualize the output, which is done by implicitly calling the render function of the environment # However, this will slow down the training process and is not recommended for EdStem # To see the complete training details, set verbose as 2 dqn . fit ( ___ ); In [0]: # Test your model by passing the environment and running for 10 episodes dqn . test ( ___ )",
        "tags": "lectures",
        "url": "lectures/lecture33/notebook/"
    }, {
        "title": "Lecture 32: Œ¨ Bellman equation, Optimality and Recursive algorithms",
        "text": "Slides Lecture 32: Bellman equation, Optimality and Recursive algorithms (PDF) Lecture 32: Bellman equation, Optimality and Recursive algorithms - Compressed (PDF) Exercises Lecture 32: Exercise: Finding the Optimal Policy (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture32/"
    }, {
        "title": "Lecture 32: Œ¨ Bellman equation, Optimality and Recursive algorithms",
        "text": "Title üÜì Exercise: Finding the Optimal Policy Description The aim of this exercise is to find the optimal policy that given the maximum reward given an environment. For this, we will be using a pre-defined environment by OpenAI Gym. We will be using an environment called FrozenLake-v0.There are many environments defined by OpenAI Gym, that you can see here. Environment Description: Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The surface is described using a grid-like the following: Possible actions are Left(0), Right(1), Down(2), Up(3). NOTE - Here we are slightly altering the value iteration algorithm. Instead of computing the optimal value function first, we compute the optimal policy and then find the value function associated with it. Instructions Initialize an environment using a pre-defined environment from OpenAI Gym. Set parameters gamma and theta . Define a function policy_improvement that returns the action which takes us to a higher valued state. This function updates the policy to the optimal policy. Define a function policy_evaluation that updates the state value of the environment given a policy. Define a function value_iteration that calls the above-defined functions to get an optimal policy, action sequence governed by the policy and the state value function. Now test the policy by checking how many episodes (with a fixed number of steps) in the 100 episode loop does the agent reach the final goal. First, try the environment with a random policy, by taking random actions at each state. Next, take actions based on the optimal policy. Hints Equation to compute the value function: $$\\pi(s)= \\sum_{\\{s',r\\}} p(\\{s',r\\}\\ |\\ s,a)\\ [r+\\gamma v(s')]$$ Equation to computer Delta: $$\\triangle\\gets\\max(\\triangle,|v-v(s)|)$$ gym.make(environment-name) Access a pre-defined environment Env.action_space.n : Returns the number of discrete actions Env.observation_space.n : Returns the number of discrete states np.zeros() Return a new array of given shape and type, filled with zeros. environment_object.env.P[s][a] : Returns the probability of reaching successor state (s') and its reward (r) np.argmax() Returns the indices of the maximum values along an axis. max() Returns the largest item in an iterable or the largest of two or more arguments. abs() Returns the absolute value of a number. In [16]: # Import necessary libraries import gym import numpy as np from helper import test_policy In [17]: # Initializing an environment using a pre-defined environment from OpenAI Gym # The environment used here is 'FrozenLake-v0' env = ___ # Setting the initial parameters required for value iteration # Set the discount factor to a value between 0 and 1 gamma = ___ # Theta indicates the threshold determining the accuracy of the iteration # Set it to a value lower than 1e-3 theta = ___ ‚è∏ How does theta affect the policy evaluation and value iteration algorithms? A. A large theta would cause the random policy to converge to the optimal policy much faster. B. Theta does not directly or indirectly affect finding the optimal policy. C. A large theta would cause policy evaluation to fasten but would slow down value iteration. D. A large theta would result in an optimal policy far from the true optimal policy. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___' POLICY IMPROVEMENT In [18]: # Function that returns the action which takes us to a higher valued state # It takes as input the environment, state-value function, policy, # action, current state and the discount rate def policy_improvement ( env , V , pi , action , s , gamma ): # Initialize a numpy array of zeros with the same of the # environment's action space action_temp = ___ # Loop over the size of the environment's action space i.e. # Iterate for every possible action for a in range ( env . action_space . n ): # Set the q value to 0 q = 0 # From the environment get P(s|a) # This will return the probability of reaching successor state (s') and its reward (r) P = np . array ( ___ ) # Iterate over the possible states for i in range ( len ( P )): # Get the possible succesor state s_ = int ( P [ i ][ 1 ]) # Get the transition Probability P(s'|s,a) p = P [ i ][ 0 ] # Get the reward r = P [ i ][ 2 ] # Compute the action value q(s|a) based on the equation # provided in the instruction q += ___ # Save the q-value of taking a particular action into the # action_temp array action_temp [ a ] = q # Get the action from action_temp that has the highest q-value m = ___ # For each state set the action which give the highest q-value action [ s ] = m # Update the policy by setting the action which give the highest # q-value for a state as 1 pi [ s ][ m ] = 1 # Return the updated policy return pi POLICY EVALUATION NOTE - We are not computing the max here as we already have the optimal policy. Instead, we just multiply with the policy which returns the value function of the best action (making others zero). In [19]: # Define a function to update the state value by taking the environment, # current state value, current state, policy and the discount rate def policy_evaluation ( env , V , s , gamma ): # Initialize a policy as a matrix of zeros with size # (Number of state, Number of actions) pi = np . zeros (( env . observation_space . n , env . action_space . n )) # Set the initial value of all actions as zero action = np . zeros (( env . observation_space . n )) # Initialize a numpy array of zeros with the same of the # action space action_temp = np . zeros ( env . action_space . n ) # Call the policy_improvement function to get the policy pi = ___ # Set the initial value as 0 value = 0 # Loop over all possible actions for a in range ( env . action_space . n ): # Set u as 0 to compute the value of each state given the # policy u = 0 # From the environment get P(s|a) P = np . array ( ___ ) # Iterate over the possible states for i in range ( len ( P )): # Get the next state s_ = int ( P [ i ][ 1 ]) # Get the probability of the next state given the current state p = P [ i ][ 0 ] # Get the reward of going from current state to next state r = P [ i ][ 2 ] # Update the value function based on the equation provided # in the instructions u += ___ # Update the value based on the policy and the value function # This step is instead of the max defined by the image above # Since we have the optimal policy, we just multiply the policy pi # to get the value associated to the best action and the others become 0 value += pi [ s , a ] * u # Set the value function of the state as the value computed above V [ s ] = value # Return the value function return V [ s ] ‚è∏ What does env.env.P[s][a] return? A. Probability of reaching successor state, successor state and reward. B. A list of all possible states that can be reached from s. C. Probability of reaching successor state, successor state, reward and whether the episode is done or not. D. A list of all possible states that can be reached from s on taking action a. In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = '___' VALUE ITERATION - Bringing everything together In [20]: # Define the function to perform value iteration def value_iteration ( env , gamma , theta ): # Set the initial value of all states as zero V = ___ # Initialize a loop while True : # Set delta as 0 to compare the estimation accuracy delta = 0 # Loop over all the states for s in range ( ___ ): # Set the value as the state value function initialize above v = V [ s ] # Update the state value function by calling the policy_evaluation function V [ s ] = ___ # Compute the delta based on the changed in value per iteration using the equation # given in the instructions delta = ___ # Check if the change is higher or lower than theta defined at the top # If so then the value has converged to the optimal value if delta < theta : break # Initialize a policy as a matrix of zeros with size # ( Number of state * Number of actions) pi = ___ # Set the initial value of all actions as zero action = ___ # To extract the optimal policy loop over all the states for s in range ( ___ ): # Call the policy_improvement function to get the optimal policy pi = ___ # Return the optimal value function, the policy and the action sequence return V , pi , action In [0]: # Call the value_iteration function to get the action sequence, optimal policy and value function V , pi , action = ___ # Print the discrete action to take in a given state print ( \"THE ACTION TO TAKE IN A GIVEN STATE IS: \\n \" , np . reshape ( action ,( 4 , 4 ))) # Print the optimal policy print ( \" \\n\\n\\n THE OPTIMAL POLICY IS: \\n \" , pi ) TESTING THE POLICY In [0]: # Use the helper function test_policy in the helper file to compute the # number of times the agent reaches the goal within a fixed number of steps # in each episode # Every time the agent reaches the goal within the fixed step we call it a sucsess # Set a variable random as 1 # This will ensure that the test_policy function gives the result of some random policy random = 1 # Call the test_policy function by passing the environment, action and random test_policy ( env , action , random ) # Set a variable random as 0 # This will ensure that the test_policy function gives the result of the optimal policy random = 0 # Call the test_policy function by passing the environment, action and random test_policy ( env , action , random ) ‚è∏ How does increasing and decreasing gamma change the policy and reward? In [0]: ### edTest(test_chow3) ### # Type your answer within in the quotes given answer3 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture32/notebook/"
    }, {
        "title": "Lecture 31: œÄ Reinforcement Learning: Basics",
        "text": "Slides Lecture 31: Introduction to Reinforcement Learning and MDP (PDF) Exercises Lecture 31: Setting up a custom environment (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture31/"
    }, {
        "title": "Lecture 31: œÄ Reinforcement Learning: Basics",
        "text": "Title Exercise: Setting up a custom environment Description The aim of this exercise is to learn how to set up a custom environment using OpenAI Gym . OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pinball. As we have discussed in class, the environment will consist of a state, reward and action. For our custom environment, we would like to implement the mouse grid we saw in the lectures. The possible rewards and state given the current state are in the helper file. The overall reward pattern is as follows, every next state gets a plus one reward, a terminal state leads to -10 and the mouse has to start from state 1 again. Every action leading to a previous state is penalized by 1 and reaching the goal i.e. state 16 gets a +100 reward. Create a class MouseGrid that inherits from OpenAI Gym's Env. This class will be your environment. Within the class constructor __init__ , initialize the observation_space and action_space. The observation space is the set of all possible state values, which are 16 in our case. The action space is the set of all possible actions one can take in an environment. 1 indicates up, 1 indicates left, 3 indicates right and 4 indicates down. Set the initial state to be 1 and the reward to be zero. Define a method step that take the action and returns the reward based on the instructions given in the helper file. Define a method reset, that reset the class variables to their initial value. Select the number of episodes. Create an instance of the MouseGrid environment. Loop over the total number of episodes. Sample an action from the set of possible action and get call the step method. At the end of each episode, print the total reward of that episode. NOTE - Remember that the reward can be negative as well. It depends on how the reward system is defined within the environment. In this exercise, we are using a random policy, by randomly sampling from the set of possible actions defined within the environment. Hints: Discrete() The Discrete space allows a fixed range of non-negative numbers Env.action_space.sample() To select a value randomly from the action space defined in the environment. In [1]: # Import necessary libraries import gym import random import numpy as np from gym import Env from gym.spaces import Discrete from helper import transition_rules , reward_rules In [62]: # *** Refer to the Hints in the Description before you start *** # Class MouseGrid that inherits from OpenAI Gym Env class MouseGrid ( Env ): # Constructor that is called when an instance of the class is created def __init__ ( self ): # Set the observation_space i.e the state to 16 discrete values self . observation_space = ___ # Set the action space to 4 discrete values i.e up, left, right and down # Action 1: Up # Action 2: Left # Action 3: Right # Action 4: Down self . action_space = ___ # Set the initial state of the mouse to 1 self . state = ___ # Set the initial reward as 0 self . reward = ___ # Define a function step that inputs an action and updates the reward def step ( self , action ): # Update the action to the action sent as the parameter of this function self . action = ___ # Set a variable prev_state as the current state of the environment prev_state = ___ # Update the state based on the current state and action by # calling the transition_rules function defined in the helper # file with the current state and action self . state = ___ # Update the reward by calling the function reward_rules defined in # the helper file by passing the current state and previous state # Remember that the reward is cummulative self . reward = ___ # If the current state is 16 that means our mouse has reached the goal # For this, we set the call the reward_rules function again # Set done as True if self . state == 16 : self . reward = self . reward + reward_rules ( self . state , prev_state ) done = True # Else set done as false else : done = False # Return the state, reward and done to indicate whether an episode is complete return self . state , self . reward , done # The reset function which is called at the end of each episode def reset ( self ): # Reset the initial state to the start point self . state = ___ # Reset the reward to 0 self . reward = ___ # Set done as False done = False SET THE AGENT TO TEST THE ENVIRONMENT In [0]: ### edTest(test_chow0) ### # Create an instance of the custom environment env = ___ # Set the maximum number of episodes to any integer between 10 and 50 episodes = ___ # Loop over all the steps for i in range ( episodes ): # Set done as False to run until the end of the episode done = False # Loop over the entire episode while done != True : # Sample an action from the action_space of the environment action = ___ # Call the step function within the environment state , reward , done = ___ # Call the reset function at the end of an episode env . reset () # Print the reward at the end of each episode print ( \"The reward of this episode is:\" , reward ) ‚è∏ Here we are using random sampling to pick an action for a given state. However, if you had a policy, which part of the exercise would you change to incorporate it? A. The step() method of the MouseGrid class. B. self.action within the init () method of the MouseGrid class. C. getting an action for each step within the for loop over all episodes. D. Call to the step() method in the last cell. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___' ‚è∏ Which of the following would be an issue if the reset() method is not called at the end of each episode? A. The next episode will not run as the state is 16. B. The action sampled next will be biased on the previous value. C. The reward of the new episode will be combined to the reward of the previous episode. D. The reset() method does not affect anything. In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below # There can be multiple correct answers. Replace the options with a hyphen # For example if you think the correct choice is A and D, then type 'A-D' answer2 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture31/notebook/"
    }, {
        "title": "Lecture 30: ‚ç∫ GANs DOS",
        "text": "Slides Lecture 30: GANS part 2 - compressed (PDF) Exercises Lecture 30: Fr√©chet distance (numpy) (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture30/"
    }, {
        "title": "Lecture 30: ‚ç∫ GANs DOS",
        "text": "Title Fr√©chet distance (numpy) Description $$FD=\\left|\\mu-\\mu_w\\right|&#94;2+tr\\left(\\Sigma+\\Sigma_w-2\\left(\\Sigma\\Sigma_w\\right)&#94;{\\frac{1}{2}}\\right)$$ Hints numpy cov numpy trace In [0]: # calculate frechet inception distance with numpy import numpy as np from scipy.linalg import sqrtm In [0]: # calculate frechet inception distance def calculate_fd ( act1 , act2 ): # calculate mean and covariance statistics # The \"mu_1\" and \"mu_2\" refer to the feature-wise mean of the real # and generated images, e.g. 2,048 element vectors where each # element is the mean feature observed across the images. mu1 , mu2 = ___ # The sigma1 and sigma2 are the covariance matrix for the real # and generated feature vectors. # Remember each row is a sample/image. sigma1 , sigma2 = ___ # calculate sum squared difference between means ssdiff = np . sum (( mu1 - mu2 ) ** 2.0 ) # calculate sqrt of product between cov covmean = sqrtm ( sigma1 . dot ( sigma2 )) # check and correct imaginary numbers from sqrt if np . iscomplexobj ( covmean ): covmean = covmean . real # calculate score fd = ___ return fd In [0]: ### edTest(test_check) ### # define two collections of activations np . random . seed ( 109 ) # We assume 10 images # The output layer of the Inception model is removed and the output # is taken as the activations from the last pooling layer, hence 2048. act1 = np . random . random ( 10 * 2048 ) act1 = act1 . reshape (( 10 , 2048 )) np . random . seed ( 295 ) act2 = np . random . random ( 10 * 2048 ) act2 = act2 . reshape (( 10 , 2048 )) # fd between act1 and act1 fd = calculate_fd ( act1 , act1 ) print ( 'FD (same): %.3f ' % fd ) # fd between act1 and act2 fd = calculate_fd ( act1 , act2 ) print ( 'FD (different): %.3f ' % fd ) In [2]: In [3]: In [4]:",
        "tags": "lectures",
        "url": "lectures/lecture30/notebook1/"
    }, {
        "title": "Lecture 29: œÄ GANs",
        "text": "Slides Lecture 29: GANS Basics - compressed (PDF) Lecture 29: GANS Basics (PDF) Exercises Lecture 29: Generative Adversarial Networks (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture29/"
    }, {
        "title": "Lecture 29: œÄ GANs",
        "text": "Title Generative Adversarial Networks Generate 1D Gaussian Distribution from Uniform Noise Description In this exercise, we are going to generate 1-D Gaussian distribution from a n-D uniform distribution. This is a toy exercise in order to understand the ability of GANs (generators) to generate arbitrary distributions from random noise. Hints tf.keras.Model.train_on_batch() tf.keras.Model.train_on_batch() Runs a single gradient update on a single batch of data. np.ones np.ones np.zeros np.zeros üé® Generative Models üñº Generate 1-D Gaussian distribution from a n-D uniform distribution In this exercise, we are going to generate 1-D Gaussian distribution from a n-D uniform distribution. This is a toy exercise in order to understand the ability of GANs (generators) to generate arbitrary distributions from random noise. In [4]: import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy from scipy.stats import norm import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Input from tensorflow.keras.utils import plot_model % matplotlib inline tf . random . set_seed ( 109 ) np . random . seed ( 109 ) In [11]: #This is our real dataset (samples from stanard normal distribution ) def generate_data ( n_samples = 10000 , n_dim = 1 ): np . random . seed ( 109 ) return np . random . randn ( n_samples , n_dim ) In [12]: # A general function to define feedforward architecture. def make_model ( input_dim , output_dim , hidden_dim = 64 , n_layers = 1 , activation = 'tanh' , optimizer = 'adam' , loss = 'binary_crossentropy' ): model = Sequential () model . add ( Dense ( hidden_dim , input_dim = input_dim , activation = activation )) for _ in range ( n_layers - 1 ): model . add ( Dense ( hidden_dim ), activation = activation ) model . add ( Dense ( output_dim )) model . compile ( loss = loss , optimizer = optimizer ) return model In [13]: # hyper-parameters NOISE_DIM = 100 DATA_DIM = 1 #this is equivalent to image size e.g. 32*32*3 G_LAYERS = 1 D_LAYERS = 1 generator = make_model ( NOISE_DIM , DATA_DIM , n_layers = G_LAYERS ) discriminator = make_model ( DATA_DIM , 1 , n_layers = D_LAYERS , activation = 'sigmoid' ) In [16]: def get_gan_network ( random_dim , optimizer = 'adam' ): #This ensures that when we combine our networks we only train the Generator. #While generator training we do not want discriminator weights to be adjusted. discriminator . trainable = False gan_input = Input ( shape = ( random_dim ,)) x = generator ( gan_input ) gan_output = discriminator ( x ) #this model will be used to train generator gan = tf . keras . Model ( inputs = gan_input , outputs = gan_output ) gan . compile ( loss = 'binary_crossentropy' , optimizer = optimizer ) return gan In [17]: def train_gan ( epochs = 15 , batch_size = 128 ): #Loads the real data x_train = generate_data ( n_samples = 12800 , n_dim = DATA_DIM ) gan = get_gan_network ( NOISE_DIM , 'adam' ) # Get GAN model for e in range ( 1 , epochs + 1 ): np . random . seed ( 109 + e ) # noise from a uniform distribution noise = np . random . rand ( batch_size , NOISE_DIM ) # generate a batch of fake data/images) generated_values = generator . predict ( noise ) #Gets a batch of real data (images) true_batch = x_train [ np . random . choice ( x_train . shape [ 0 ], batch_size , replace = False ), :] # Train discriminator on real data, use train_on_batch # real data has label of all 1s disc_history_true = ___ # Train discriminator on generated values, use train_on_batch # fake data has label of all 0s disc_history_noise = ___ # Train generator/GAN noise = np . random . rand ( batch_size , NOISE_DIM ) y_gen = np . ones ( batch_size ) # Train gan with noise, with label all 1s. gan_loss = ___ return generator , discriminator , gan_loss , disc_history_true , disc_history_noise generator , discriminator , gan_loss , disc_history_true , disc_history_noise = train_gan () In [0]: ### edTest(test_check) ### print ( gan_loss , disc_history_true , disc_history_noise ) In [18]: noise = np . random . rand ( 10000 , NOISE_DIM ) generated_values = generator . predict ( noise ) plt . hist ( generated_values , bins = 100 ) true_gaussian = [ np . random . randn () for x in range ( 10000 )] print ( '1st order moment - ' , 'True : ' , scipy . stats . moment ( true_gaussian , 1 ) , ', GAN :' , scipy . stats . moment ( generated_values , 1 )) print ( '2nd order moment - ' , 'True : ' , scipy . stats . moment ( true_gaussian , 2 ) , ', GAN :' , scipy . stats . moment ( generated_values , 2 )) print ( '3rd order moment - ' , 'True : ' , scipy . stats . moment ( true_gaussian , 3 ) , ', GAN :' , scipy . stats . moment ( generated_values , 3 )) print ( '4th order moment - ' , 'True : ' , scipy . stats . moment ( true_gaussian , 4 ) , ', GAN :' , scipy . stats . moment ( generated_values , 4 )) plt . show () 1st order moment - True : 0.0 , GAN : [0.] 2nd order moment - True : 1.0016611800894593 , GAN : [0.11815298] 3rd order moment - True : 0.003238467835095741 , GAN : [-0.00209988] 4th order moment - True : 3.0480390897072773 , GAN : [0.04024393] CONCLUSIONS 1. GANs are able to learn a generative model from general noise distributions. 2. Traditional GANs do not learn the higher-order moments well. Possible issues : Number of samples, approximating higher moments is hard. Usually known to under-predict higher order variances. For people interested in learning why, read more about divergence measures between distributions (particularly about Wasserstein etc.) References GANs in Action (Jakub Langr; Vladimir Bok) https://machinelearningmastery.com/generative-adversarial-network-loss-functions/ https://arxiv.org/pdf/1406.2661.pdf In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture29/notebook1/"
    }, {
        "title": "Advanced Section 4: Variational Inference",
        "text": "Slides Variational Inference [PDF]",
        "tags": "a-sections",
        "url": "a-sections/a-sec04/"
    }, {
        "title": "Lecture 28: Œø Variational AE 2",
        "text": "Slides Lecture 28: Variational AE 2 (PDF) Exercises Lecture 28: Variational Auto-Encoder From scratch (Notebook) Lecture 28: Variational Auto-Encoder Simplified (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture28/"
    }, {
        "title": "Lecture 28: Œø Variational AE 2",
        "text": "Title Variational Auto-Encoder From scratch Description : The goal of this exercise is to build a VAE from scratch to reconstruct images of the MNIST dataset. In [2]: # Import required libraries import numpy as np import pandas as p import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras.datasets import mnist from tensorflow.keras import layers , models , optimizers , regularizers from scipy.stats import norm % matplotlib inline In [3]: # Get a subset of the mnist data x_train , x_test = np . load ( 'mnist_mini_train.npy' ), np . load ( 'mnist_mini_test.npy' ) Preprocessing Images As per the original paper on VAE Kingma et al , we make an Independent Bernoulli assumption on all of the pixels of our image. However, the original MNIST image pixel values are not labels but values between 0 & 255. Hence we must convert the individual pixel values to a Bernoulli distribution. We can do that by choosing a threshold, and assigning value 1 if the pixel value is above the threshold, else zero. In [4]: # Function to # 1. Change dimensions # 2. Change datatype def binary_preprocess ( imageset ): imageset = imageset . reshape ( imageset . shape [ 0 ], 28 , 28 , 1 ) / 255. return np . where ( imageset > . 5 , 1.0 , 0.0 ) . astype ( 'float32' ) In [5]: # Pre-processed images to satisfy the Independent Bernoulli condition x_train_images = binary_preprocess ( x_train ) x_test_images = binary_preprocess ( x_test ) In [6]: # Dataset object to get a mini-batch batch_size = 100 train_size = x_train_images . shape [ 0 ] latent_size = 2 input_shape = ( 28 , 28 , 1 ) In [7]: # Model encoder architecture encoder = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( input_shape = ( 28 , 28 , 1 )), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 32 , activation = 'relu' ), # No activation tf . keras . layers . Dense ( 4 ), ] ) In [8]: # Model decoder architecture decoder = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( input_shape = ( 2 ,)), tf . keras . layers . Dense ( 32 , activation = 'relu' ), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 784 , activation = 'sigmoid' ), tf . keras . layers . Reshape (( 28 , 28 , 1 )) ] ) In [9]: # Encoding step # Note: We use logvariance instead of variance # Get the mean and the logvariance def encode ( encoder , x ): activations = encoder ( x ) mean , logvariance = tf . split ( activations , num_or_size_splits = 2 , axis = 1 ) return mean , logvariance # Reparametrization step def sample ( mu , logvariance ): # Here we sample from N(0,1) e = tf . random . normal ( shape = mu . shape ) return e * tf . exp ( logvariance / 2 ) + mu # Combine the autoencoder def autoencoder ( encoder , decoder , x ): mean , logvariance = encode ( encoder , x ) z = sample ( mean , logvariance ) output = decoder ( z ) return output Log space We will be using log loss. This is because numerically is more stable. Log Normal PDF $$f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} e&#94;{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)&#94;{2}}$$$$ \\log f(x)= -\\log(\\sigma) -\\frac{1}{2} \\left(\\log(2 \\pi) -(\\frac{x-\\mu}{\\sigma})&#94;2)\\right)$$ KL Divergence Analytical form We will use this analytical form to compute the KL divergence $\\mathrm{KL} [ q_{\\phi}(\\mathbf{z} | \\mathbf{x}) || p(\\mathbf{z}) ] = - \\frac{1}{2} \\sum_{k=1}&#94;K { 1 + \\log \\sigma_k&#94;2 - \\mu_k&#94;2 - \\sigma_k&#94;2 }$ where $K$ is the number of hidden dimensions. Reconstruction loss: Binary CrossEntropy $H_{p}=-\\frac{1}{N} \\sum_{i=1}&#94;{N} \\sum_j y_{ij} \\cdot \\log \\left(p\\left(y_{ij}\\right)\\right)+\\left(1-y_{ij}\\right) \\cdot \\log \\left(1-p\\left(y_{ij}\\right)\\right)$ where $p(y_i)$ is the output of the NN, $N$ is the number of images and $j$ represents the pixel. In [10]: # Quick way to get the log likelihood of a normal distribution def log_normal_pdf ( value , mean , logvariance , raxis = 1 ): log_2pi = tf . math . log ( 2. * np . pi ) logpdf = - ( logvariance + log_2pi + ( value - mean ) ** 2. * tf . exp ( logvariance )) / 2 return tf . reduce_sum ( logpdf , axis = 1 ) # Loss over the assumed distribution(qz_x) and the prior(pz) def analytical_kl ( encoder , x ): mean , logvariance = encode ( encoder , x ) # tf.reduce_sum is over the hidden dimensions lossval = tf . reduce_sum ( - 0.5 * ( 1 + logvariance - tf . square ( mean ) - tf . exp ( logvariance )), axis =- 1 ) return tf . reduce_mean ( lossval ) # This is now binary cross entropy # Crucially, observe that we sum across the image dimensions # and only take the mean in the images dimension def reconstruction_loss ( encoder , decoder , x ): x_pred = autoencoder ( encoder , decoder , x ) loss = tf . keras . losses . binary_crossentropy ( x , x_pred ) # tf.reduce_sum is over all pixels and tf.reduce_mean is over all images return tf . reduce_mean ( tf . reduce_sum ( loss , axis = [ 1 , 2 ])) In [11]: # Instantiate an optimizer with a learning rate optimizer = tf . keras . optimizers . RMSprop ( learning_rate = 1e-3 ) # Define number of epochs num_epochs = 300 # Loop over the required number of epochs for i in range ( num_epochs ): for j in range ( int ( train_size / batch_size )): # Randomly choose a minitbatch x_train_batch = x_train_images [ np . random . choice ( train_size , batch_size )] # Open the gradienttape to map the computational graph with tf . GradientTape ( persistent = True ) as t : # get predictions from autoencoder() decoder_output = ___ # get reconstruction_loss L1 = ___ # get KL Loss L2 = ___ # Adding the reconstruction loss and KL divergence loss = L1 + L2 # We take the gradients with respect to the decoder gradients1 = t . gradient ( loss , decoder . trainable_weights ) # We take the gradients with respect to the encoder gradients2 = t . gradient ( loss , encoder . trainable_weights ) # We update the weights of the decoder optimizer . apply_gradients ( zip ( gradients1 , decoder . trainable_weights )) # We update the weights of the decoder optimizer . apply_gradients ( zip ( gradients2 , encoder . trainable_weights )) # We display the loss after every 10 epochs if ( i + 1 ) % 10 ==0: print ( f 'Loss at epoch { i + 1 } is { loss : .2f } , KL Divergence is { L2 : .2f } ' ) In [0]: ### edTest(test_check) ### print ( loss , L2 ) Visualize stochastic predictions In [12]: # We choose a text sample index test_sample = 10 # We make a prediction # NOTE: Since we did not add a sigmoid activation, # We must specify it now to convert logits to probabilities pred = autoencoder ( encoder , decoder , x_test_images [ test_sample : test_sample + 1 ]) # We make class predictions for each pixel (ON or OFF) pred = np . where ( pred > 0.5 , 1 , 0 ) pred = pred . squeeze () pred . shape Out[12]: (28, 28) In [13]: # We plot the reconstruction with the true input fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . imshow ( x_test_images [ test_sample ] . squeeze (), cmap = 'gray' ) ax [ 1 ] . imshow ( pred , cmap = 'gray' ) ax [ 0 ] . set_title ( 'True image' , fontsize = 14 ) ax [ 1 ] . set_title ( 'Reconstruction' , fontsize = 14 ); ax [ 0 ] . axis ( 'off' ); ax [ 1 ] . axis ( 'off' ); plt . show () Optional segment Decoding the prior distribution We expect a continuous latent space representation In [14]: # display a 2D manifold of the digits n = 15 # figure with 15x15 digits digit_size = 28 latent_dim = 2 # linearly spaced coordinates on the unit square were transformed # through the inverse CDF (ppf) of the Gaussian to produce values # of the latent variables z, since the prior of the latent space # is Gaussian z1 = norm . ppf ( np . linspace ( 0.01 , 0.99 , n )) z2 = norm . ppf ( np . linspace ( 0.01 , 0.99 , n )) z_grid = np . dstack ( np . meshgrid ( z1 , z2 )) x_pred_grid = tf . sigmoid ( decoder . predict ( z_grid . reshape ( n * n , latent_dim ))) . numpy () \\ . reshape ( n , n , digit_size , digit_size ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . imshow ( np . block ( list ( map ( list , x_pred_grid ))), cmap = 'binary' ) # ax.axis('off') ax . set_xlabel ( '$z_1$ ' , fontsize = 32 ) ax . set_ylabel ( '$z_2$ ' , fontsize = 32 , rotation = 0 ) ax . set_xticklabels ( '' ) ax . set_yticklabels ( '' ) plt . tight_layout () In [15]:",
        "tags": "lectures",
        "url": "lectures/lecture28/notebook1/"
    }, {
        "title": "Lecture 28: Œø Variational AE 2",
        "text": "Title Variational Auto-Encoder From scratch Description : The goal of this exercise is to build a VAE from scratch to reconstruct images of the MNIST dataset. In [2]: # Import required libraries import numpy as np import pandas as p import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras.datasets import mnist from tensorflow.keras import layers , models , optimizers , regularizers from scipy.stats import norm % matplotlib inline In [3]: # Get a subset of the mnist data x_train , x_test = np . load ( 'mnist_mini_train.npy' ), np . load ( 'mnist_mini_test.npy' ) Preprocessing Images As per the original paper on VAE Kingma et al , we make an Independent Bernoulli assumption on all of the pixels of our image. However, the original MNIST image pixel values are not labels but values between 0 & 255. Hence we must convert the individual pixel values to a Bernoulli distribution. We can do that by choosing a threshold, and assigning value 1 if the pixel value is above the threshold, else zero. In [4]: # Function to # 1. Change dimensions # 2. Change datatype def binary_preprocess ( imageset ): imageset = imageset . reshape ( imageset . shape [ 0 ], 28 , 28 , 1 ) / 255. return np . where ( imageset > . 5 , 1.0 , 0.0 ) . astype ( 'float32' ) In [5]: # Pre-processed images to satisfy the Independent Bernoulli condition x_train_images = binary_preprocess ( x_train ) x_test_images = binary_preprocess ( x_test ) In [6]: # Dataset object to get a mini-batch batch_size = 100 train_size = x_train_images . shape [ 0 ] latent_size = 2 input_shape = ( 28 , 28 , 1 ) In [7]: # Model encoder architecture encoder = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( input_shape = ( 28 , 28 , 1 )), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 32 , activation = 'relu' ), # No activation tf . keras . layers . Dense ( 4 ), ] ) In [8]: # Model decoder architecture decoder = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( input_shape = ( 2 ,)), tf . keras . layers . Dense ( 32 , activation = 'relu' ), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 784 , activation = 'sigmoid' ), tf . keras . layers . Reshape (( 28 , 28 , 1 )) ] ) In [9]: # Encoding step # Note: We use logvariance instead of variance # Get the mean and the logvariance def encode ( encoder , x ): activations = encoder ( x ) mean , logvariance = tf . split ( activations , num_or_size_splits = 2 , axis = 1 ) return mean , logvariance # Reparametrization step def sample ( mu , logvariance ): # Here we sample from N(0,1) e = tf . random . normal ( shape = mu . shape ) return e * tf . exp ( logvariance / 2 ) + mu # Combine the autoencoder def autoencoder ( encoder , decoder , x ): mean , logvariance = encode ( encoder , x ) z = sample ( mean , logvariance ) output = decoder ( z ) return output Log space We will be using log loss. This is because numerically is more stable. Log Normal PDF $$f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} e&#94;{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)&#94;{2}}$$$$ \\log f(x)= -\\log(\\sigma) -\\frac{1}{2} \\left(\\log(2 \\pi) -(\\frac{x-\\mu}{\\sigma})&#94;2)\\right)$$ KL Divergence Analytical form We will use this analytical form to compute the KL divergence $\\mathrm{KL} [ q_{\\phi}(\\mathbf{z} | \\mathbf{x}) || p(\\mathbf{z}) ] = - \\frac{1}{2} \\sum_{k=1}&#94;K { 1 + \\log \\sigma_k&#94;2 - \\mu_k&#94;2 - \\sigma_k&#94;2 }$ where $K$ is the number of hidden dimensions. Reconstruction loss: Binary CrossEntropy $H_{p}=-\\frac{1}{N} \\sum_{i=1}&#94;{N} \\sum_j y_{ij} \\cdot \\log \\left(p\\left(y_{ij}\\right)\\right)+\\left(1-y_{ij}\\right) \\cdot \\log \\left(1-p\\left(y_{ij}\\right)\\right)$ where $p(y_i)$ is the output of the NN, $N$ is the number of images and $j$ represents the pixel. In [10]: # Quick way to get the log likelihood of a normal distribution def log_normal_pdf ( value , mean , logvariance , raxis = 1 ): log_2pi = tf . math . log ( 2. * np . pi ) logpdf = - ( logvariance + log_2pi + ( value - mean ) ** 2. * tf . exp ( logvariance )) / 2 return tf . reduce_sum ( logpdf , axis = 1 ) # Loss over the assumed distribution(qz_x) and the prior(pz) def analytical_kl ( encoder , x ): mean , logvariance = encode ( encoder , x ) # tf.reduce_sum is over the hidden dimensions lossval = tf . reduce_sum ( - 0.5 * ( 1 + logvariance - tf . square ( mean ) - tf . exp ( logvariance )), axis =- 1 ) return tf . reduce_mean ( lossval ) # This is now binary cross entropy # Crucially, observe that we sum across the image dimensions # and only take the mean in the images dimension def reconstruction_loss ( encoder , decoder , x ): x_pred = autoencoder ( encoder , decoder , x ) loss = tf . keras . losses . binary_crossentropy ( x , x_pred ) # tf.reduce_sum is over all pixels and tf.reduce_mean is over all images return tf . reduce_mean ( tf . reduce_sum ( loss , axis = [ 1 , 2 ])) In [11]: # Instantiate an optimizer with a learning rate optimizer = tf . keras . optimizers . RMSprop ( learning_rate = 1e-3 ) # Define number of epochs num_epochs = 300 # Loop over the required number of epochs for i in range ( num_epochs ): for j in range ( int ( train_size / batch_size )): # Randomly choose a minitbatch x_train_batch = x_train_images [ np . random . choice ( train_size , batch_size )] # Open the gradienttape to map the computational graph with tf . GradientTape ( persistent = True ) as t : decoder_output = autoencoder ( encoder , decoder , x_train_batch ) L1 = reconstruction_loss ( encoder , decoder , x_train_batch ) L2 = analytical_kl ( encoder , x_train_batch ) # Adding the reconstruction loss and KL divergence loss = L1 + L2 # We take the gradients with respect to the decoder gradients1 = t . gradient ( loss , decoder . trainable_weights ) # We take the gradients with respect to the encoder gradients2 = t . gradient ( loss , encoder . trainable_weights ) # We update the weights of the decoder optimizer . apply_gradients ( zip ( gradients1 , decoder . trainable_weights )) # We update the weights of the decoder optimizer . apply_gradients ( zip ( gradients2 , encoder . trainable_weights )) # We display the loss after every 10 epochs if i + 1 % 10 ==0: print ( f 'Loss at epoch { i + 1 } is { loss : .2f } , KL Divergence is { L2 : .2f } ' ) Visualize stochastic predictions In [12]: # We choose a text sample index test_sample = 10 # We make a prediction # NOTE: Since we did not add a sigmoid activation, # We must specify it now to convert logits to probabilities pred = autoencoder ( encoder , decoder , x_test_images [ test_sample : test_sample + 1 ]) # We make class predictions for each pixel (ON or OFF) pred = np . where ( pred > 0.5 , 1 , 0 ) pred = pred . squeeze () pred . shape Out[12]: (28, 28) In [13]: # We plot the reconstruction with the true input fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . imshow ( x_test_images [ test_sample ] . squeeze (), cmap = 'gray' ) ax [ 1 ] . imshow ( pred , cmap = 'gray' ) ax [ 0 ] . set_title ( 'True image' , fontsize = 14 ) ax [ 1 ] . set_title ( 'Reconstruction' , fontsize = 14 ); ax [ 0 ] . axis ( 'off' ); ax [ 1 ] . axis ( 'off' ); plt . show () Optional segment Decoding the prior distribution We expect a continuous latent space representation In [14]: # display a 2D manifold of the digits n = 15 # figure with 15x15 digits digit_size = 28 latent_dim = 2 # linearly spaced coordinates on the unit square were transformed # through the inverse CDF (ppf) of the Gaussian to produce values # of the latent variables z, since the prior of the latent space # is Gaussian z1 = norm . ppf ( np . linspace ( 0.01 , 0.99 , n )) z2 = norm . ppf ( np . linspace ( 0.01 , 0.99 , n )) z_grid = np . dstack ( np . meshgrid ( z1 , z2 )) x_pred_grid = tf . sigmoid ( decoder . predict ( z_grid . reshape ( n * n , latent_dim ))) . numpy () \\ . reshape ( n , n , digit_size , digit_size ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . imshow ( np . block ( list ( map ( list , x_pred_grid ))), cmap = 'binary' ) # ax.axis('off') ax . set_xlabel ( '$z_1$ ' , fontsize = 32 ) ax . set_ylabel ( '$z_2$ ' , fontsize = 32 , rotation = 0 ) ax . set_xticklabels ( '' ) ax . set_yticklabels ( '' ) plt . tight_layout () In [15]:",
        "tags": "lectures",
        "url": "lectures/lecture28/notebook2/"
    }, {
        "title": "Lecture 27: œÑ AE the Bayesian Approach",
        "text": "Slides Lecture 27: AE the Bayesian Approach (PDF) Exercises Lecture 27: MCMC from Scratch for Linear Regression (Notebook) Lecture 27: MCMC from Scratch for Neural Networks (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture27/"
    }, {
        "title": "Lecture 27: œÑ AE the Bayesian Approach",
        "text": "Title MCMC from Scratch for Linear Regression Description : The aim of this exercise is to perform Monte Carlo Markov Chain (MCMC) from scratch for linear regression. For this, we will be using our old friend the Advertising dataset. On completing the exercise you should be able to see the following distribution. One for each of the beta value: Instructions: Read the data file Advertising.csv and set the predictor and response variables. Fit a linear regression model on the advertising data and take a look at the beta values. Create 2 lists to store the beta values and initialize the beta values. Define a function get_prior to compute the prior value given the beta values. Compute the likelihood, prior and posterior for the initial beta values. For a selected number of sampling \"epochs\": Compute new beta values Compute the corresponding likelihood, prior and posterior. Compute the exponential ratio of the current and previous posterior. Based on the ratio, select or reject the new beta values. Choose a burn rate. Plot the histogram of the beta values. Hints: np.log() Computes the natural logarithm, element-wise. np.exp() Calculates the exponential of all elements in the input array. LinearRegression() Initiates an ordinary least squares Linear Regression. .fit() Fits the linear model to the data. model.coef_ Estimated coefficients for the linear regression problem model.intercept_ Independent term in the linear model. np.random.normal() Draw random samples from a normal (Gaussian) distribution. norm.pdf() A normal continuous random variable. np.sum() Sum of array elements over a given axis. np.random.uniform() Draw samples from a uniform distribution. In [0]: # Import necessary libraries from sklearn.linear_model import LinearRegression import matplotlib.pyplot as plt import pandas as pd import numpy as np % matplotlib inline from scipy import stats from scipy.stats import norm In [0]: # Read the data file 'Advertising.csv' df = pd . read_csv ( \"Advertising.csv\" ) # Use the column \"tv\" as the predictor x = df [[ 'tv' ]] # Use the column \"sales\" as the response y = df . sales . values In [0]: # Take a quick look at the data df . head () In [0]: # Initiate a Linear Regression model model = ___ # Fit the model on the predictor and response data ___ In [0]: # Take a quick look at the beta values got after model fitting # Use the model.intercept_ and model.coef_ for this b0 = ___ b1 = ___ print ( \"Beta0 is\" , b0 ) print ( \"Beta1 is\" , b1 ) In [0]: # Helper code to plot the true and predicted data plt . plot ( x , y , 'o' , label = \"True Data\" , color = 'darkblue' ) plt . plot ( x , model . predict ( df [[ 'tv' ]]), label = \"Prediction\" , color = 'coral' ) plt . xlabel ( \"TV\" ) plt . ylabel ( \"Sales\" ) plt . legend () In [0]: # Define 2 empty lists to store the accepted beta values in a list beta0_list = [] beta1_list = [] In [0]: # Initialize beta0 to a resonable value based on the model parameter seen above beta0 = ___ # Initialize beta1 to a resonable value based on the model parameter seen above beta1 = ___ In [0]: # Function to get the prior given the beta0 and beta1 values # NOTE - All the computations are done in the log space so that the numbers are managable. def get_log_prior ( beta0 , beta1 ): # The prior of beta0 is a value from a normal PDF of beta0 with mean as 100 and standard deviation as 50 # Take the log of these value log_prior_b0 = ___ # The prior of beta1 is a value from a normal PDF of beta1 with mean as 1 and standard deviation as 1 # Take the log of this value log_prior_b1 = ___ # Compute the prior as the sum of the log priors of beta0 and beta1 log_prior = ___ # Return the prior value return log_prior In [0]: # Compute the log-likelihood for the initial beta values # pay attention to the dimensions of y and x. log_likelihood = - np . sum ( np . log ( ( y . reshape ( - 1 , 1 ) - np . array ( beta1 * x + beta0 )) ** 2 ) ) # Get the prior of the intial beta values by calling the get_log_prior function log_prior = ___ # Compute the log posterior of the initial beta values # The posterior is the sum of the log_likelihood and log_prior log_posterior = ___ In [0]: # Save the initial posterior value as prev_posterior for comparision later prev_logposterior = log_posterior # Append the initial beta values i.e. beta0 and beta1 to the list beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) In [0]: # Specify the number of sampling \"epochs\" (less than 500k) epochs = ___ ‚è∏ How does the number of samples generated affect results of MCMC? A. As the number of samples are increased the beta values chosen grow increasing random. B. For a smaller number of samples the beta values are closer to the true value with reduced randomness. C. The number of samples does not affect the beta values, it only depends on the prior. D. As the number of samples increase, the beta values slowly converge to their true values. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' In [0]: # Loop over the range of sampling \"epochs\" for i in range ( epochs ): # Get a new beta1 value with mean as the latest element beta1 and scale as 0.1 beta0 = ___ # Get a new beta0 value with mean as the latest element beta0 and scale as 0.5 beta1 = ___ # Get the prior values for the new beta values by calling the get_log_prior function log_prior = ___ # Compute P(data|w) i.e. the log-likelihood for all the data points log_likelihood = ___ # To compute the posterior given the likelihood and prior # The posterior is the sum of the likelihood and prior log_posterior = ___ # Compute the the exponential of the ratio of the posterior given its previous value # Since it is the log, the ratio is computed as the difference between the values exp_ratio = ___ # If the ratio is greater than 1 then accept the new beta values in this case if exp_ratio > 1 : # Append the beta0 and beta1 to the beta list values beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) # Save the accepted posterior as the previous posterior prev_logposterior = log_posterior # If the ratio is less than 1 then get a random value between 0 and 1 else : coin = ___ # Set a threshold value threshold = ___ # Check if the random value is higher than the threshold # Append the beta values to the list and update the previous posterior if coin > threshold : beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) prev_logposterior = log_posterior ‚è∏ If the threshold is set to a higher value, new beta values are rejected more often if they do not improve the convergence to the true value The statement above is: A. True for all cases B. False for all cases C. True only when the number of samples is less D. True only when prior is extremely far from the real value In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer2 = '___' In [0]: # The number of data points to consider after the beta list has been populated burn_rate = int ( len ( beta0_list ) * 0.3 ) In [0]: ### edTest(test_chow3) ### # Check posterior mean for beta0 and beta1 print ( np . mean ( beta0_list [ burn_rate :]), np . mean ( beta1_list [ burn_rate :])) In [0]: # Helper code to plot the histogram of the beta values # Plot the histogram of the beta0 values fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 6 )) ax1 . hist ( beta0_list [ burn_rate :], color = '#B2D7D0' , edgecolor = \"black\" , linewidth = 1 ) ax1 . set_xlabel ( \"BETA 0\" , fontsize = 14 ) ax1 . set_ylabel ( \"FREQUENCY\" , fontsize = 14 ); # Plot the histogram of the beta1 values ax2 . hist ( beta1_list [ burn_rate :], color = '#EFAEA4' , edgecolor = \"black\" , linewidth = 1 ) ax2 . set_xlabel ( \"BETA 1\" , fontsize = 14 ) ax2 . set_ylabel ( \"FREQUENCY\" , fontsize = 14 );",
        "tags": "lectures",
        "url": "lectures/lecture27/notebook1/"
    }, {
        "title": "Lecture 27: œÑ AE the Bayesian Approach",
        "text": "Title MCMC from Scratch for Neural Networks Description : The aim of this exercise is to perform Monte Carlo Markov Chain (MCMC) from scratch for a simple neural network. On completing the exercise you should be able to see a distribution similar to the following. One for each network parameter: Instructions: Read the data file backprop.csv and set the predictor and response variables. Create 3 lists to store the weights and bias (i.e. the network parameters) and initialize the parameter values. Define a function get_log_prior to compute the prior value given the network parameter values. Compute the likelihood, prior and posterior for the initial parameter values. For a selected number of sampling \"epochs\": Compute new weights and bias. Compute the corresponding likelihood, prior and posterior. Compute the exponential ratio of the current and previous posterior. Based on the ratio, select or reject the new parameter values. Choose a burn rate. Plot the histogram of the weights and bias. Hints: np.log() Computes the natural logarithm, element-wise. np.exp() Calculates the exponential of all elements in the input array. tf.reshape() Reshapes a tensor. .fit() Fits the linear model to the data. np.random.normal() Draw random samples from a normal (Gaussian) distribution. norm.pdf() A normal continuous random variable. np.sum() Sum of array elements over a given axis. np.random.uniform() Draw samples from a uniform distribution. np.zeros() Return a new array of given shape and type, filled with zeros. In [0]: # Import necessary libraries import matplotlib.pyplot as plt from scipy.stats import norm from random import sample import tensorflow as tf tf . random . set_seed ( 42 ) import pandas as pd import numpy as np % matplotlib inline dtype = 'float32' In [0]: # Read the data file \"backprop.csv\" df = pd . read_csv ( \"backprop.csv\" ) # Take a quick look at the data df . head () In [0]: # Get the predictor and response data X_data = df . iloc [:, 0 ] y_data = df . iloc [:, 1 ] In [0]: # Helper code to visualize the data plt . figure ( figsize = ( 4 , 6 )) plt . scatter ( X_data , y_data , color = 'g' , s = 20 , alpha = 0.5 , label = 'sample data' ) plt . xlabel ( 'X' , fontsize = 14 ); plt . ylabel ( 'Y' , fontsize = 14 ) plt . subplots_adjust ( left = 0.0 , bottom = 0.0 , right = 2.0 , top = 1.0 , wspace = 0.2 , hspace = 0.2 ) plt . legend () plt . show () In [0]: # Convert the predictor and response variables to tensor data x = tf . convert_to_tensor ( X_data ) x = tf . reshape ( x ,( - 1 , 1 )) y = tf . convert_to_tensor ( y_data ) y = tf . reshape ( y , ( - 1 , 1 )) In [0]: # Function to define the neural network model # The network has 2 hidden nodes and one output node # We use sin activation for this exercise # The network has a total of 5 parameters - 4 weights and one bias for the output def basic_nn ( w0 , w1 , b1 , x = x ): h1 = tf . matmul ( x , w0 ) a1 = tf . math . sin ( h1 ) h2 = tf . matmul ( a1 , w1 ) + b1 y = tf . math . sin ( h2 ) return y In [0]: # Define 3 empty lists to store the accepted network parameters # The weights0_list will contain 2 weights that connects the input # to the hidden layer weights0_list = [] # The weights1_list will contain 2 weights that connects the hidden # nodes to the output weights1_list = [] # The bias_list will hold the bias added to the output of the hidden nodes bias_list = [] In [0]: # Initialize the input to hidden weights randomly from a normal distribution # with mean=(1,1) and standard deviation=(1,1) # Reshape the values to shape (1,2) weights0 = tf . reshape ( np . random . normal ( loc = ( - 0.4 , 12 ), scale = ( 0.1 , 0.1 ), size = ( 1 , 2 )), shape = ( 1 , 2 )) # Initialize the hidden to output weights randomly from a normal distribution # with mean=(-0.4,12) and standard deviation=(0.1,0.1) # Reshape the values to shape (2,1) weights1 = tf . reshape ( np . random . normal ( loc = ( 0.6 , 0 ), scale = ( 0.1 )), shape = ( 2 , 1 )) # Initialize the bias randomly from a normal distribution # with mean=1 and standard deviation=1 bias = np . random . normal ( loc = 2.5 , scale = 0.1 ) In [0]: # Function to get the prior given the network parameters # NOTE - All the computations are done in the log space so # that the numbers are managable. def get_log_prior ( weights0 , weights1 , bias ): # Initialize a numpy array of zeros with shape 2,1 prior_w0 = ___ # Find the probability of the first weight given the normal PDF with mean =1 and std=1 # Take the log of this value prior_w0 [ 0 ] = np . log ( norm . pdf ( weights0 [ 0 ][ 0 ], - 0.4 , 0.1 )) # Find the probability of the second weight given the normal PDF with mean =1 and std=1 # Take the log of this value prior_w0 [ 1 ] = np . log ( ___ ) # Initialize a numpy array of zeros with shape 2,1 prior_w1 = ___ # Find the probability of the third weight given the normal PDF with mean =0.6 and std=0.1 # Take the log of this value prior_w1 [ 0 ] = np . log ( ___ ) # Find the probability of the first weight given the normal PDF with mean =0 and std=0.1 # Take the log of this value prior_w1 [ 1 ] = np . log ( ___ ) # Find the probability of the bias given the normal PDF with mean=2.5 and std=0.1 # Take the log of this value prior_bias = np . log ( ___ ) # Compute the prior as the sum of the previously computed priors log_prior = ___ # Return the prior value return log_prior In [0]: # Get the prior of the initial network parameters by calling the get_log_prior function log_prior = ___ # Get the network predictions by calling the basic_nn function ypred = ___ # Compute the -ve log likelihood given the true y and predicted y log_likelihood = - np . sum ((( y - ypred ) ** 2 )) # Compute the posterior as the sum of the likelihood and prior posterior = ___ # Save the current posterior value as prev_posterior for comparision later prev_posterior = ___ # Append weights0 to the weights0_list weights0_list . append ( weights0 ) # Append weights1 to the weights1_list weights1_list . append ( weights1 ) # Append bias to the bias_list bias_list . append ( bias ) In [0]: # Specify the number of sampling \"epochs\". Set it to atleast 10000 epochs = ___ # Loop over the range of sampling \"epochs\" for i in range ( epochs ): if i % 5000 ==0: print ( \"EPOCH: \" , i ) # Get the candidate input to hidden weights randomly from a normal distribution # with mean as the last element added to weights0_list and standard deviation=(0.1,0.1) # Reshape the values to shape (1,2) weights0 = tf . reshape ( np . random . normal ( loc = weights0_list [ - 1 ], scale = ( 0.1 , 0.1 )), shape = ( 1 , 2 )) # Get the candidate hidden to output weights randomly from a normal distribution # with mean as the last element added to weights1_list and standard deviation=1 # Reshape the values to shape (2,1) weights1 = tf . reshape ( ___ , shape = ( 2 , 1 )) # Get the candidate bias randomly from a normal distribution # with mean as the last element added to bias_list and standard deviation=1 bias = ___ # Get the prior values for the candidate values by calling the get_log_prior function log_prior = ___ # Get the network predictions by calling the basic_nn function with the candidate values ypred = ___ # Compute P(data|w) i.e. the log-likelihood given the true y and predicted y log_likelihood = ___ # To compute the posterior given the likelihood and prior # The posterior is the sum of the likelihood and prior posterior = ___ # Compute the the exponential of the ratio of the posterior given its previous value exp_ratio = ___ # If the ratio is greater than or equal to 1 then accept the candidate values in this case if exp_ratio >= 1 : # Append the candidate values to the weights and bias list weights0_list . append ( weights0 ) weights1_list . append ( weights1 ) bias_list . append ( bias ) # Save the accepted posterior as the previous posterior prev_posterior = posterior # If the ratio is less than 1 then get a random value between 0 and 1 else : coin = ___ # Set a threshold value threshold = 0.98 # Check if the random value is higher than the threshold # Append the candidate values to the list and update the previous posterior if coin > threshold : weights0_list . append ( weights0 ) weights1_list . append ( weights1 ) bias_list . append ( bias ) prev_posterior = posterior In [0]: # The number of data points to consider after the beta list has been populated burn_rate = int ( len ( bias_list ) * 0.2 ) In [0]: # Helper code to plot the distribution of network parameter fig , ax = plt . subplots ( 5 , 2 , figsize = ( 15 , 15 )) col1 = \"#B2D7D0\" col2 = \"#EFAEA4\" fs = 12 with plt . xkcd ( scale = 0.01 ): ax [ 0 ][ 0 ] . hist ( np . array ( weights0_list )[:, 0 , 0 ], color = col1 , linewidth = 1.2 , edgecolor = 'black' ) ax [ 0 ][ 0 ] . set_xlabel ( \"Weight 1\" ) ax [ 0 ][ 0 ] . set_ylabel ( \"Frequency\" ) ax [ 0 ][ 1 ] . plot ( np . array ( weights0_list )[:, 0 , 0 ], color = col2 ) ax [ 0 ][ 1 ] . set_xlabel ( \"Weight 1\" ) ax [ 0 ][ 1 ] . set_title ( \"CHAIN\" , fontsize = 14 ) ax [ 1 ][ 0 ] . hist ( np . array ( weights0_list )[:, 0 , 1 ], color = col1 , linewidth = 1.2 , edgecolor = 'black' ) ax [ 1 ][ 0 ] . set_xlabel ( \"Weight 2\" ) ax [ 1 ][ 0 ] . set_ylabel ( \"Frequency\" ) ax [ 1 ][ 1 ] . plot ( np . array ( weights0_list )[:, 0 , 1 ], color = col2 ) ax [ 1 ][ 1 ] . set_xlabel ( \"Weight 2\" ) ax [ 2 ][ 0 ] . hist ( np . array ( weights1_list )[:, 0 , 0 ], color = col1 , linewidth = 1.2 , edgecolor = 'black' ) ax [ 2 ][ 0 ] . set_xlabel ( \"Weight 3\" ) ax [ 2 ][ 0 ] . set_ylabel ( \"Frequency\" ) ax [ 2 ][ 1 ] . plot ( np . array ( weights1_list )[:, 0 , 0 ], color = col2 ) ax [ 2 ][ 1 ] . set_xlabel ( \"Weight 3\" ) ax [ 3 ][ 0 ] . hist ( np . array ( weights1_list )[:, 1 , 0 ], color = col1 , linewidth = 1.2 , edgecolor = 'black' ) ax [ 3 ][ 0 ] . set_xlabel ( \"Weight 4\" ) ax [ 3 ][ 0 ] . set_ylabel ( \"Frequency\" ) ax [ 3 ][ 1 ] . plot ( np . array ( weights1_list )[:, 1 , 0 ], color = col2 ) ax [ 3 ][ 1 ] . set_xlabel ( \"Weight 4\" ) ax [ 4 ][ 0 ] . hist ( np . array ( bias_list ), color = col1 , linewidth = 1.2 , edgecolor = 'black' ) ax [ 4 ][ 0 ] . set_xlabel ( \"Bias\" ) ax [ 4 ][ 0 ] . set_ylabel ( \"Frequency\" ) ax [ 4 ][ 1 ] . plot ( np . array ( bias_list ), color = col2 ) ax [ 4 ][ 1 ] . set_xlabel ( \"Bias\" ) plt . tight_layout (); ‚è∏ Go back and change the mean and standard deviation of weights1 while intializing to 0 and 1 respectively. What change do you notice in the result distribution? In [0]: ### edTest(test_chow1) ### # Type your answer in the space given below answer1 = '___' ‚è∏ How is the distribution affected if mean and standard deviation of prior_w1[0] and prior_w1[0] are set to 0 and 1? In [0]: ### edTest(test_chow2) ### # Type your answer in the space given below answer2 = '___' ‚è∏ For each network parameter, what affect does increase in scale of the prior from 0.1 to 10 expect to bring out, given you start from a prior that is very close to the true value? A. With a wider range of possible values, the parameters would converge to the true values faster. B. The prior is a constant, and hence does not affect the the parameter convergence. C. The parameters will take longer to converge to their true values as the values bounce around more. D. The parameter only depends on the mean and previous prior, not the standard deviation. Hence, there would be no affect with a change in the standard devidation. In [0]: ### edTest(test_chow3) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer3 = '___' In [0]: # Helper code to visualize the prediction by taking the mean of the network parameters wl = np . array ( weights0_list [ burn_rate :]) wl2 = np . array ( weights1_list [ burn_rate :]) bi = np . array ( bias_list [ burn_rate :]) # Take the mean of the model parameters w0 = np . mean ( wl [:, 0 ,:], axis = 0 ) . reshape ( 1 , 2 ) w1 = np . mean ( wl2 [:,:, 0 ], axis = 0 ) w1 = tf . reshape ( tf . cast ( w1 , dtype = 'float32' ), shape = ( 2 , 1 )) b1 = np . mean ( bi ) # Get the network prediction h1 = tf . matmul ( tf . cast ( x , dtype = 'float32' ), w0 ) a1 = tf . math . sin ( h1 ) h2 = tf . matmul ( a1 , w1 ) + b1 y_pred = tf . math . sin ( h2 ) # Plot the true data and model prediction plt . plot ( X_data , y_data , 'b+' , label = \"True Data\" ) plt . plot ( X_data , y_pred , 'ro' , label = \"Prediction\" ) plt . legend ()",
        "tags": "lectures",
        "url": "lectures/lecture27/notebook2/"
    }, {
        "title": "Lecture 26: ‚çµ AutoEncoder (AE)",
        "text": "Slides Lecture 26: Autoencoders Part A (PDF) Lecture 26: Autoencoders Part B (PDF) Exercises Lecture 26: Introduction to Autoencoders using MNIST (Notebook) Lecture 26: Recreating image of Pavlos (Notebook) Lecture 26: Autoencoders on Iris data (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture26/"
    }, {
        "title": "Lecture 26: ‚çµ AutoEncoder (AE)",
        "text": "Title Introduction to Autoencoders using MNIST Description : The goal of the exercise is to use an autoencoder to first compress hand-written digit images from the MNIST dataset down to lower-dimensional representations and then expand them back to the original images. Your final output will look similar to the image below: Instructions: Load the MNIST dataset with the mnist.load_data() function provided by keras. Load the images in two separate lists, x_train and x_test . Create the first part of the autoencoder - the encoder model. The encoder model compresses the input image down to a lower dimensional latent space Next create the 2nd half of the autoencoder - the decoder. The decoder expands an image representation in the latent space back to the full dimensions of the original input image. Normalize your data by dividing each pixel by 255. Finally, we combine the encoder and decoder into the autoencoder. The autoencoder shrinks the image down to the latent space representation and then expands it again to the original dimensions. Visualize the model predictions on the test set after every epoch using the helper code given. You can experiment with the different latent_size , layer_size and regularization . Hints: More on Keras Functional API here . keras.compile() Compiles the layers into a network. keras.Sequential() Models a sequential neural network. keras.Dense() A regular densely-connected NN layer. layers.Flatten() Flattens the input. Does not affect the batch size. tf.keras.Input() Used to instantiate a Keras tensor. NOTE: To keep things simple we will use dense layers, so no convolutions here. In [0]: # import required libraries from tensorflow.keras.datasets import mnist from tensorflow.keras.layers import Dense , Input , Flatten , Reshape from tensorflow.keras import models from tensorflow.keras.models import Model , Sequential from tensorflow.keras import layers from matplotlib import pyplot as plt from IPython import display import numpy as np % matplotlib inline In [0]: ### edTest(test_normalize) ### # First we load in the MNIST dataset. # Do not fill in the blank space ( x_train , _ ), ( x_test , _ ) = mnist . load_data () # We will only take 4000 data points from the original dataset to demonstrate the autoencoders sample_size = 4000 x_train = x_train [: sample_size ] x_test = x_test [: sample_size ] # We normalize the pixel data (i.e divide by 255) x_train = ___ x_test = ___ # We print image dimensions to confirm print ( f 'image shape: { x_train [ 0 ] . shape } and random pixel value is { x_train [ 0 ][ 20 ][ 10 ] } ' ) # We also plot example image from x_train plt . imshow ( x_train [ 0 ], cmap = \"gray\" ) plt . show () In [0]: ### edTest(test_model_encoder) ### # Now we create the encoder model to take compress each image down to a lower dimensional latent space. # pick a size for the latent dimension like 32 latent_size = ___ # Note how sequential models can also be passed a list of layers # This can be more concise than using add() model_1 = models . Sequential ( name = 'Encoder' ) # add a flatten layer to convert image of size (28,28) to 784 # don't forget to include the `input_shape` argument model_1 . add ( ___ ) # add a dense layer with 128 neurons model_1 . add ( ___ ) # add another dense layer with 64 neurons model_1 . add ( ___ ) # Finally add the last dense layer with latent_size number of neurons model_1 . add ( ___ ) # Take a quick look at the model summary model_1 . summary () In [0]: ### edTest(test_model_decoder) ### # Now we create the decoder model to take compress each image down to a lower dimensional latent space. model_2 = models . Sequential ( name = 'Decoder' ) # add a dense layer with 64 neurons model_2 . add ( ___ ) # add a dense layer with 128 neurons model_2 . add ( ___ ) # add a dense layer with 784 neurons and especially choose an appropriate activation function model_2 . add ( ___ ) # finally reshape it back to size 28,28 model_2 . add ( ___ ) # Take a quick look at the model summary model_2 . summary () In [0]: ### edTest(test_model_autoencoder) ### # To build autoencoders, we will use the keras 'functional api' # read more here -> https://www.tensorflow.org/guide/keras/functional # define an input of the dimension of the image img = Input ( shape = ( 28 , 28 )) # Use the 'encoder' i.e model_1 from above to get a variable `latent_vector` latent_vector = model_1 ( ___ ) # Use the 'decoder' i.e model_2 from above to get the output variable output = model_2 ( ___ ) # using functional api to define autoencoder model autoencoder = Model ( inputs = ___ , outputs = ___ ) # choose an appropriate loss function for 'reconstruction error' and optimizer = nadam autoencoder . compile ( ___ ) # Take a quick look at the model summary autoencoder . summary () In [0]: # # You can train for 10 or more epochs to see how well our autoencoder model performs epochs = 10 for i in range ( epochs + 1 ): # Note: epoch 0 is before any fitting fig , axs = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sample_x = x_test [ np . random . choice ( x_test . shape [ 0 ])] axs [ 0 ] . imshow ( sample_x , cmap = \"gray\" ) axs [ 0 ] . set_title ( 'Test image' , fontsize = 16 ) axs [ 1 ] . imshow ( autoencoder . predict ( sample_x . reshape ( 1 , 28 , 28 ))[ 0 ], cmap = \"gray\" ) axs [ 1 ] . set_title ( 'Autoencoder Prediction' , fontsize = 16 ); fig . suptitle ( f 'Autoencoder recreation after epoch number { i } ' , fontsize = 14 ) plt . show () # specify predictors and targets for train and validation and train for an epoch autoencoder . fit ( x = x_train , y = x_train , validation_data = ( x_test , x_test )) Mindchow üç≤ Go back and change the latent_space dimension to a lower value like 2. Does your autoencoder's reconstructions become better or worse? Why? Your answer here",
        "tags": "lectures",
        "url": "lectures/lecture26/notebook1/"
    }, {
        "title": "Lecture 26: ‚çµ AutoEncoder (AE)",
        "text": "Title Autoencoders on Iris data Description : The goal of this exercise is to visualize the latent-space for the autoencoder trained on the IRIS dataset. Instructions: Load the IRIS dataset with the load_iris() function provided by keras. Load the predictors as the variable X and the targets as the variable y . Make a basic autoencoder model (Encoder - Decoder) as follows: Map encoder to a latent (hidden) space - input dimension is 4 and output dimension is 2. Use the decoder to reconstruct - input dimension is 2 and output dimension is 4. Make the final autoencoder model with the help of the keras functional API Compile the model with an appropriate optimizer and loss . Train the model for several epochs and save the training into a variable history . Plot the loss and validation_loss over the epochs. Finally plot the latent space representation along with the clusters using the plot3clusters() function. This plot will look similar to the one given above. Hints: keras.compile() Compiles the layers into a network. keras.Sequential() Models a sequential neural network. keras.Dense() A regular densely-connected NN layer. tf.keras.Input() Used to instantiate a Keras tensor. In [1]: # Import the necessary libraries import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import MinMaxScaler from sklearn.decomposition import PCA from matplotlib import pyplot as plt % matplotlib inline import tensorflow as tf In [2]: # Run this cell for more readable visuals large = 22 ; med = 16 ; small = 10 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'axes.linewidth' : 2 , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . style . use ( 'seaborn-white' ) plt . rcParams . update ( params ) % matplotlib inline In [63]: #Load Iris Dataset iris = load_iris () # Get the predictor and response variables X = iris . data y = iris . target # Get the Iris label names target_names = iris . target_names print ( X . shape , y . shape ) # Standardize the data scaler = MinMaxScaler () X_scaled = scaler . fit_transform ( X ) (150, 4) (150,) In [81]: # Helper function to plot the data as clusters # based on the iris species label def plot3clusters ( X , title , vtitle ): plt . figure () # Select the colours of the clusters colors = [ '#A43F98' , '#5358E0' , '#DE0202' ] lw = 2 plt . figure ( figsize = ( 9 , 7 )); for color , i , target_name in zip ( colors , [ 0 , 1 , 2 ], target_names ): plt . scatter ( X [ y == i , 0 ], X [ y == i , 1 ], color = color , alpha = 1. , lw = lw , label = target_name ); plt . legend ( loc = 'best' , shadow = False , scatterpoints = 1 ) plt . title ( title ); plt . xlabel ( vtitle + \"1\" ) plt . ylabel ( vtitle + \"2\" ) plt . show (); In [0]: ### edTest(test_check_ae) ### # Create an AE and fit it with our data using 2 neurons in the dense layer # using keras' functional API # Get the number of data samples i.e. the number of columns input_dim = ___ output_dim = ___ # Specify the number of neurons for the dense layers encoding_dim = ___ # Specify the input layer input_features = tf . keras . Input ( ___ ) # Add a denser layer as the encode layer following the input layer # with 2 neurons and no activation function encoded = tf . keras . layers . Dense ( ___ )( input_features ) # Add a denser layer as the decode layer following the encode layer # with output_dim as a parameter and no activation function decoded = tf . keras . layers . Dense ( ___ )( encoded ) # Create an autoencoder model with # input as input_features and outputs decoded autoencoder = tf . keras . Model ( ___ , ___ ) # Complile the autoencoder model autoencoder . compile ( ___ ) # View the summary of the autoencoder autoencoder . summary () In [0]: # Use the helper function to plot the model history # Get the history of the model to plot history = autoencoder . fit ( X_scaled , X_scaled , epochs = ___ , batch_size = 16 , shuffle = ___ , validation_split = 0.1 , verbose = 0 ) # Plot the loss plt . plot ( history . history [ 'loss' ], color = '#FF7E79' , linewidth = 3 , alpha = 0.5 ) plt . plot ( history . history [ 'val_loss' ], color = '#007D66' , linewidth = 3 , alpha = 0.4 ) plt . title ( 'Model train vs Validation loss' ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . legend ([ 'Train' , 'Validation' ], loc = 'best' ) plt . show () In [0]: # Create a model which has input as input_features and # output as encoded encoder = tf . keras . Model ( ___ , ___ ) # Predict on the entire data using the encoder model, # remember to use X_scaled encoded_data = encoder . predict ( ___ ) # Call the function plot3clusters to plot the predicted data # using the encoded layer plot3clusters ( encoded_data , 'Encoded data latent-space' , 'dimension ' ); Mindchow üç≤ Go back and train for more epochs. Does your latent-space distinguish between the plant types better? Your answer here",
        "tags": "lectures",
        "url": "lectures/lecture26/notebook2/"
    }, {
        "title": "Lecture 26: ‚çµ AutoEncoder (AE)",
        "text": "Title Recreating image of Pavlos Description : The aim of this exercise is to understand image reconstruction using autoencoders. Instructions: Load the 3 images given. Use the helper code to reshape and flatten the images. Create an autoencoder by defining the encode and decode layers. Fit on the image of Pavlos. Visualise the train and validation loss. This will look similar to the image given above. Go through the reconstruction part carefully and understand what is happening in each step. There is no code you will have to fill in this part. Reconstruction Description: The reconstruction part of this exercise tries to recreate the input image given. It is important to remember that our model has learnt from only one image i.e. the one of Pavlos. In the first section of this part, we give Pavlos's image as the input and the model recreates and outputs the correct image with very little noise. This looks similar to the image given below. We then give the image of an eagle as the input, however, the output is still Pavlos! The noise is represented in the 3 part of the image given below. Finally, we try to get a different output image by giving the input image of a human. Pavlos triumphs again. Hints: keras.compile() Compiles the layers into a network. keras.Sequential() Models a sequential neural network. keras.Dense() A regular densely-connected NN layer. In [0]: # Import the libraries import tensorflow as tf import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import MinMaxScaler from matplotlib import pyplot as plt % matplotlib inline from tensorflow.keras.models import Sequential , Model import tensorflow.keras as keras from tensorflow.keras import layers , Model , Input from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam from PIL import Image import scipy.ndimage as ndi In [0]: # Loading the 3 images pavlos_img_ptr = np . array ( Image . open ( \"pavlos.jpeg\" )) notpavlos_img_ptr = np . array ( Image . open ( \"not-pavlos.jpeg\" )) notpavlos2_img_ptr = np . array ( Image . open ( \"not-pavlos2.jpg\" )) In [0]: # Helper function to re-size the images def img_resize ( imgs_in , factor ): imgs_out_train = ndi . zoom ( imgs_in , ( 1 , factor , factor , 1 ), order = 2 ) return imgs_out_train In [0]: # Use the helper code to reduce image size to 100x100 SIZE = 100 pavlos_img_ptr = pavlos_img_ptr [:,:, 2 ] . reshape ( 1 , 150 , 150 , 1 ) pavlos_img_ptr = img_resize ( pavlos_img_ptr , SIZE / pavlos_img_ptr . shape [ 1 ]) pavlos_img_nice = pavlos_img_ptr notpavlos_img_ptr = notpavlos_img_ptr [:,:, 2 ] . reshape ( 1 , 132 , 132 , 1 ) notpavlos_img_ptr = img_resize ( notpavlos_img_ptr , SIZE / notpavlos_img_ptr . shape [ 1 ]) notpavlos_img_nice = notpavlos_img_ptr notpavlos2_img_ptr = notpavlos2_img_ptr [:,:, 2 ] . reshape ( 1 , 100 , 100 , 1 ) notpavlos2_img_ptr = img_resize ( notpavlos2_img_ptr , SIZE / notpavlos2_img_ptr . shape [ 1 ]) notpavlos2_img_nice = notpavlos2_img_ptr # Flatten the images pavlos_flatten = pavlos_img_nice . reshape ( 100 * 100 , 1 ) print ( pavlos_flatten . shape ) notpavlos_flatten = notpavlos_img_nice . reshape ( 100 * 100 , 1 ) print ( notpavlos_flatten . shape ) notpavlos2_flatten = notpavlos2_img_nice . reshape ( 100 * 100 , 1 ) print ( notpavlos2_flatten . shape ) Create model and train In [0]: ### edTest(test_check) ### # Create an Autoencoder and fit it with our data using # 8 neurons in the dense layer using keras' functional API # Get the input size from the shape of the flattened image input_dim = pavlos_flatten . shape [ 0 ] encoding_dim = 8 # Create an input \"layer\" using input_dim as a parameter input_section = ___ # Create a denser layer as the encode layer with 8 neurons and linear activation encoded = ___ # Create an autoencoder model which has input as input_section and outputs encoded encoder = ___ # Decoder # Create an input \"layer\" using encoding_dim as shape latent_input = ___ # Create a denser layer as the encode layer with input_dim and linear activation decoded = ___ # Create a model which has input as latent_input and outputs decoded decoder = ___ In [0]: ### edTest(test_architecture) ### # Create an autoencoder using keras Sequential autoencoder = ___ # Add the encoder followed by the decoder initialised above to the autoencoder model autoencoder . ___ autoencoder . ___ # Compile the model with mse as the loss and Adam optimizer with parameter 0.001 autoencoder . ___ # Take a look at the summary of the model autoencoder . summary () In [0]: # Get the history of the model by fitting on pavlos_flatten after reshape # Specify 100 epochs and batch size of 1000 with verbose=1 # keras expects a shape of (1,n) in the case of a flattened input. history = ___ In [0]: # Use the helper function to plot the loss plt . plot ( np . log ( history . history [ 'loss' ]), linewidth = 2 , color = 'darkblue' ) plt . title ( 'Epochs vs Training loss' ) plt . ylabel ( 'Log loss' ) plt . xlabel ( 'Epoch' ) plt . legend ([ 'Train' ], loc = 'best' ) plt . show () Reconstruct Pavlos In [0]: ### Reconstruct Pavlos pavlos_flatten_reconstructed = autoencoder ( pavlos_flatten . reshape ( - 1 , input_dim )) . numpy () pavlos_reconstructed = pavlos_flatten_reconstructed . reshape ( 100 , 100 ) In [0]: # Helper code to display the images fig , ax = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 )) ax [ 0 ] . imshow ( pavlos_img_nice . reshape ( 100 , 100 ), cmap = 'gray' ) ax [ 0 ] . set_title ( 'Original' ) ax [ 0 ] . axis ( 'off' ) ax [ 1 ] . imshow ( pavlos_reconstructed , cmap = 'gray' ) ax [ 1 ] . set_title ( 'Recon' ) ax [ 1 ] . axis ( 'off' ) ax [ 2 ] . imshow ( pavlos_img_nice . reshape ( 100 , 100 ) - pavlos_reconstructed , cmap = 'gray' ); ax [ 2 ] . axis ( 'off' ); Reconstruct Eagle In [0]: ### Reconstruct eagle notpavlos_flatten_reconstructed = autoencoder ( notpavlos_flatten . reshape ( - 1 , 10000 )) . numpy () notpavlos_reconstructed = notpavlos_flatten_reconstructed . reshape ( 100 , 100 ) In [0]: # Helper code to display the images fig , ax = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 )) ax [ 0 ] . imshow ( notpavlos_img_nice . reshape ( 100 , 100 ), cmap = 'gray' ) ax [ 0 ] . set_title ( 'Eagle - original (A)' ) ax [ 0 ] . axis ( 'off' ) ax [ 1 ] . imshow ( notpavlos_reconstructed , cmap = 'gray' ) ax [ 1 ] . set_title ( 'Eagle - Recon (B)' ) ax [ 1 ] . axis ( 'off' ) ax [ 2 ] . imshow ( notpavlos_img_nice . reshape ( 100 , 100 ) - notpavlos_reconstructed , cmap = 'gray' ) ax [ 2 ] . set_title ( 'A - B' ) ax [ 2 ] . axis ( 'off' ); Reconstruct Not Pavlos In [0]: ### Reconstruct an image that is not of Pavlos notpavlos2_flatten_reconstructed = autoencoder ( notpavlos2_flatten . reshape ( - 1 , input_dim )) . numpy () notpavlos2_reconstructed = notpavlos2_flatten_reconstructed . reshape ( 100 , 100 ) In [0]: # Helper code to display the images fig , ax = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 )) ax [ 0 ] . imshow ( notpavlos2_img_nice . reshape ( 100 , 100 ), cmap = 'gray' ) ax [ 0 ] . set_title ( 'Marios - Not Pavlos \\n original (A)' ) ax [ 0 ] . axis ( 'off' ) ax [ 1 ] . imshow ( notpavlos2_reconstructed , cmap = 'gray' ) ax [ 1 ] . set_title ( 'Marios - Not Pavlos \\n Recon (B)' ) ax [ 1 ] . axis ( 'off' ) ax [ 2 ] . imshow ( notpavlos2_img_nice . reshape ( 100 , 100 ) - notpavlos_reconstructed , cmap = 'gray' ) ax [ 2 ] . set_title ( 'A - B' ) ax [ 2 ] . axis ( 'off' ); Mindchow üç≤ Go back and decrease the number of epochs to see when the reconstruction starts getting grainy. Your answer here",
        "tags": "lectures",
        "url": "lectures/lecture26/notebook3/"
    }, {
        "title": "Lecture 25: ü§ñ (Transformers II) NLP 4/4",
        "text": "Slides Lecture 25: Transformers (PDF) Exercises Lecture 25: Self-Attention (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture25/"
    }, {
        "title": "Lecture 25: ü§ñ (Transformers II) NLP 4/4",
        "text": "Title Exercise: Self-Attention Description : In this exercise, you will implement a Self-Attention Head for the 3rd word of a 4-word input. From a Pickled file, we load query , key , and value vectors that corresponds to 4 different inputs (thus, a total of 12 vectors). Specifically, this is loaded into 3 respective dicts . You only need to calculate the final z \"context\" vector that corresponds to the 3rd word (i.e., z2 ). **REMINDER** : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. In [1]: # imports useful libraries import math import pickle import numpy as np YOU DO NOT NEED TO EDIT THE CELL BELOW The follow code loads the queries , keys , and values vectors that correspond to 4 distinct words. Here, all vectors have a length of 25. Each of these variables (e.g., queries ) is a dict , indexed by the word number. For example queries[2] corresponds to the 3rd word (it's 0-indexed), and its value is a list of length 25, which corresponds to the actual query vector. In [2]: pickled_content = pickle . load ( open ( \"L25.p\" , \"rb\" )) queries , keys , values = [ pickled_content [ i ] for i in range ( 3 )] # to illustrate, let's print the query, key, and value vectors that correspond to teh 3rd word in the sentence: print ( \"query vector for 3rd word:\" , queries [ 2 ]) print ( \" \\n key vector for 3rd word:\" , keys [ 2 ]) print ( \" \\n value vector for 3rd word:\" , values [ 2 ]) query vector for 3rd word: [0.15272, 0.36181, -0.22168, 0.066051, 0.13029, 0.37075, -0.75874, -0.44722, 0.22563, 0.10208, 0.054225, 0.13494, -0.43052, -0.2134, 0.56139, -0.21445, 0.077974, 0.10137, -0.51306, -0.40295, 0.40639, 0.23309, 0.20696, -0.12668, -0.50634] key vector for 3rd word: [0.88387, -0.14199, 0.13566, 0.098682, 0.51218, 0.49138, -0.47155, -0.30742, 0.01963, 0.12686, 0.073524, 0.35836, -0.60874, -0.18676, 0.78935, 0.54534, 0.1106, -0.2923, 0.059041, -0.69551, -0.18804, 0.19455, 0.32269, -0.49981, 0.306] value vector for 3rd word: [0.30045, 0.25006, -0.16692, 0.1923, 0.026921, -0.079486, -0.91383, -0.1974, -0.053413, -0.40846, -0.26844, -0.28212, -0.5, 0.1221, 0.3903, 0.17797, -0.4429, -0.40478, -0.9505, -0.16897, 0.77793, 0.33525, 0.3346, -0.1754, -0.12017] YOU DO NOT NEED TO EDIT THE CELL BELOW In [3]: # returns the dot product of two passed-in vectors def calculate_dot_product ( v1 , v2 ): return sum ( a * b for a , b in zip ( v1 , v2 )) In the cell below, populate the self_attention_scores list by calculating the four Attention scores that correspond to the 3rd word . Each Attention score should be divided by $\\sqrt{d_k}$ (where $d_k$ represents the length of the key vector), and the ordering should be natural. That is, the 1st item in self_attention_scores should correspond to the Attention score for the 1st word, the 2nd item in self_attention_scores should correspond to the Attention score for 2nd word, and so on. In [4]: ### edTest(test_a) ### self_attention_scores = [] # YOUR CODE HERE In the cell below, populate the softmax list by calculating the softmax of each of the four Attention scores found in the previous cell. In [5]: ### edTest(test_b) ### softmax_scores = [] # YOUR CODE HERE In the cell below, create the final $z2$ list that corresponds to the 3rd word. $z2$ should have a length of 25. In [6]: ### edTest(test_c) ### z2 = [] # YOUR CODE HERE",
        "tags": "lectures",
        "url": "lectures/lecture25/notebook/"
    }, {
        "title": "Advanced Section 3: Word Embeddings",
        "text": "Slides Word Embeddings [PDF]",
        "tags": "a-sections",
        "url": "a-sections/a-sec03/"
    }, {
        "title": "Lecture 24: üß† Attention (Transformers I) NLP 3/4",
        "text": "Slides Lecture 24: Attention (PDF) Exercises Lecture 24: Attention (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture24/"
    }, {
        "title": "Lecture 24: üß† Attention (Transformers I) NLP 3/4",
        "text": "Title Exercise: Attention Description : In this exercise, you will implement an Attention mechanism. We load three encoder hidden states into enc_states , and 1 decoder hidden state into dec_state . Your task is to compute the final context_vector . That is, you should calculate an Attention score for every encoder hidden state, exponentiate these, then normalize them so they sum to 1. These are your Attention weights. Then, produce a context vector by multiplying each Attention weight by its corresponding encoder hidden state. **REMINDER** : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. In [63]: # imports useful libraries import math import numpy as np YOU DO NOT NEED TO EDIT THE CELL BELOW The follow code loads three encoder states into the dictionary enc_states , whereby the keys are 0, 1, and 2, and their respective values are lists of 50 floats (representing each hidden state). The code also populates a single list of floats, dec_state , which contains 50 floats (representing the hidden state). In [64]: # assumes we're passing in several enc states but only 1 dec states def load_hidden_states ( filename ): enc_states = {} dec_state = [] f = open ( filename ) for line in f . readlines (): model , num = line . split ()[ 0 ] . split ( \"_\" ) if model == \"enc\" : enc_states [ int ( num )] = [ float ( t ) for t in line . split ( \" \" )[ 1 :]] else : dec_state = [ float ( t ) for t in line . split ( \" \" )[ 1 :]] return enc_states , dec_state enc_states , dec_state = load_hidden_states ( \"hidden_states.txt\" ) YOU DO NOT NEED TO EDIT THE CELL BELOW The follow code simply computes the attention score as the dot-product between the two passed-in embeddings. In [65]: # calculates the attention score as the dot product def calculate_attention_score ( v1 , v2 ): return sum ( a * b for a , b in zip ( v1 , v2 )) In the cell below, populate attention_scores with the exponentiated attention scores: $e&#94;{(\\text{score(enc_i, dec_j)})}$. The main aspect to figure out is which hidden states to pass to calculate_attention_score() . In [ ]: ### edTest(test_a) ### attention_scores = [] # YOUR CODE HERE In the cell below, simply normalize each of the exponentiated scores and store them in attention_weights . They should sum to 1. In [ ]: ### edTest(test_b) ### attention_weights = [] # YOUR CODE HERE In the cell below, create the final context vector context_vector . In [ ]: ### edTest(test_c) ### # YOUR CODE HERE context_vector = # YOUR CODE HERE",
        "tags": "lectures",
        "url": "lectures/lecture24/notebook/"
    }, {
        "title": "Lecture 23: üî¢ Language Representations NLP 2/4",
        "text": "Slides Lecture 23: Representing Language (PDF) Exercises Lecture 23: GloVe embeddings (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture23/"
    }, {
        "title": "Lecture 23: üî¢ Language Representations NLP 2/4",
        "text": "Title GloVe embeddings Description : In this exercise, you'll get practice loading word embeddings and finding the most similar words to a given query word ('bank'). The file glove_mini.txt contains GloVe embeddings for nearly 3,000 of the most common English words. Each line of the file contains 51 space-separated pieces of data: word 50-values e.g. the 1st two lines of the file: the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581 of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375 We provide the cosine_similarity() function. HINT : One can sort a dict d by values (as opposed to keys) via: sorted(d.items(), key=operator.itemgetter(1)) Alternative approaches are detailed here . **REMINDER** : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. In [1]: # imports useful libraries import math import operator In [2]: # calculates the cosine simularity of the passed-in lists def cosine_sim ( a , b ): numerator = 0 denom_a = 0 denom_b = 0 for i in zip ( a , b ): numerator += i [ 0 ] * i [ 1 ] denom_a += i [ 0 ] * i [ 0 ] denom_b += i [ 1 ] * i [ 1 ] denom_a = math . sqrt ( denom_a ) denom_b = math . sqrt ( denom_b ) return numerator / ( denom_a * denom_b ) Write the function load_embeddings() , which takes a filename as input and returns the embeddings saved to a dict . That is, the key should be the word ( string ), and the value should be a list of floats . For example, {\"the\": [0.418, 0.24968, -0.41242, ..., -0.78581]} should exist within your dictionary. In [9]: ### edTest(test_a) ### def load_embeddings ( filename ): embeddings = {} f = open ( filename ) # YOUR CODE STARTS HERE # YOUR CODE ENDS HERE f . close () return embeddings # DO NOT EDIT THIS LINE embeddings = load_embeddings ( \"glove_mini.txt\" ) Write the function get_most_similar() , which finds the top K most similar words (per cosine similarity of their embeddings) to the passed-in word. To be clear, the function's inputs are: a word ( string ), to which all other words will be compared k ( int ), which is the number of top words to return ( int ). The output should be a list of strings (the words). In [4]: ### edTest(test_b) ### def get_most_similar ( word , k ): top_words = [] # YOUR CODE STARTS HERE # YOUR CODE ENDS HERE return top_words # DO NOT EDIT THIS LINE bank_words = get_most_similar ( 'bank' , 10 )",
        "tags": "lectures",
        "url": "lectures/lecture23/notebook/"
    }, {
        "title": "Lecture 22: üí¨ Language Modelling NLP 1/4",
        "text": "Slides Lecture 22: LSTMs (PDF) Lecture 22: LSTMs (PDF) Exercises Lecture 21: Unigram LM (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture22/"
    }, {
        "title": "Lecture 22: üí¨ Language Modelling NLP 1/4",
        "text": "Title : Unigram LM Description : Text data is unlike the typical \"design matrix\", i.i.d. data that we've often worked with. Here, you'll gain practice working with actual words, as you'll parse, count, and calculate a probability. An individual unigram's likelihood ( unsmoothed ) is defined as: $$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)}{n_o\\left(D_t\\right)}$$ where the numerator represents the number of times word $w$ appeared in the training corpus $D_t$. For this exercise, we will define the smoothed unigram's likelihood as: $$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)\\ +\\alpha}{n_o\\left(D_t\\right)\\ +\\alpha\\left|V\\right|}$$ where $\\alpha$ is a specified real-valued number (doesn't have to be an integer), and $|V|$ is the cardinality of the lexicon (i.e., the number of distinct word types in the vocabulary) The likelihood of a new sequence $H$ is simply defined by the likelihood of each token, multiplied by each other: $$L\\left(H\\right)=\\prod_{w\\ \\in H}&#94;{ }L\\left(w\\right)$$ HINTS : Depending on your approach, these functions could help you: re.sub() (regular expression) .split() .lower() .strip() .replace() .sum() defaultdict data structure Counter data structure **REMINDER** : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. In [1]: # imports some libraries you might find useful import re import math from collections import Counter from collections import defaultdict In [2]: # necessary for our experiments training_file = \"ex1_train.txt\" dev_file = \"ex1_dev.txt\" punctuation = [ '.' , '!' , '?' ] sample1 = \"I love data science!\" sample2 = \"I love NLP!\" Write a function parse_string() which takes as input a string (e.g., the contents of a file). It should return this text as a list of tokens. Specifically, the tokens should: be lowercased be separated by whitespace and any character present in the list of punctuation . include no trailing or preceeding whitespace (none of the returned tokens should be of white space or empty) For example, if the input is \" I LOVE daTa!!\" , it should return [\"i\", love\", \"data\", \"!\", \"!\"] In [3]: ### edTest(test_a) ### def parse_string ( text ): # YOUR CODE STARTS HERE # YOUR CODE ENDS HERE return text # DO NOT EDIT THE LINES BELOW text = open ( training_file ) . read () tokens = parse_string ( text ) Write a function count_tokens() that takes a list of tokens and simply outputs a dictionary-style count of the items. For example, if the input is ['run', 'forrest', 'run'] , it should return a dict , defaultdict , or Counter with 2 keys: {'run':2, 'forrest':1} In [4]: ### edTest(test_b) ### def count_tokens ( tokens ): # YOUR CODE STARTS HERE # YOUR CODE ENDS HERE return word_counts # DO NOT EDIT THIS LINE word_counts = count_tokens ( tokens ) Write a function calculate_likelihood() that takes tokens (a list of strings) and word_counts (dictionary-type) and returns the likelihood of the sequence of tokens. You will run your function with the tokens parsed from the sample1 string. In [5]: ### edTest(test_c) ### def calculate_likelihood ( tokens , word_counts ): total_likelihood = 1 # YOUR CODE STARTS HERE # YOUR CODE ENDS HERE return total_likelihood # DO NOT EDIT THE LINES BELOW sample1_tokens = parse_string ( sample1 ) likelihood = calculate_likelihood ( sample1_tokens , word_counts ) Write a function calculate_smoothed_likelihood() that is the same as the previous function but includes a smoothing parameter alpha . Again, you should return the likelihood of the sequence of tokens. In [6]: ### edTest(test_d) ### def calculate_smoothed_likelihood ( alpha , tokens , word_counts ): total_likelihood = 1 # YOUR CODE STARTS HERE # YOUR CODE ENDS HERE return total_likelihood # DO NOT EDIT THE LINES BELOW sample1_tokens = parse_string ( sample1 ) sample1_likelihood = calculate_smoothed_likelihood ( 0.5 , sample1_tokens , word_counts ) sample2_tokens = parse_string ( sample2 ) sample2_likelihood = calculate_smoothed_likelihood ( 0.5 , sample2_tokens , word_counts )",
        "tags": "lectures",
        "url": "lectures/lecture22/notebook/"
    }, {
        "title": "Lecture 21: ‚ç¥ LSTMs",
        "text": "Slides Lecture 21: LSTMs (PDF) Exercises Lecture 21: LSTM v/s GRU (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture21/"
    }, {
        "title": "Lecture 21 - ‚ç¥ LSTMs",
        "text": "Title : LSTM v/s GRU Description : The goal of this exercise is to compare the performance between two popular gating methods, i.e LSTM and GRUs: Instructions : Read the IMDB dataset from the helper code given. Take a quick look at your training inputs and labels. Pad the values to a fix number max_words in-order to have sequences of the same size. Build, compile and fit a GRU model Evaluate the model performance on the test set and report the test set accuracy. Again build, compile and fit a model but use the LSTM architecture instead. Evaluate the LSTM model's performance on the test set and report the test set accuracy. Compare the performance of all the two models. Hints: tf.keras.layers.Embedding() Turns positive integers (indexes) into dense vectors of fixed size. tf.keras.layers.LSTM() Long Short-Term Memory layer - Hochreiter 1997. tf.keras.layers.Dense() Just your regular densely-connected NN layer. LSTM We will use both GRU and LSTM to perform sentiment analysis in tensorflow.keras and compare their performance using the custom IMDB dataset. In [1]: # Import necessary libraries import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import backend as K from tensorflow.keras.layers import RNN from tensorflow.keras.datasets import imdb from tensorflow.keras.models import Model , Sequential from tensorflow.keras.layers import Input , Dense , LSTM , GRU , Embedding from tensorflow.keras.preprocessing import sequence from prettytable import PrettyTable import pickle In [2]: # We use the same dataset as the previous exercise with open ( 'imdb_mini.pkl' , 'rb' ) as f : X_train , y_train , X_test , y_test = pickle . load ( f ) In [3]: # Similar to the previous exercise, we will pre-preprocess our review sequences # We fix the vocabulary size to 5000 because our custom # dataset was curated with that vocabulary_size = 5000 # Max word length for each review will be 500 max_words = 200 # we set the embedding size to 32 embedding_size = 32 # Pre-padding sequences to max_words lenth X_train = sequence . pad_sequences ( X_train , maxlen = max_words , padding = 'pre' ) X_test = sequence . pad_sequences ( X_test , maxlen = max_words , padding = 'pre' ) In [4]: # We create the mapping between words and sequences word2id = imdb . get_word_index () # We need to adjust the mapping by 3 because of tensorflow.keras preprocessing # more here: https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset word2id = { k :( v + 3 ) for k , v in word2id . items ()} word2id [ \" \" ] = 0 word2id [ \" \" ] = 1 word2id [ \" \" ] = 2 word2id [ \" \" ] = 3 # Reversing the key,value pair will give the id2word id2word = { i : word for word , i in word2id . items ()} ‚è∏ For this problem with embedding_size=32 ($X_t$) and hidden_size=100 ($H_{t-1}$), how many trainable weights are associated with the GRU Cell (assuming use_bias=True )? A. 39600 B. 39800 C. 40200 D. 40400 In [5]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = 'C' In [6]: # Comparing with GRU model embedding_size = 32 hidden_size = 100 gru_model = Sequential () # Add Embedding, GRU and a Dense layer # Add Embedding layer with vocabulary_size, embedding_size and input_length # Add GRU with hidden_size # Add Dense layer with 1 unit and sigmoid activation gru_model . add ( Embedding ( vocabulary_size , embedding_size , input_length = max_words )) gru_model . add ( GRU ( hidden_size )) gru_model . add ( Dense ( 1 , activation = 'sigmoid' )) gru_model . compile ( loss = 'binary_crossentropy' , optimizer = 'Adam' , metrics = [ 'accuracy' ]) In [7]: gru_model . summary () Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 200, 32) 160000 _________________________________________________________________ gru (GRU) (None, 100) 40200 _________________________________________________________________ dense (Dense) (None, 1) 101 ================================================================= Total params: 200,301 Trainable params: 200,301 Non-trainable params: 0 _________________________________________________________________ In [8]: ### edTest(test_chow2) ### gru_cnt_params = gru_model . count_params () In [9]: batch_size = 256 num_epochs = 3 gru_model . fit ( X_train , y_train , batch_size = batch_size , epochs = num_epochs ) gru_score = gru_model . evaluate ( X_test , y_test ) print ( f 'Model accuracy on the test set is { gru_score [ 1 ] : .2f } ' ) Epoch 1/3 40/40 [==============================] - 31s 711ms/step - loss: 0.6919 - accuracy: 0.5424 Epoch 2/3 40/40 [==============================] - 35s 871ms/step - loss: 0.6306 - accuracy: 0.5924 Epoch 3/3 40/40 [==============================] - 46s 1s/step - loss: 0.5879 - accuracy: 0.7208 157/157 [==============================] - 20s 120ms/step - loss: 0.5664 - accuracy: 0.7166 Model accuracy on the test set is 0.72 ‚è∏ For this problem with embedding_size=32 ($X_t$) and hidden_size=100 ($H_{t-1}$), how many trainable weights are associated with the LSTM Cell (assuming use_bias=True )? A. 52800 B. 53200 C. 54200 D. 51400 In [10]: ### edTest(test_chow3) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = 'A' In [11]: # Comparing with LSTM model embedding_size = 32 hidden_size = 100 lstm_model = Sequential () # Add Embedding, LSTM and a Dense layer # Add Embedding layer with vocabulary_size, embedding_size and input_length # Add LSTM with hidden_size # Add Dense layer with 1 unit and sigmoid activation lstm_model . add ( Embedding ( vocabulary_size , embedding_size , input_length = max_words )) lstm_model . add ( LSTM ( 100 )) lstm_model . add ( Dense ( 1 , activation = 'sigmoid' )) lstm_model . compile ( loss = 'binary_crossentropy' , optimizer = 'Adam' , metrics = [ 'accuracy' ]) In [12]: lstm_model . summary () Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 200, 32) 160000 _________________________________________________________________ lstm (LSTM) (None, 100) 53200 _________________________________________________________________ dense_1 (Dense) (None, 1) 101 ================================================================= Total params: 213,301 Trainable params: 213,301 Non-trainable params: 0 _________________________________________________________________ In [13]: ### edTest(test_chow4) ### lstm_cnt_params = lstm_model . count_params () In [14]: batch_size = 256 num_epochs = 3 lstm_model . fit ( X_train , y_train , batch_size = batch_size , epochs = num_epochs ) lstm_score = lstm_model . evaluate ( X_test , y_test ) print ( f 'Model accuracy on the test set is { lstm_score [ 1 ] : .2f } ' ) Epoch 1/3 40/40 [==============================] - 45s 1s/step - loss: 0.6872 - accuracy: 0.5577 Epoch 2/3 40/40 [==============================] - 38s 956ms/step - loss: 0.5472 - accuracy: 0.7252 Epoch 3/3 40/40 [==============================] - 30s 746ms/step - loss: 0.3207 - accuracy: 0.8693 157/157 [==============================] - 9s 56ms/step - loss: 0.3433 - accuracy: 0.8530 Model accuracy on the test set is 0.85 In [15]: # Finally, we compare the results from the three implementations above pt = PrettyTable () pt . field_names = [ \"Strategy\" , \"Test set accuracy\" ] pt . add_row ([ \"GRU RNN\" , f ' { gru_score [ 1 ] * 100 : .2f } %' ]) pt . add_row ([ \"LSTM RNN\" , f ' { lstm_score [ 1 ] * 100 : .2f } %' ]) print ( pt ) +----------+-------------------+ | Strategy | Test set accuracy | +----------+-------------------+ | GRU RNN | 71.66% | | LSTM RNN | 85.30% | +----------+-------------------+ üç≤ Which variant is better, LSTM or GRU? Both LSTM & GRUs solve the vanishing gradient problem of RNNs but each has their advantages and disadvantages. (Read this paper for a thorough analysis of the two methods) Based on your understanding, which architecture is more appropriate for the current analysis? In [17]: ### edTest(test_chow5) ### # Type your answer within in the quotes given answer3 = 'LSTM' In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture21/notebook/"
    }, {
        "title": "Advanced Section 2: Echo-State Reservoir Computing",
        "text": "",
        "tags": "a-sections",
        "url": "a-sections/a-sec02/"
    }, {
        "title": "Lecture 20: Œ† GRUs",
        "text": "Slides Lecture 20: GRUs (PDF) Exercises Lecture 20: RNN from scratch (Notebook) Lecture 20: Vanishing Gradients (Notebook) Lecture 20: Pavlos Recurrent Unit (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture20/"
    }, {
        "title": "Lecture 20 - Œ† GRUs",
        "text": "Title : RNN from scratch Description : The aim of this exercise is to understand what happens within an RNN unit that is wrapped within the tensorflow.keras.layers.SimpleRNN The idea is to write a Recurrent Neural Network from scratch that generates names of dinosaurs by training on the existing names character-wise. Instructions: Read the file dino.txt and convert all the names in the files to lowercase. Save the names in the file as a list. Get the number of words in the file and the vocabulary size which is equal to the number of alphabets plus the newline character. Define a dictionary char_to_ix where the key is the sorted vocabulary and the value is the integer value assigned to it. Define a dictionary ix_to_char where the key is the integer value assigned to the unique vocabulary and the value is the sorted vocabulary. To get the model parameters (weights and biases) call the get_weights function twice once by: Setting the random parameter as 1 to get random weights Setting the random parameter as 0 to get the trained weights by specifying the number of iterations. Define a function rnn_model that takes in the network parameters and outputs the generated dinosaur name based on the instructions in the scaffold. Hints: $$h_t\\ =\\ \\tanh\\left(\\ Uh_{t-1}\\ +\\ Vx_t\\ +\\ \\beta_1\\right)$$$$Y_{t\\ }\\ =\\ \\sigma\\left(Wh_t\\ +\\ \\beta_2\\right)$$ sorted() Return a new sorted list from the items in iterable. enumerate() Allows to loop over something and have an automatic counter. lower() Return a copy of the string with all the cased characters converted to lowercase. strip() Return a copy of the string with the leading and trailing characters removed. np.random.shuffle() Modify a sequence in-place by shuffling its contents. np.zeros() Return a new array of given shape and type, filled with zeros. join() Return a string which is the concatenation of the strings in iterable. np.tanh() Compute hyperbolic tangent element-wise. np.dot() Returns the dot product of two arrays. In [1]: # Import necessary libraries import random import numpy as np from helper import softmax , get_weights In [1]: # Function to predict the next set of characters which forms the dinosaur name def rnn_model ( parameters , char_to_ix ): # Get the weights and biases from the parameters dictionary U , V , W = parameters [ 'U' ], parameters [ 'V' ], parameters [ 'W' ] beta1 , beta2 = parameters [ 'beta1' ], parameters [ 'beta2' ] # Get the size of the vocabulary i.e. 27 # One for each alphabet plus the new line character vocab_size = beta2 . shape [ 0 ] # Get the size of the weights n_h = U . shape [ 1 ] # Initialize the input as an array of zeroes with size as (vocab_size,1) # This one-hot encodes the input x = ___ # Initialize the inital hidden state as an array of zeroes with size as (n_h,1) h_prev = ___ # Initialize a list to store the indices of the predicted characters indices = [] # Initialize an idx variable to hold the index values of the characters idx = - 1 # Initialize a counter to fix the maximum length of the predicted word counter = 0 # Get the value of the new line from the char_to_ix dictionary newline_character = char_to_ix [ ' \\n ' ] # Loop until the newline_character is predicted or until the max length of the word is 50 while ( idx != newline_character and counter != 50 ): # Compute the new state h of the RNN unit using the equation given in the instructions h = ___ # Compute the output of the RNN unit using the equation # given in the instructions using the softmax function y = softmax ( ___ ) # Get the index value of the predicted/generated character # Instead of taking the argmax, we perform sampling on the probabilities # got from the softmax function idx = np . random . choice ( list ( range ( vocab_size )), p = y . ravel ()) # Append the index value to the indices list indices . append ( idx ) # Initialize an array of with zeroes with size (vocab_size,1) x = np . zeros (( vocab_size , 1 )) # Set the idx position of x as 1. # This will act as the output y and the next input. x [ idx ] = 1 # Update the previous state value with the current state h_prev = ___ # Increment the counter counter += 1 # If the counter value reaches 50 append a newline character to the indices list if ( counter == 50 ): indices . append ( char_to_ix [ ' \\n ' ]) # Return the list of indices return indices In [0]: # Read the dinos.txt file data = open ( 'dinos.txt' , 'r' ) . read () # Convert the data to lower case data = data . lower () # Convert the file data into list chars = list ( set ( data )) # Get length of the file and length of the vocabulary data_size , vocab_size = len ( data ), len ( chars ) # Define a dictionary with the sorted vocabulary as key and # value as a unique integer using enumerate char_to_ix = ___ # Define a dictionary with the unique integers assigned to the sorted vocabulary as key # and value as a unique integer using enumerate ix_to_char = ___ # Call the get_weights function to get the model weights # To get random weights set random=1 # To get the trained weights specify the number of iterations and set random=0 parameters = get_weights ( num_iterations = 1000 , random = 0 ) In [0]: # Call the predict function defined above passing # the parameters dictionary, char_to_ix dictionary sampled_indices = rnn_model ( parameters , char_to_ix ) # Convert the list of indices returned by the predict function to # their respective characters and then join to form a word txt = ___ # Captializing the first character txt = txt [ 0 ] . upper () + txt [ 1 :] # Print the generated dinosaur name print ( ' %s ' % ( txt , ), end = '' ) ‚è∏ What do you observe from the generated names when the number of iterations in the get_weights function increase to 20,000 with random=0? A. The length of the names generated increases proportionately with the number of iterations. B. Insufficient storage because of large window size. C. The names generated are better due to longer training. D. Larger number of iterations causes the loss of the model to increase. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___' ‚è∏ What is the difference in the generated name from when the model is given random weights and trained weights? In [0]: ### edTest(test_chow2) ### # Type your answer within in the quotes given answer2 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture20/notebook1/"
    }, {
        "title": "Lecture 20 - Œ† GRUs",
        "text": "Title : Vanishing Gradients Description : The goal of this exercise is to understand the vanishing gradient problem in training RNNs and using various methods to improve training. In order to do this exercise, we will use the IMDB movie review dataset to perform sentiment analysis. Your final comparison for the trace plot may look something like this: Instructions: Read the IMDB dataset from the helper code given. Take a quick look at your training inputs and labels. Pad the values to a fix number max_words in-order to have sequences of the same size. First post pad the inputs with padding='post' i.e sequences smaller than max_words will be followed by zero padding. Build, compile and fit a Vanilla RNN and evaluate it on the test set. Now refit the model, but this time post pad the inputs with padding='pre' Again evaluate the model performance on the test set. Finally, rebuild a model with the Gated Recurrent Unit and fit and evaluate on the training and test set respectively. Compare the performance of all three models similar to the table above. Hints: tensorflow.keras.preprocessing.sequence.pad_sequences() Pad the sequences to same length - pre/post. Vanilla RNNs We will use Vanilla Recurrent Neural Networks to perform sentiment analysis in tensorflow.keras using imdb movie reviews dataset. The dataset is a subset consisting of 10,000 reviews in the training and test set each. see here for more info In [1]: # Import required libraries from tensorflow.keras.datasets import imdb from tensorflow.keras.preprocessing import sequence from tensorflow.keras import Sequential from tensorflow.keras.layers import Embedding , SimpleRNN , Dense , GRU import pickle import numpy as np import matplotlib.pyplot as plt from prettytable import PrettyTable from pprint import pprint In [2]: # We fix a vocabulary size of 5000 # Use the code below to call a small subset of the imdb dataset # We keep the vocabulary size fixed because it was used to curate the sub-dataset vocabulary_size = 5000 with open ( 'imdb_mini.pkl' , 'rb' ) as f : X_train , y_train , X_test , y_test = pickle . load ( f ) Inspect a sample review and its label In [3]: # Run the code below to see the first tokenized review and the label print ( '---review---' ) print ( X_train [ 0 ]) print ( '---label---' ) print ( y_train [ 0 ]) ---review--- [1, 48, 2498, 2, 16, 4, 4, 1554, 149, 14, 22, 95, 198, 51, 29, 62, 2039, 46, 11, 189, 10, 10, 146, 806, 1588, 21, 2, 195, 75, 69, 79, 3514, 4, 1122, 292, 2, 5, 150, 14, 803, 4, 4351, 57, 45, 24, 618, 6, 2, 15, 289, 7, 4, 2, 323, 2, 2, 5, 4, 85, 381, 160, 87, 698, 284, 4, 293, 1141, 2, 11, 148, 2, 9, 4, 2, 7, 4, 108, 36, 1173, 93, 8, 4171, 363, 36, 71, 2812, 631, 108, 19, 6, 955, 1382, 841, 15, 43, 566, 30, 2, 11, 4, 2997, 4, 681, 9, 1215, 5, 51, 128, 96, 8, 2330, 6, 1215, 22, 74, 8, 2, 12, 19, 6, 1034, 42, 60, 6, 755, 10, 10, 2, 2, 69, 6, 1211, 2790, 159, 12, 60, 569, 4, 1320, 2, 2, 9, 60, 53, 2, 15, 85, 301, 1215, 108, 12, 2, 512, 1089, 3167, 6, 1242, 156, 4, 228, 603, 270, 1328, 15, 2, 2786, 5, 32, 4, 537, 3392, 4, 22, 10, 10, 38, 133, 266, 6, 1034, 92, 3250, 57, 2, 71, 2, 11, 4, 231, 7, 14, 1034, 49, 678, 1409, 7, 4, 65, 887, 8, 30, 2, 18, 4, 682, 2997, 2, 2, 2, 21, 198, 43, 44, 4, 226, 863, 38, 75, 202, 4, 1002, 6, 2, 405, 2, 501, 601, 19, 1465, 228, 2183, 18, 4, 706, 2629, 95, 19, 57, 2, 1699, 2, 23, 4, 1111, 15, 2580, 4, 2, 46, 21, 11, 4, 147, 182, 14, 586, 593, 1788, 43, 92, 140, 1012, 202, 90, 6, 541, 4304, 18, 3666, 247, 74, 4, 2, 7, 4580, 5, 25, 28, 4, 1034, 2, 1523, 151, 218, 12, 10, 10, 45, 43, 15, 12, 16, 32, 2984, 23, 19, 6, 2, 4, 403, 2, 71, 331, 2, 220, 1671, 23, 50, 16, 57, 281, 7, 1830, 23, 4, 1111, 57, 2, 7, 513, 8, 1277, 129, 2, 43, 6, 171, 4165, 2, 44, 6, 2585, 5, 15, 16, 12, 2, 16, 43, 616, 34, 24, 743, 46, 101, 2, 33, 32, 5, 1935, 16, 3506, 8, 387, 41, 79, 245, 19, 12, 54, 29, 435, 83, 4, 73, 25, 43, 697, 29, 62, 79, 2897, 12, 4, 881, 16, 2, 32, 4, 96, 8, 4, 130, 5, 25, 43, 473, 12, 8, 2, 56, 5, 130, 4, 2, 16, 427, 642, 5, 161, 124, 54, 8, 570, 10, 10, 15, 277, 9, 242, 4, 118, 96, 8, 2, 4, 1474, 200, 4, 107, 31, 630, 11, 4, 91, 307, 2, 103, 4, 91, 2993, 251, 4, 85, 630, 19, 6, 1208, 365, 1260, 12, 32, 8, 4, 2, 552, 1174, 10, 10, 13, 447, 4, 204, 21, 435, 8, 4, 438, 19, 35, 911, 330, 5, 16, 2229, 8, 67, 4, 22, 13, 317, 2, 11, 4, 1857, 15, 14, 22, 80, 242, 130, 56, 4158, 6, 2, 1198, 64, 14, 58, 2, 1551, 1437] ---label--- 0 In [4]: # You can get the word2id mapping by # using the imdb.get_word_index() function word2id = imdb . get_word_index () # We need to adjust the mapping by 3 because of tensorflow.keras preprocessing # more here: https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset word2id = { k :( v + 3 ) for k , v in word2id . items ()} word2id [ \" \" ] = 0 word2id [ \" \" ] = 1 word2id [ \" \" ] = 2 word2id [ \" \" ] = 3 # Reversing the key,value pair will give the id2word id2word = { i : word for word , i in word2id . items ()} pprint ( '---review with words---' ) pprint ( \" \" . join ([ id2word [ i ] for i in X_train [ 0 ]])) pprint ( '---label---' ) pprint ( y_train [ 0 ]); '---review with words---' (\" if edward was the the flicks watching this film then that's \" \"what he would scream out in horror br br i'm sorry folks but enough we \" \"had get carter the italian job and now this what's the similarities no \" \"it's not exactly a that three of the star and the \" 'other stars another great british actor the main common in those ' \"is the of the films they weren't made to impress hollywood they were \" 'quirky english films with a unique charm atmosphere that just cannot be ' ' in the usa the word is cult and what better way to destroy a cult film ' 'than to it with a remake or even a sequel br br had a ' 'tough task before it even hit the road is even more that ' 'other said cult films it genre intelligent scripts a grade actors the ' 'music score set pieces that description and all the stories ' \"surrounding the film br br so here comes a remake don't worry no were \" ' in the making of this remake some major aspects of the story needed to ' \"be for the modern usa but that's just about the \" 'whole premise so we give the cop a style past complete with ' 'shock music flashbacks for the cheap scares then with no phone ' \"on the island that sorts the out but in the real world this wouldn't \" \"happen cops just don't go missing give him a blood link for motivation \" 'rather than the of beliefs and you have the remake thin though ' \"isn't it br br it's just that it was all laid on with a the name \" 'were simply almost carry on there was no sense of community on the ' 'island no of town to catch your just a few houses about a ' 'forest and that was it was just annoying by not giving out any ' 'at all and cage was useless to let her get away with it when he went into ' 'the well you just knew he would get locked it the screenplay was all ' 'the way to the end and you just wanted it to up and end the was ' \"absolutely hilarious and didn't know when to stop br br that ending is \" 'probably the best way to the difference between the two one ends in ' 'the most beautiful after the most horrific day the other ends with a ' 'post production explain it all to the type conclusion br br i loved ' 'the original but went to the cinema with an open mind and was excited to see ' 'the film i left in the knowledge that this film will probably end up ' 'beneath a somewhere only this time forgotten forever') '---label---' 0 Maximum review length and minimum review length ‚è∏ The tag is reserved for? A. Special characters B. Out of vocabulary words C. Start of sentence D. End of sentence In [5]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' In [6]: # For training we need our sequences to be of fixed length, but the reviews # are of different sizes print ( f 'Maximum review length: { len ( max ([ i for i in X_train ] + [ i for i in X_test ], key = len )) } ' ) print ( f 'Minimum review length: { len ( min ([ i for i in X_train ] + [ i for i in X_test ], key = len )) } ' ) Maximum review length: 2494 Minimum review length: 7 In [7]: # We also create two indices for short and long reviews # we will use this later idx_short = [ i for i , val in enumerate ( X_train ) if len ( val ) < 100 ] idx_long = [ i for i , val in enumerate ( X_train ) if len ( val ) > 500 ] Pad sequences In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews. We can accomplish this using the pad_sequences() function in tensorflow.keras . For now, set max_words to 500. ‚è∏ If we use post-padding on a sequence, the new sequence is\" A. \"Padded with zeros before the start of original sequence\" B. \"Padded with zeros after the end of the original sequence\" C. \"Padded with ones before the start of original sequence\" D. \"Padded with ones after the end of the original sequence\" In [8]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer2 = '___' In [9]: # We will clip large reviews and pad smaller reviews to 500 words max_words = 500 # We can pad the smaller sequences with 0s before, or after. # This choice can severely affect network performance # In the first case we, will pad after the sequence # Please utilize sequence.pad_sequences() postpad_X_train = ___ postpad_X_test = ___ RNN model for sentiment analysis We build the model architecture in the code cell below. We have imported some layers from tensorflow.keras that you might need but feel free to use any other layers / transformations you like. Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_words, and our output is a binary sentiment label (0 or 1). In [10]: def model_maker ( summary = True , gru = False ): # One layer RNN model with 32 rnn cells embedding_size = 32 model = Sequential () model . add ( Embedding ( vocabulary_size , embedding_size , input_length = max_words )) # We can specify if we want the GRU cell or the vanilla RNN cell if gru : model . add ( GRU ( 32 )) else : model . add ( SimpleRNN ( 32 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) if summary : print ( model . summary ()) # model compile step model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model Trace Plot analysis We expect the postpadded model to perform train slowly because of vanishing gradients. Let us investigate the cause by training two models, One with shorter reviews (that were post padded) using idx_short The other with longer reviews (that were truncated) using idx_long In [11]: # We build two new models with vanilla RNNs model_short = model_maker ( summary = False ) model_long = model_maker ( summary = False ) In [12]: # First we train `model_short` with short reviews # set epochs to 10 X_short = ___ y_short = ___ epochs = ___ history_short = model_short . fit ( ___ , ___ , epochs = epochs , batch_size = 640 , verbose = 0 ); In [13]: # Then we train `model_long` with short reviews X_long = ___ y_long = ___ history_long = model_long . fit ( ___ , ___ , epochs = epochs , batch_size = 640 , verbose = 0 ); In [0]: ### edTest(test_chow2_1) ### X_short_shape , y_short_shape = X_short . shape , y_short . shape X_long_shape , y_long_shape = X_long . shape , y_long . shape print ( X_short_shape , y_short_shape , X_long_shape , y_long_shape ) In [14]: # Helper function to plot the data # Plot the MSE of the model plt . rcParams [ \"figure.figsize\" ] = ( 8 , 6 ) plt . title ( \"Padding='post'\" , fontsize = 20 ) plt . semilogy ( history_short . history [ 'loss' ], label = 'Shorter reviews' , color = '#FF9A98' , linewidth = 3 ) plt . semilogy ( history_long . history [ 'loss' ], label = 'Longer reviews' , color = '#75B594' , linewidth = 3 ) plt . legend () # Set the axes labels plt . xlabel ( 'Epochs' , fontsize = 14 ) plt . xticks ( range ( 1 , epochs , 4 )) plt . ylabel ( 'MSE Loss' , fontsize = 14 ) plt . legend ( fontsize = 14 ) plt . show () Pre-padding sequences As we can see, the vanishing gradient problem is real and can severely affect the training of our network. The short review network negligibly trains, whereas the longer review model trains very well. To counter this, we will now pre-pad the shorter sequences. In [15]: # We can pre-pad by using `sequence.pad_sequences` with `padding='pre'` max_words = 500 prepad_X_train = ___ prepad_X_test = ___ Trace Plot - Take 2 Again, we investigate the trace plots for the two categories, but this time, with pre-padding In [16]: # Reinitializing the models for the two categories model_short = model_maker ( summary = False ) model_long = model_maker ( summary = False ) In [17]: # Again we train `model_short` with short reviews X_short = ___ y_short = ___ epochs = 10 history_short = model_short . fit ( ___ , ___ , epochs = epochs , batch_size = 640 , verbose = 0 ); In [18]: # Then we train `model_long` with short reviews X_long = ___ y_long = ___ history_long = model_long . fit ( ___ , ___ , epochs = epochs , batch_size = 640 , verbose = 0 ); In [19]: # Helper function to plot the data # Plot the MSE of the model plt . rcParams [ \"figure.figsize\" ] = ( 8 , 6 ) plt . title ( \"Padding='pre'\" , fontsize = 20 ) plt . semilogy ( history_short . history [ 'loss' ], label = 'Shorter reviews' , color = '#FF9A98' , linewidth = 3 ) plt . semilogy ( history_long . history [ 'loss' ], label = 'Longer reviews' , color = '#75B594' , linewidth = 3 ) plt . legend () # Set the axes labels plt . xlabel ( 'Epochs' , fontsize = 14 ) plt . xticks ( range ( 1 , epochs , 4 )) plt . ylabel ( 'MSE Loss' , fontsize = 14 ) plt . legend ( fontsize = 14 ) plt . show () üç≤ Further improvements We solved the vanishing gradient problem by pre-padding the sequences, but what other design choices can help you improve performance? In [20]: ### edTest(test_chow3) ### # Submit your answer as a string below answer3 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture20/notebook2/"
    }, {
        "title": "Lecture 20 - Œ† GRUs",
        "text": "Title : Pavlos Recurrent Unit Description : The goal of this exercise is to build the Pavlos Recurrent Unit discussed in class. Alternative notation used in the exercise: Instructions: Read the IMDB dataset from the helper code given. Take a quick look at your training inputs and labels. Pad the values to a fix number max_words in-order to have sequences of the same size. Fill in the helper code given to build the PRU cell. Using the tensorflow.keras Functional API, build, compile and fit the PRU RNN and evaluate it on the test set. For reference, also refit the model with a vanilla RNN and a GRU. Again evaluate the model performance on the test set of both models and compare it with the PRU unit. Pavlos Recurrent Unit In this exercise, we will build the PRU as discussed in class to perform sentiment analysis in tensorflow.keras. We will continue to use the custom dataset from the previous exercise. In [1]: # Import necessary libraries import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import backend as K from tensorflow.keras.layers import RNN from tensorflow.keras.models import Model , Sequential from tensorflow.keras.layers import Input , Dense , Embedding from tensorflow.keras.layers import SimpleRNN from tensorflow.keras.preprocessing import sequence import pickle from tensorflow.keras.datasets import imdb In [2]: # We use the same dataset as the previous exercise with open ( 'imdb_mini.pkl' , 'rb' ) as f : X_train , y_train , X_test , y_test = pickle . load ( f ) In [3]: # Similar to the previous exercise, we will pre-preprocess our review sequences # We fix the vocabulary size to 5000 because our custom # dataset was curated with that vocabulary_size = 5000 # Max word length for each review will be 500 max_words = 500 # we set the embedding size to 32 embedding_size = 32 # Pre-padding sequences to max_words lenth X_train = sequence . pad_sequences ( X_train , maxlen = max_words , padding = 'pre' ) X_test = sequence . pad_sequences ( X_test , maxlen = max_words , padding = 'pre' ) In [4]: # We create the mapping between words and sequences word2id = imdb . get_word_index () # We need to adjust the mapping by 3 because of tensorflow.keras preprocessing # more here: https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset word2id = { k :( v + 3 ) for k , v in word2id . items ()} word2id [ \" \" ] = 0 word2id [ \" \" ] = 1 word2id [ \" \" ] = 2 word2id [ \" \" ] = 3 # Reversing the key,value pair will give the id2word id2word = { i : word for word , i in word2id . items ()} Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json 1646592/1641221 [==============================] - 2s 1us/step ‚è∏ For the current problem, if the memory state size is 5, what will be the dimension of $W_{xh}$? A. (32,32) B. (32,5) D. (5,5) In [40]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___' In [5]: # Complete the helper code below to build the Pavlos Recurrent Unit # We do this by building a PRU cell unit # which we can wrap around tf.keras.layers.RNN # Read more here on layer subclassing https://keras.io/guides/making_new_layers_and_models_via_subclassing/ class PRUCell ( tf . keras . layers . Layer ): def __init__ ( self , units , ** kwargs ): self . units = units self . state_size = units self . activation = tf . math . tanh self . recurrent_activation = tf . math . sigmoid super ( PRUCell , self ) . __init__ ( ** kwargs ) # In the build function we initialize the weights # Which will be used for training def build ( self , input_shape ): # Initializing weights for candidate Ht ## W_{XH} self . kernel_h = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'uniform' , name = 'kernel' ) ## W_{HH} self . recurrent_kernel_h = self . add_weight ( shape = ( self . units , self . units ), initializer = 'uniform' , name = 'recurrent_kernel' ) # Initializing weights for PP gate ## W_{XPP} self . kernel_pp = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'uniform' , name = 'PP_kernel' ) ## W_{HPP} self . recurrent_kernel_pp = self . add_weight ( shape = ( self . units , self . units ), initializer = 'uniform' , name = 'PP_recurrent_kernel' ) self . built = True # Note that we do not include a bias term for ease of understanding def call ( self , inputs , states ): ## inputs: X_t ## states: h_{t-1} ## self.XXXX contains the weights (see above) # Previous output comes from states tuple, H_{t-1} prev_output = states [ 0 ] # First we compute the PPgate PP_XW = K . dot ( ___ , ___ ) PP_HV = K . dot ( ___ , ___ ) PPgate = self . recurrent_activation ( ___ + ___ ) # Now we use the PPgate as per the equation for candidate Ht nn_XW = K . dot ( ___ , ___ ) dotted_output = ___ * ___ nn_HV = K . dot ( dotted_output , ___ ) output = self . activation ( ___ + ___ ) return output , [ output ] In [6]: # Now that we have our PRU RNN # we will build a simple model similar to the previous exercise # We will use the functional API to do this hidden_state_units = 5 # Specify the input dimensions HINT: It is max_words inputs = Input ( shape = ( max_words ,)) # The inputs will go in an embedding layer embedding = Embedding ( vocabulary_size , embedding_size , input_length = max_words )( inputs ) # The embeddings will be an input to the PRU layer cell = PRUCell ( hidden_state_units ) layer = RNN ( cell ) hidden_output = layer ( embedding ) # The output from the PRU block will go in a dense layer output = Dense ( 1 , activation = 'sigmoid' )( hidden_output ) # Connecting the architecture using tf.keras.models.Model pru_model = Model ( inputs = inputs , outputs = output ) # Get the summary to see if your model is built correctly print ( pru_model . summary ()) Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 500)] 0 _________________________________________________________________ embedding (Embedding) (None, 500, 32) 160000 _________________________________________________________________ rnn (RNN) (None, 5) 370 _________________________________________________________________ dense (Dense) (None, 1) 6 ================================================================= Total params: 160,376 Trainable params: 160,376 Non-trainable params: 0 _________________________________________________________________ None ‚è∏ For the current PRU model, how many weights are associated with the PPgate ? Bias is not included A. 370 B. 10 C. 185 D. 190 In [26]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = '____' In [18]: # Compile the model using 'binary_crossentropy' loss # and 'adam' optimizer, additionally add 'accuracy' metric pru_model . compile ( ___ ) In [19]: # Train the model with appropriate batch size and number of epochs batch_size = 256 num_epochs = 3 pru_model . fit ( ___ ) Epoch 1/3 40/40 [==============================] - 6s 146ms/step - loss: 0.6923 - accuracy: 0.5330 Epoch 2/3 40/40 [==============================] - 6s 152ms/step - loss: 0.6785 - accuracy: 0.6459 Epoch 3/3 40/40 [==============================] - 6s 144ms/step - loss: 0.6301 - accuracy: 0.6881 Out[19]: In [33]: # Evaluate the model on the custom test set and report the accuracy = pru_model . evaluate ( X_test , y_test )[ 1 ] print ( f 'The accuracy for the PRU model is { 100 * accuracy : .2f } %' ) 157/157 [==============================] - 2s 14ms/step - loss: 0.6337 - accuracy: 0.6324 The accuracy for the PRU model is 63.24% üç≤ Adding the bias to the PRU model Go back and add a bias term to the PRUCell (one for the PPGate and the other for $H_t$) Does your model performance improve under the same training conditions? In [27]: ### edTest(test_chow3) ### # Type your answer within in the quotes given answer3 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture20/notebook3/"
    }, {
        "title": "Lecture 19: œÇ RNNs",
        "text": "Slides Lecture 19: RNN (PDF) Exercises Lecture 19: Exercise - Transfer Learning (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture19/"
    }, {
        "title": "Lecture 19 - œÇ RNNs",
        "text": "Title : Transfer Learning Description : The goal of this exercise is to use Transfer Learning to achieve near-perfect accuracy for a highly customized task. The task at hand is to distinguish images of people with Sun Glasses or Hat. Instructions : Use the helper code to get the image data. Use the ImageDataGenerator function to process the image data with a validation_split of 0.2. Create a train and validation generator with flow_from_directory . Ensure that the processed input images are correctly split into the train and validation sets using flow_from_directory . Use the Keras Functional API to call the MobileNet architecture with imagenet weights. Add an appropriate number of dense layers to the top of the called architecture. The output layers consists of 2 nodes with softmax activation. Take a quick look at the summary to understand your model architecture. Freeze the first 10 layers to ensure it does not train and make the remaining layers trainable. Compile the model and fit on the train and validation data. Take a look at how your model performs by predicting on unseen images using the helper code. Hints : tf.keras.ImageDataGenerator() Generate batches of tensor image data with real-time data augmentation. ImageDataGenerator.flow_from_directory() Takes the path to a directory & generates batches of augmented data. tf.keras.applications.MobileNet() Instantiates the MobileNet architecture. tf.keras.layers.Dense() Returns a regular densely-connected NN layer. keras.Model() Model groups layers into an object with training and inference features. model.summary() Print a useful summary of the model. model.compile() Configures the model for training. model.fit() Trains the model for a fixed number of epochs (iterations on a dataset). layer.trainable() To set layers to trainable. (Note: This notebook will not run on Ed. Please click the button above to run in Google Colab) In [0]: # Importing necessary packages and libraries import tensorflow.keras as keras from tensorflow.keras import backend as K from tensorflow.keras.layers import Dense , Activation from tensorflow.keras.optimizers import Adam from tensorflow.keras.metrics import categorical_crossentropy from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.preprocessing import image from tensorflow.keras.models import Model from tensorflow.keras.applications import imagenet_utils from tensorflow.keras.layers import Dense , GlobalAveragePooling2D from tensorflow.keras.applications import MobileNet from tensorflow.keras.applications.mobilenet import preprocess_input import numpy as np import os from IPython.display import Image from tensorflow.keras.optimizers import Adam import matplotlib.pyplot as plt import matplotlib.image as mpimg In [0]: # resized ! wget https://cs109b-course-data.s3.amazonaws.com/Lecture18/resized_lecture18.zip ! unzip -qq resized_lecture18.zip In [0]: DATA_DIR = '.' ‚è∏ If you were to build a custom classifier using Transfer Learning, which pre-trained model would you use: (Please answer this in quiz) A. VGG16 model trained on medical images B. MobileNet model trained on ImageNet C. InceptionNet model trained on Landscape images In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' Get dataset In [0]: ### edTest(test_split) ### # Path of image data # data_path = os.path.join(DATA_DIR, 'images/train') data_path = os . path . join ( DATA_DIR , 'resized_lecture18/train' ) # Use the `ImageDataGenerator` function from keras to generate new images based on our existing ones # Mention the preprocessing function as mobilenet's preprocess_input and specify a validation split of 20% train_datagen = ImageDataGenerator ( ___ ) # Build your train_generator by specifying the directory using the data_path variable defined above # Mention target size, color mode, batch_size, subset as 'train' and shuffle = True train_generator = train_datagen . flow_from_directory ( ___ ) # Build your validation_generator similar to the previous step # Specifying using the data_path variable defined above with subset as 'validation' validation_generator = train_datagen . train_generator = train_datagen . flow_from_directory ( ___ ) Mobilenet plug and play Lets now use MobileNet as it is quite lightweight (17Mb), freeze the base layers and lets add and train the top few layers. Note only two classifiers. In [0]: # Use the mobilenet architecture as a starting point for our base model # Import the mobilenet model with pre-trained imagenet weights # Discard the last 1000 neuron layer ie. the final fully connected layer base_model = MobileNet ( ___ ) In [0]: x = base_model . output x = GlobalAveragePooling2D ()( x ) # On top of mobile net, add a few dense layers with 'relu' activation # Using functional API, add a dense layer with 1024 neurons x = Dense ( ___ )( x ) # Add a dense layer with 512 neurons x = Dense ( ___ )( x ) # Add a final layer with 2 neurons and softmax activation preds = Dense ( ___ )( x ) In [0]: # Using the functional API of keras, specify the input from the base model and the output as `preds` described above model = Model ( ___ ) #specify the inputs and outputs ‚è∏ When you used the pre-trained model in the exercise, did you use the entire pre-trained model (convolution layers and classification dense layers) with all the layers? (Please answer this in quiz) A. True B. False In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer2 = '___' We will use pre-trained weights as the model has been trained already on the Imagenet dataset. We ensure all the weights are non-trainable. We will only train the last few dense layers. In [0]: ### edTest(test_layers) ### # For transfer learning, we need to freeze some layers. Below we freeze the first 10 layers # Freeze the first 10 layers of the network to be non-trainable for layer in model . layers [: 10 ]: ___ Lets check the model architecture In [0]: ### edTest(test_summary) ### # Look at the summary of your model model . ___ () ‚è∏ In the pre trained model from the exercise how many trainable params did you have? (Please answer this in quiz) In [0]: ### edTest(test_chow3) ### # Submit an answer as 10,000 or 10000 answer3 = '___' Now lets load the training data into the ImageDataGenerator. Specify path, and it automatically sends the data for training in batches, simplifying the code. Compile the model. Now lets train it. Should take less than two minutes on a GTX1070 GPU. Training the model In [0]: # We now train our model, but first we will compile it with an appropriate loss function and optimizer # Adam optimizer # loss function will be categorical crossentropy # evaluation metric will be accuracy model . compile ( ___ ) In [0]: # Fit the model using the step size for train and validation specified below # Given the limited resources, please restrict the number of epochs to less than 5 step_size_train = train_generator . n // train_generator . batch_size step_size_validation = validation_generator . n // validation_generator . batch_size model . fit ( ___ ) Model is now trained. Now lets test some independent input images to check the predictions. Inference on unseen data In [0]: # A helper function that takes a standard image and converts it into a tensor that can be used by the model def load_image ( img_path , show = False ): img = image . load_img ( img_path , target_size = ( 224 , 224 )) img_tensor = image . img_to_array ( img ) # (height, width, channels) img_tensor = np . expand_dims ( img_tensor , axis = 0 ) # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels) img_tensor = preprocess_input ( img_tensor ) # imshow expects values in the range [0, 1] if show : plt . imshow ( img_tensor [ 0 ]) plt . axis ( 'off' ) plt . show () return img_tensor In [0]: # # We specify the paths of the six images # We specify the paths of the six images #31.jpg 458.jpg 571.jpg 667.jpg 672.jpg # First set of images img_path1 = os . path . join ( DATA_DIR , 'resized_lecture18/test/31.jpg' ) img_path2 = os . path . join ( DATA_DIR , 'resized_lecture18/test/458.jpg' ) img_path3 = os . path . join ( DATA_DIR , 'resized_lecture18/test/571.jpg' ) # Second set of images img_path4 = os . path . join ( DATA_DIR , 'resized_lecture18/test/rashmi.jpg' ) img_path5 = os . path . join ( DATA_DIR , 'resized_lecture18/test/pavlos.jpg' ) img_path6 = os . path . join ( DATA_DIR , 'resized_lecture18/test/shivas.jpg' ) In [0]: # Helper function that nicely predicts the class along with the input image def prediction ( img_loc , ax ): new_image = load_image ( img_loc ) pred = model . predict ( new_image ) classmap = { v : k for k , v in ( train_generator . class_indices ) . items ()} plot_img = mpimg . imread ( img_loc ); ax . imshow ( plot_img , vmin = 0 , vmax = 255 ) ax . set_title ( f 'Prediction: { classmap [ pred . argmax ( - 1 )[ 0 ]] } \\n (with confidence: { str ( pred [ 0 ][ pred . argmax ( - 1 )][ 0 ])[: 4 ] } )' , fontsize = 18 ) ax . axis ( 'off' ) In [0]: # Make predictions on first set of images defined above that were never shown to the model before fig , axes = plt . subplots ( 1 , 3 , figsize = ( 12 , 6 )) # For each prediction mention the axes prediction ( img_path1 , axes [ 0 ]) prediction ( img_path2 , axes [ 1 ]) prediction ( img_path3 , axes [ 2 ]) In [0]: # Make predictions on second set of images defined above that were never shown to the model before fig , axes = plt . subplots ( 1 , 3 , figsize = ( 15 , 5 )) # # Call the prediction function defined above for this # # For each prediction mention the axes # ___ # ___ # ___ # For each prediction mention the axes # For each prediction mention the axes prediction ( img_path4 , axes [ 0 ]) prediction ( img_path5 , axes [ 1 ]) prediction ( img_path6 , axes [ 2 ]) Mindchow üç≤ Go back and change the number of trainable parameters. How does it affect your network performance? Your answer here In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture19/notebook1/"
    }, {
        "title": "Lecture 18: ùóà State of the art models (SOTA) and Transfer Learning",
        "text": "Slides Lecture 18: CNN5 - SOTA (PDF) Lecture 18: Transfer Learning (PDF) Exercises Lecture 18: Performance comparison of different SOTAs (Notebook) Lecture 18: Exercise - Transfer Learning (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture18/"
    }, {
        "title": "Lecture 18 - ùóà State of the art models (SOTA) and Transfer Learning",
        "text": "Title : Transfer Learning Description : The goal of this exercise is to use Transfer Learning to achieve near-perfect accuracy for a highly customized task. The task at hand is to distinguish images of people with Sun Glasses or Hat. Instructions : Use the helper code to get the image data. Use the ImageDataGenerator function to process the image data with a validation_split of 0.2. Create a train and validation generator with flow_from_directory . Ensure that the processed input images are correctly split into the train and validation sets using flow_from_directory . Use the Keras Functional API to call the MobileNet architecture with imagenet weights. Add an appropriate number of dense layers to the top of the called architecture. The output layers consists of 2 nodes with softmax activation. Take a quick look at the summary to understand your model architecture. Freeze the first 10 layers to ensure it does not train and make the remaining layers trainable. Compile the model and fit on the train and validation data. Take a look at how your model performs by predicting on unseen images using the helper code. Hints : tf.keras.ImageDataGenerator() Generate batches of tensor image data with real-time data augmentation. ImageDataGenerator.flow_from_directory() Takes the path to a directory & generates batches of augmented data. tf.keras.applications.MobileNet() Instantiates the MobileNet architecture. tf.keras.layers.Dense() Returns a regular densely-connected NN layer. keras.Model() Model groups layers into an object with training and inference features. model.summary() Print a useful summary of the model. model.compile() Configures the model for training. model.fit() Trains the model for a fixed number of epochs (iterations on a dataset). layer.trainable() To set layers to trainable. (Note: This notebook will not run on Ed. Please click the button above to run in Google Colab) In [0]: # Importing necessary packages and libraries import tensorflow.keras as keras from tensorflow.keras import backend as K from tensorflow.keras.layers import Dense , Activation from tensorflow.keras.optimizers import Adam from tensorflow.keras.metrics import categorical_crossentropy from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.preprocessing import image from tensorflow.keras.models import Model from tensorflow.keras.applications import imagenet_utils from tensorflow.keras.layers import Dense , GlobalAveragePooling2D from tensorflow.keras.applications import MobileNet from tensorflow.keras.applications.mobilenet import preprocess_input import numpy as np import os from IPython.display import Image from tensorflow.keras.optimizers import Adam import matplotlib.pyplot as plt import matplotlib.image as mpimg In [0]: # resized ! wget https://cs109b-course-data.s3.amazonaws.com/Lecture18/resized_lecture18.zip ! unzip -qq resized_lecture18.zip In [0]: DATA_DIR = '.' ‚è∏ If you were to build a custom classifier using Transfer Learning, which pre-trained model would you use: (Please answer this in quiz) A. VGG16 model trained on medical images B. MobileNet model trained on ImageNet C. InceptionNet model trained on Landscape images In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' Get dataset In [0]: ### edTest(test_split) ### # Path of image data # data_path = os.path.join(DATA_DIR, 'images/train') data_path = os . path . join ( DATA_DIR , 'resized_lecture18/train' ) # Use the `ImageDataGenerator` function from keras to generate new images based on our existing ones # Mention the preprocessing function as mobilenet's preprocess_input and specify a validation split of 20% train_datagen = ImageDataGenerator ( ___ ) # Build your train_generator by specifying the directory using the data_path variable defined above # Mention target size, color mode, batch_size, subset as 'train' and shuffle = True train_generator = train_datagen . flow_from_directory ( ___ ) # Build your validation_generator similar to the previous step # Specifying using the data_path variable defined above with subset as 'validation' validation_generator = train_datagen . train_generator = train_datagen . flow_from_directory ( ___ ) Mobilenet plug and play Lets now use MobileNet as it is quite lightweight (17Mb), freeze the base layers and lets add and train the top few layers. Note only two classifiers. In [0]: # Use the mobilenet architecture as a starting point for our base model # Import the mobilenet model with pre-trained imagenet weights # Discard the last 1000 neuron layer ie. the final fully connected layer base_model = MobileNet ( ___ ) In [0]: x = base_model . output x = GlobalAveragePooling2D ()( x ) # On top of mobile net, add a few dense layers with 'relu' activation # Using functional API, add a dense layer with 1024 neurons x = Dense ( ___ )( x ) # Add a dense layer with 512 neurons x = Dense ( ___ )( x ) # Add a final layer with 2 neurons and softmax activation preds = Dense ( ___ )( x ) In [0]: # Using the functional API of keras, specify the input from the base model and the output as `preds` described above model = Model ( ___ ) #specify the inputs and outputs ‚è∏ When you used the pre-trained model in the exercise, did you use the entire pre-trained model (convolution layers and classification dense layers) with all the layers? (Please answer this in quiz) A. True B. False In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer2 = '___' We will use pre-trained weights as the model has been trained already on the Imagenet dataset. We ensure all the weights are non-trainable. We will only train the last few dense layers. In [0]: ### edTest(test_layers) ### # For transfer learning, we need to freeze some layers. Below we freeze the first 10 layers # Freeze the first 10 layers of the network to be non-trainable for layer in model . layers [: 10 ]: ___ Lets check the model architecture In [0]: ### edTest(test_summary) ### # Look at the summary of your model model . ___ () ‚è∏ In the pre trained model from the exercise how many trainable params did you have? (Please answer this in quiz) In [0]: ### edTest(test_chow3) ### # Submit an answer as 10,000 or 10000 answer3 = '___' Now lets load the training data into the ImageDataGenerator. Specify path, and it automatically sends the data for training in batches, simplifying the code. Compile the model. Now lets train it. Should take less than two minutes on a GTX1070 GPU. Training the model In [0]: # We now train our model, but first we will compile it with an appropriate loss function and optimizer # Adam optimizer # loss function will be categorical crossentropy # evaluation metric will be accuracy model . compile ( ___ ) In [0]: # Fit the model using the step size for train and validation specified below # Given the limited resources, please restrict the number of epochs to less than 5 step_size_train = train_generator . n // train_generator . batch_size step_size_validation = validation_generator . n // validation_generator . batch_size model . fit ( ___ ) Model is now trained. Now lets test some independent input images to check the predictions. Inference on unseen data In [0]: # A helper function that takes a standard image and converts it into a tensor that can be used by the model def load_image ( img_path , show = False ): img = image . load_img ( img_path , target_size = ( 224 , 224 )) img_tensor = image . img_to_array ( img ) # (height, width, channels) img_tensor = np . expand_dims ( img_tensor , axis = 0 ) # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels) img_tensor = preprocess_input ( img_tensor ) # imshow expects values in the range [0, 1] if show : plt . imshow ( img_tensor [ 0 ]) plt . axis ( 'off' ) plt . show () return img_tensor In [0]: # # We specify the paths of the six images # We specify the paths of the six images #31.jpg 458.jpg 571.jpg 667.jpg 672.jpg # First set of images img_path1 = os . path . join ( DATA_DIR , 'resized_lecture18/test/31.jpg' ) img_path2 = os . path . join ( DATA_DIR , 'resized_lecture18/test/458.jpg' ) img_path3 = os . path . join ( DATA_DIR , 'resized_lecture18/test/571.jpg' ) # Second set of images img_path4 = os . path . join ( DATA_DIR , 'resized_lecture18/test/rashmi.jpg' ) img_path5 = os . path . join ( DATA_DIR , 'resized_lecture18/test/pavlos.jpg' ) img_path6 = os . path . join ( DATA_DIR , 'resized_lecture18/test/shivas.jpg' ) In [0]: # Helper function that nicely predicts the class along with the input image def prediction ( img_loc , ax ): new_image = load_image ( img_loc ) pred = model . predict ( new_image ) classmap = { v : k for k , v in ( train_generator . class_indices ) . items ()} plot_img = mpimg . imread ( img_loc ); ax . imshow ( plot_img , vmin = 0 , vmax = 255 ) ax . set_title ( f 'Prediction: { classmap [ pred . argmax ( - 1 )[ 0 ]] } \\n (with confidence: { str ( pred [ 0 ][ pred . argmax ( - 1 )][ 0 ])[: 4 ] } )' , fontsize = 18 ) ax . axis ( 'off' ) In [0]: # Make predictions on first set of images defined above that were never shown to the model before fig , axes = plt . subplots ( 1 , 3 , figsize = ( 12 , 6 )) # For each prediction mention the axes prediction ( img_path1 , axes [ 0 ]) prediction ( img_path2 , axes [ 1 ]) prediction ( img_path3 , axes [ 2 ]) In [0]: # Make predictions on second set of images defined above that were never shown to the model before fig , axes = plt . subplots ( 1 , 3 , figsize = ( 15 , 5 )) # # Call the prediction function defined above for this # # For each prediction mention the axes # ___ # ___ # ___ # For each prediction mention the axes # For each prediction mention the axes prediction ( img_path4 , axes [ 0 ]) prediction ( img_path5 , axes [ 1 ]) prediction ( img_path6 , axes [ 2 ]) Mindchow üç≤ Go back and change the number of trainable parameters. How does it affect your network performance? Your answer here In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture18/notebook1/"
    }, {
        "title": "Lecture 18 - ùóà State of the art models (SOTA) and Transfer Learning",
        "text": "Title : Performance comparison of different SOTAs Description : The goal of this exercise is to compare different architectures on speed, size and performance. Your final plot may resemble the one below: To read more about Imagenet Dataset. Instructions : Load the data and choose only the first 100 images from the validation dataset. Using the helper code obtain the model statistics for each of the following SOTAs below: VGG16 VGG19 InceptionNetV3 ResNet50 MobileNetV2 Hints : model.layers Accesses layers of the model tf.keras.activations.linear Linear activation function model.predict() Used to predict the values given the model np.argsort() Returns the indices that would sort an array. tf.keras.applications.VGG16 Instantiates the VGG16 model. tf.keras.applications.VGG19 Instantiates the VGG16 model. tf.keras.applications.ResNet50 Instantiates the ResNet50 architecture. tf.keras.applications.InceptionV3 Instantiates the Inception v3 architecture. tf.keras.applications.MobileNetV2 Instantiates the MobileNetV2 architecture. Performance comparision on SOTAs (Note: This notebook will not run on Ed. Please click the button above to run in Google Colab) In [1]: # Import required libraries import sys , os , time import numpy as np import matplotlib.pyplot as plt % matplotlib inline from matplotlib.colors import ListedColormap colors = [ 'k' , 'g' , 'r' , 'b' , 'c' ] plt . style . use ( 'seaborn-whitegrid' ) from helper import ellipse import pickle from tensorflow.keras import backend as K from tensorflow.keras.utils import to_categorical from tensorflow.keras.layers import Input import timeit Loading the data In [2]: # Useful dictionary to go from label index to actual label with open ( 'idx2name.pkl' , 'rb' ) as handle : keras_idx_to_name = pickle . load ( handle ) In [3]: # Loading input image and labels images = np . load ( \"/course/data/x_val.npy\" ) # loaded as RGB labels = np . load ( \"/course/data/y_val.npy\" ) # Taking only 100 samples for quicker computation x_val = images [: 100 ] y_val = labels [: 100 ] # One hot encoding the labels y_val_one_hot = to_categorical ( y_val , 1000 ) In [0]: # Print a sample image and set the label as title plt . title ( ___ ) plt . imshow ( ___ ) ‚è∏ What is the label for the first image in the validation set? (Please answer this in quiz) A. Cabbage Butterfly B. Mixing bowl C. Wok D. French horn In [0]: # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' Benchmark models In [0]: # Helper function to get key stats # (evaluation speed, top-1 % accuracy, total model parameters) def model_stats ( model , x_val , name ): #Time for evaluation time = timeit . timeit ( lambda : model . predict ( x_val , verbose = 1 ), number = 1 ) # Accuracy y_pred = model . predict ( x_val ) top_1 = np . any ( np . argsort ( y_pred )[:, - 1 :] . T == y_val_one_hot . argmax ( axis = 1 ), axis = 0 ) . mean () # Model size params = model . count_params () return ( time , top_1 , params , name ) SOTA architectures For this exercise, we will consider the following SOTAs: VGG16 VGG19 InceptionV3 ResNet50 MobileNet In [0]: # VGG16 stats from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # Preprocess step # We need to call the data because some preprocess steps # change the value inplace x_val = np . load ( \"/course/data/x_val.npy\" ) # loaded as RGB x_val = x_val [: 100 ] x_val = preprocess_input ( x_val ) # Call the VGG16 model model = ___ # Collect stats vgg16stats = model_stats ( model , x_val , 'VGG16' ) In [0]: # VGG19 stats from tensorflow.keras.applications.vgg19 import VGG19 , preprocess_input x_val = np . load ( \"/course/data/x_val.npy\" ) # loaded as RGB x_val = x_val [: 100 ] x_val = preprocess_input ( x_val ) # Call the VGG19 model model = ___ # Collect stats vgg19stats = model_stats ( model , x_val , 'VGG19' ) In [0]: # Inception Stats from tensorflow.keras.applications.inception_v3 import InceptionV3 , preprocess_input x_val = np . load ( \"/course/data/x_val.npy\" ) # loaded as RGB x_val = x_val [: 100 ] x_val = preprocess_input ( x_val ) # Call the InceptionV3 model model = ___ # Collect stats inceptionstats = model_stats ( model , x_val , 'Inception' ) In [0]: # Resnet50 stats from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input x_val = np . load ( \"/course/data/x_val.npy\" ) # loaded as RGB x_val = x_val [: 100 ] x_val = preprocess_input ( x_val ) # Call the ResNet50 model model = ___ # Collect stats resnetstats = model_stats ( model , x_val , 'Resnet50' ) In [0]: # MobileNet stats from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 , preprocess_input x_val = np . load ( \"/course/data/x_val.npy\" ) # loaded as RGB x_val = x_val [: 100 ] x_val = preprocess_input ( x_val ) # Call the MobielNetV2 model model = ___ # Collect stats mobilestats = model_stats ( model , x_val , 'MobileNet' ) ‚è∏ Which SOTA architecture from above has the highest number of trainable parameters? (Please answer this in quiz) A. VGG-16 B. VGG-19 C. ResNet50 D. MobileNet In [0]: # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer2 = '___' In [0]: # Use the helper code below # to plot the model stats for each SOTA fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i , val in enumerate ([ vgg16stats , vgg19stats , inceptionstats , resnetstats , mobilestats ]): r = val [ 2 ] / 10 ** 9 + 0.04 ellipse ( val [ 0 ] / 40 , val [ 1 ], width = r , height = 0.44 * r , color = colors [ i ], ax = ax ) ax . text ( val [ 0 ] / 40 + 0.035 , val [ 1 ] + r / 4 + 0.004 , val [ 3 ], va = 'center' , ha = 'center' , fontsize = 12 ) ax . set_ylim ([ 0.6 , 0.85 ]) ax . set_ylabel ( 'Top-1 accuracy [%]' , fontsize = 20 ) ax . set_xlabel ( 'Time for evaluation [s]' , fontsize = 20 ) ax . set_xticklabels ( range ( 0 , 60 , 8 )); ax . set_yticklabels ( range ( 50 , 110 , 10 )); for axis in [ 'bottom' , 'left' ]: ax . spines [ axis ] . set_linewidth ( 3 ) ax . spines [ axis ] . set_color ( 'k' ) üç≤ Larger dataset Go back and take a larger sample of images, do your results remain consistent? In [0]: # Type your answer within in the quotes given answer3 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture18/notebook2/"
    }, {
        "title": "Advanced Section 1: Semantic Segmentation and Object Detection",
        "text": "",
        "tags": "a-sections",
        "url": "a-sections/a-sec01/"
    }, {
        "title": "Lecture 17: Œª Saliency maps",
        "text": "Slides Lecture 17: CNN2 (PDF) Exercises Lecture 17: Grad-CAM from scratch (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture17/"
    }, {
        "title": "Investigating CNNs",
        "text": "Title : Grad-CAM from scratch Description : The goal of this exercise is to make a saliency map using Grad-CAM. Your final image may resemble the one below: For this exercise, we will use the MobileNetV2 pre-trained model. You will apply Grad-CAM to the input cat image using what we learnt from lecture: Instructions: Load the pre-trained model and pre-process the given image to make a prediction. Find the predicted class of the image. It should be an Egyptian cat . Using the tf.keras Functional API, build a model that gives the model predictions and the feature maps after the last convolution in the pre-trained network. Using tf.GradientTape() find the gradients of the output with respect to the activations. As per the Grad-CAM implementation, pool the gradients and find the heatmap. Upsample the heatmap using the helper function and superimpose it on the original image to get the output like the one shown above. Hints: model.layers Accesses layers of the model tf.keras.activations.linear Linear activation function model.predict() Used to predict the values given the model tf.keras.applications.mobilenet_v2.MobileNetV2 Instantiates the MobileNet v2 architecture. In [1]: # Import required libraries import tensorflow as tf from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 , preprocess_input , decode_predictions from tensorflow.keras.preprocessing import image from tensorflow.keras.models import Model import numpy as np import matplotlib.pyplot as plt import cv2 import pickle In [2]: # Load the MobileNet V2 pre-trained model # Rather than training a model from scratch we can use a pre trained # model that has already been trained in the imagenet dataset # MobileNetV2 is a SOTA model for image classification model = MobileNetV2 ( weights = 'imagenet' ) model . summary () Model: \"mobilenetv2_1.00_224\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 224, 224, 3) 0 __________________________________________________________________________________________________ Conv1 (Conv2D) (None, 112, 112, 32) 864 input_1[0][0] __________________________________________________________________________________________________ bn_Conv1 (BatchNormalization) (None, 112, 112, 32) 128 Conv1[0][0] __________________________________________________________________________________________________ Conv1_relu (ReLU) (None, 112, 112, 32) 0 bn_Conv1[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288 Conv1_relu[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128 expanded_conv_depthwise[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0 expanded_conv_depthwise_BN[0][0] __________________________________________________________________________________________________ expanded_conv_project (Conv2D) (None, 112, 112, 16) 512 expanded_conv_depthwise_relu[0][0 __________________________________________________________________________________________________ expanded_conv_project_BN (Batch (None, 112, 112, 16) 64 expanded_conv_project[0][0] __________________________________________________________________________________________________ block_1_expand (Conv2D) (None, 112, 112, 96) 1536 expanded_conv_project_BN[0][0] __________________________________________________________________________________________________ block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384 block_1_expand[0][0] __________________________________________________________________________________________________ block_1_expand_relu (ReLU) (None, 112, 112, 96) 0 block_1_expand_BN[0][0] __________________________________________________________________________________________________ block_1_pad (ZeroPadding2D) (None, 113, 113, 96) 0 block_1_expand_relu[0][0] __________________________________________________________________________________________________ block_1_depthwise (DepthwiseCon (None, 56, 56, 96) 864 block_1_pad[0][0] __________________________________________________________________________________________________ block_1_depthwise_BN (BatchNorm (None, 56, 56, 96) 384 block_1_depthwise[0][0] __________________________________________________________________________________________________ block_1_depthwise_relu (ReLU) (None, 56, 56, 96) 0 block_1_depthwise_BN[0][0] __________________________________________________________________________________________________ block_1_project (Conv2D) (None, 56, 56, 24) 2304 block_1_depthwise_relu[0][0] __________________________________________________________________________________________________ block_1_project_BN (BatchNormal (None, 56, 56, 24) 96 block_1_project[0][0] __________________________________________________________________________________________________ block_2_expand (Conv2D) (None, 56, 56, 144) 3456 block_1_project_BN[0][0] __________________________________________________________________________________________________ block_2_expand_BN (BatchNormali (None, 56, 56, 144) 576 block_2_expand[0][0] __________________________________________________________________________________________________ block_2_expand_relu (ReLU) (None, 56, 56, 144) 0 block_2_expand_BN[0][0] __________________________________________________________________________________________________ block_2_depthwise (DepthwiseCon (None, 56, 56, 144) 1296 block_2_expand_relu[0][0] __________________________________________________________________________________________________ block_2_depthwise_BN (BatchNorm (None, 56, 56, 144) 576 block_2_depthwise[0][0] __________________________________________________________________________________________________ block_2_depthwise_relu (ReLU) (None, 56, 56, 144) 0 block_2_depthwise_BN[0][0] __________________________________________________________________________________________________ block_2_project (Conv2D) (None, 56, 56, 24) 3456 block_2_depthwise_relu[0][0] __________________________________________________________________________________________________ block_2_project_BN (BatchNormal (None, 56, 56, 24) 96 block_2_project[0][0] __________________________________________________________________________________________________ block_2_add (Add) (None, 56, 56, 24) 0 block_1_project_BN[0][0] block_2_project_BN[0][0] __________________________________________________________________________________________________ block_3_expand (Conv2D) (None, 56, 56, 144) 3456 block_2_add[0][0] __________________________________________________________________________________________________ block_3_expand_BN (BatchNormali (None, 56, 56, 144) 576 block_3_expand[0][0] __________________________________________________________________________________________________ block_3_expand_relu (ReLU) (None, 56, 56, 144) 0 block_3_expand_BN[0][0] __________________________________________________________________________________________________ block_3_pad (ZeroPadding2D) (None, 57, 57, 144) 0 block_3_expand_relu[0][0] __________________________________________________________________________________________________ block_3_depthwise (DepthwiseCon (None, 28, 28, 144) 1296 block_3_pad[0][0] __________________________________________________________________________________________________ block_3_depthwise_BN (BatchNorm (None, 28, 28, 144) 576 block_3_depthwise[0][0] __________________________________________________________________________________________________ block_3_depthwise_relu (ReLU) (None, 28, 28, 144) 0 block_3_depthwise_BN[0][0] __________________________________________________________________________________________________ block_3_project (Conv2D) (None, 28, 28, 32) 4608 block_3_depthwise_relu[0][0] __________________________________________________________________________________________________ block_3_project_BN (BatchNormal (None, 28, 28, 32) 128 block_3_project[0][0] __________________________________________________________________________________________________ block_4_expand (Conv2D) (None, 28, 28, 192) 6144 block_3_project_BN[0][0] __________________________________________________________________________________________________ block_4_expand_BN (BatchNormali (None, 28, 28, 192) 768 block_4_expand[0][0] __________________________________________________________________________________________________ block_4_expand_relu (ReLU) (None, 28, 28, 192) 0 block_4_expand_BN[0][0] __________________________________________________________________________________________________ block_4_depthwise (DepthwiseCon (None, 28, 28, 192) 1728 block_4_expand_relu[0][0] __________________________________________________________________________________________________ block_4_depthwise_BN (BatchNorm (None, 28, 28, 192) 768 block_4_depthwise[0][0] __________________________________________________________________________________________________ block_4_depthwise_relu (ReLU) (None, 28, 28, 192) 0 block_4_depthwise_BN[0][0] __________________________________________________________________________________________________ block_4_project (Conv2D) (None, 28, 28, 32) 6144 block_4_depthwise_relu[0][0] __________________________________________________________________________________________________ block_4_project_BN (BatchNormal (None, 28, 28, 32) 128 block_4_project[0][0] __________________________________________________________________________________________________ block_4_add (Add) (None, 28, 28, 32) 0 block_3_project_BN[0][0] block_4_project_BN[0][0] __________________________________________________________________________________________________ block_5_expand (Conv2D) (None, 28, 28, 192) 6144 block_4_add[0][0] __________________________________________________________________________________________________ block_5_expand_BN (BatchNormali (None, 28, 28, 192) 768 block_5_expand[0][0] __________________________________________________________________________________________________ block_5_expand_relu (ReLU) (None, 28, 28, 192) 0 block_5_expand_BN[0][0] __________________________________________________________________________________________________ block_5_depthwise (DepthwiseCon (None, 28, 28, 192) 1728 block_5_expand_relu[0][0] __________________________________________________________________________________________________ block_5_depthwise_BN (BatchNorm (None, 28, 28, 192) 768 block_5_depthwise[0][0] __________________________________________________________________________________________________ block_5_depthwise_relu (ReLU) (None, 28, 28, 192) 0 block_5_depthwise_BN[0][0] __________________________________________________________________________________________________ block_5_project (Conv2D) (None, 28, 28, 32) 6144 block_5_depthwise_relu[0][0] __________________________________________________________________________________________________ block_5_project_BN (BatchNormal (None, 28, 28, 32) 128 block_5_project[0][0] __________________________________________________________________________________________________ block_5_add (Add) (None, 28, 28, 32) 0 block_4_add[0][0] block_5_project_BN[0][0] __________________________________________________________________________________________________ block_6_expand (Conv2D) (None, 28, 28, 192) 6144 block_5_add[0][0] __________________________________________________________________________________________________ block_6_expand_BN (BatchNormali (None, 28, 28, 192) 768 block_6_expand[0][0] __________________________________________________________________________________________________ block_6_expand_relu (ReLU) (None, 28, 28, 192) 0 block_6_expand_BN[0][0] __________________________________________________________________________________________________ block_6_pad (ZeroPadding2D) (None, 29, 29, 192) 0 block_6_expand_relu[0][0] __________________________________________________________________________________________________ block_6_depthwise (DepthwiseCon (None, 14, 14, 192) 1728 block_6_pad[0][0] __________________________________________________________________________________________________ block_6_depthwise_BN (BatchNorm (None, 14, 14, 192) 768 block_6_depthwise[0][0] __________________________________________________________________________________________________ block_6_depthwise_relu (ReLU) (None, 14, 14, 192) 0 block_6_depthwise_BN[0][0] __________________________________________________________________________________________________ block_6_project (Conv2D) (None, 14, 14, 64) 12288 block_6_depthwise_relu[0][0] __________________________________________________________________________________________________ block_6_project_BN (BatchNormal (None, 14, 14, 64) 256 block_6_project[0][0] __________________________________________________________________________________________________ block_7_expand (Conv2D) (None, 14, 14, 384) 24576 block_6_project_BN[0][0] __________________________________________________________________________________________________ block_7_expand_BN (BatchNormali (None, 14, 14, 384) 1536 block_7_expand[0][0] __________________________________________________________________________________________________ block_7_expand_relu (ReLU) (None, 14, 14, 384) 0 block_7_expand_BN[0][0] __________________________________________________________________________________________________ block_7_depthwise (DepthwiseCon (None, 14, 14, 384) 3456 block_7_expand_relu[0][0] __________________________________________________________________________________________________ block_7_depthwise_BN (BatchNorm (None, 14, 14, 384) 1536 block_7_depthwise[0][0] __________________________________________________________________________________________________ block_7_depthwise_relu (ReLU) (None, 14, 14, 384) 0 block_7_depthwise_BN[0][0] __________________________________________________________________________________________________ block_7_project (Conv2D) (None, 14, 14, 64) 24576 block_7_depthwise_relu[0][0] __________________________________________________________________________________________________ block_7_project_BN (BatchNormal (None, 14, 14, 64) 256 block_7_project[0][0] __________________________________________________________________________________________________ block_7_add (Add) (None, 14, 14, 64) 0 block_6_project_BN[0][0] block_7_project_BN[0][0] __________________________________________________________________________________________________ block_8_expand (Conv2D) (None, 14, 14, 384) 24576 block_7_add[0][0] __________________________________________________________________________________________________ block_8_expand_BN (BatchNormali (None, 14, 14, 384) 1536 block_8_expand[0][0] __________________________________________________________________________________________________ block_8_expand_relu (ReLU) (None, 14, 14, 384) 0 block_8_expand_BN[0][0] __________________________________________________________________________________________________ block_8_depthwise (DepthwiseCon (None, 14, 14, 384) 3456 block_8_expand_relu[0][0] __________________________________________________________________________________________________ block_8_depthwise_BN (BatchNorm (None, 14, 14, 384) 1536 block_8_depthwise[0][0] __________________________________________________________________________________________________ block_8_depthwise_relu (ReLU) (None, 14, 14, 384) 0 block_8_depthwise_BN[0][0] __________________________________________________________________________________________________ block_8_project (Conv2D) (None, 14, 14, 64) 24576 block_8_depthwise_relu[0][0] __________________________________________________________________________________________________ block_8_project_BN (BatchNormal (None, 14, 14, 64) 256 block_8_project[0][0] __________________________________________________________________________________________________ block_8_add (Add) (None, 14, 14, 64) 0 block_7_add[0][0] block_8_project_BN[0][0] __________________________________________________________________________________________________ block_9_expand (Conv2D) (None, 14, 14, 384) 24576 block_8_add[0][0] __________________________________________________________________________________________________ block_9_expand_BN (BatchNormali (None, 14, 14, 384) 1536 block_9_expand[0][0] __________________________________________________________________________________________________ block_9_expand_relu (ReLU) (None, 14, 14, 384) 0 block_9_expand_BN[0][0] __________________________________________________________________________________________________ block_9_depthwise (DepthwiseCon (None, 14, 14, 384) 3456 block_9_expand_relu[0][0] __________________________________________________________________________________________________ block_9_depthwise_BN (BatchNorm (None, 14, 14, 384) 1536 block_9_depthwise[0][0] __________________________________________________________________________________________________ block_9_depthwise_relu (ReLU) (None, 14, 14, 384) 0 block_9_depthwise_BN[0][0] __________________________________________________________________________________________________ block_9_project (Conv2D) (None, 14, 14, 64) 24576 block_9_depthwise_relu[0][0] __________________________________________________________________________________________________ block_9_project_BN (BatchNormal (None, 14, 14, 64) 256 block_9_project[0][0] __________________________________________________________________________________________________ block_9_add (Add) (None, 14, 14, 64) 0 block_8_add[0][0] block_9_project_BN[0][0] __________________________________________________________________________________________________ block_10_expand (Conv2D) (None, 14, 14, 384) 24576 block_9_add[0][0] __________________________________________________________________________________________________ block_10_expand_BN (BatchNormal (None, 14, 14, 384) 1536 block_10_expand[0][0] __________________________________________________________________________________________________ block_10_expand_relu (ReLU) (None, 14, 14, 384) 0 block_10_expand_BN[0][0] __________________________________________________________________________________________________ block_10_depthwise (DepthwiseCo (None, 14, 14, 384) 3456 block_10_expand_relu[0][0] __________________________________________________________________________________________________ block_10_depthwise_BN (BatchNor (None, 14, 14, 384) 1536 block_10_depthwise[0][0] __________________________________________________________________________________________________ block_10_depthwise_relu (ReLU) (None, 14, 14, 384) 0 block_10_depthwise_BN[0][0] __________________________________________________________________________________________________ block_10_project (Conv2D) (None, 14, 14, 96) 36864 block_10_depthwise_relu[0][0] __________________________________________________________________________________________________ block_10_project_BN (BatchNorma (None, 14, 14, 96) 384 block_10_project[0][0] __________________________________________________________________________________________________ block_11_expand (Conv2D) (None, 14, 14, 576) 55296 block_10_project_BN[0][0] __________________________________________________________________________________________________ block_11_expand_BN (BatchNormal (None, 14, 14, 576) 2304 block_11_expand[0][0] __________________________________________________________________________________________________ block_11_expand_relu (ReLU) (None, 14, 14, 576) 0 block_11_expand_BN[0][0] __________________________________________________________________________________________________ block_11_depthwise (DepthwiseCo (None, 14, 14, 576) 5184 block_11_expand_relu[0][0] __________________________________________________________________________________________________ block_11_depthwise_BN (BatchNor (None, 14, 14, 576) 2304 block_11_depthwise[0][0] __________________________________________________________________________________________________ block_11_depthwise_relu (ReLU) (None, 14, 14, 576) 0 block_11_depthwise_BN[0][0] __________________________________________________________________________________________________ block_11_project (Conv2D) (None, 14, 14, 96) 55296 block_11_depthwise_relu[0][0] __________________________________________________________________________________________________ block_11_project_BN (BatchNorma (None, 14, 14, 96) 384 block_11_project[0][0] __________________________________________________________________________________________________ block_11_add (Add) (None, 14, 14, 96) 0 block_10_project_BN[0][0] block_11_project_BN[0][0] __________________________________________________________________________________________________ block_12_expand (Conv2D) (None, 14, 14, 576) 55296 block_11_add[0][0] __________________________________________________________________________________________________ block_12_expand_BN (BatchNormal (None, 14, 14, 576) 2304 block_12_expand[0][0] __________________________________________________________________________________________________ block_12_expand_relu (ReLU) (None, 14, 14, 576) 0 block_12_expand_BN[0][0] __________________________________________________________________________________________________ block_12_depthwise (DepthwiseCo (None, 14, 14, 576) 5184 block_12_expand_relu[0][0] __________________________________________________________________________________________________ block_12_depthwise_BN (BatchNor (None, 14, 14, 576) 2304 block_12_depthwise[0][0] __________________________________________________________________________________________________ block_12_depthwise_relu (ReLU) (None, 14, 14, 576) 0 block_12_depthwise_BN[0][0] __________________________________________________________________________________________________ block_12_project (Conv2D) (None, 14, 14, 96) 55296 block_12_depthwise_relu[0][0] __________________________________________________________________________________________________ block_12_project_BN (BatchNorma (None, 14, 14, 96) 384 block_12_project[0][0] __________________________________________________________________________________________________ block_12_add (Add) (None, 14, 14, 96) 0 block_11_add[0][0] block_12_project_BN[0][0] __________________________________________________________________________________________________ block_13_expand (Conv2D) (None, 14, 14, 576) 55296 block_12_add[0][0] __________________________________________________________________________________________________ block_13_expand_BN (BatchNormal (None, 14, 14, 576) 2304 block_13_expand[0][0] __________________________________________________________________________________________________ block_13_expand_relu (ReLU) (None, 14, 14, 576) 0 block_13_expand_BN[0][0] __________________________________________________________________________________________________ block_13_pad (ZeroPadding2D) (None, 15, 15, 576) 0 block_13_expand_relu[0][0] __________________________________________________________________________________________________ block_13_depthwise (DepthwiseCo (None, 7, 7, 576) 5184 block_13_pad[0][0] __________________________________________________________________________________________________ block_13_depthwise_BN (BatchNor (None, 7, 7, 576) 2304 block_13_depthwise[0][0] __________________________________________________________________________________________________ block_13_depthwise_relu (ReLU) (None, 7, 7, 576) 0 block_13_depthwise_BN[0][0] __________________________________________________________________________________________________ block_13_project (Conv2D) (None, 7, 7, 160) 92160 block_13_depthwise_relu[0][0] __________________________________________________________________________________________________ block_13_project_BN (BatchNorma (None, 7, 7, 160) 640 block_13_project[0][0] __________________________________________________________________________________________________ block_14_expand (Conv2D) (None, 7, 7, 960) 153600 block_13_project_BN[0][0] __________________________________________________________________________________________________ block_14_expand_BN (BatchNormal (None, 7, 7, 960) 3840 block_14_expand[0][0] __________________________________________________________________________________________________ block_14_expand_relu (ReLU) (None, 7, 7, 960) 0 block_14_expand_BN[0][0] __________________________________________________________________________________________________ block_14_depthwise (DepthwiseCo (None, 7, 7, 960) 8640 block_14_expand_relu[0][0] __________________________________________________________________________________________________ block_14_depthwise_BN (BatchNor (None, 7, 7, 960) 3840 block_14_depthwise[0][0] __________________________________________________________________________________________________ block_14_depthwise_relu (ReLU) (None, 7, 7, 960) 0 block_14_depthwise_BN[0][0] __________________________________________________________________________________________________ block_14_project (Conv2D) (None, 7, 7, 160) 153600 block_14_depthwise_relu[0][0] __________________________________________________________________________________________________ block_14_project_BN (BatchNorma (None, 7, 7, 160) 640 block_14_project[0][0] __________________________________________________________________________________________________ block_14_add (Add) (None, 7, 7, 160) 0 block_13_project_BN[0][0] block_14_project_BN[0][0] __________________________________________________________________________________________________ block_15_expand (Conv2D) (None, 7, 7, 960) 153600 block_14_add[0][0] __________________________________________________________________________________________________ block_15_expand_BN (BatchNormal (None, 7, 7, 960) 3840 block_15_expand[0][0] __________________________________________________________________________________________________ block_15_expand_relu (ReLU) (None, 7, 7, 960) 0 block_15_expand_BN[0][0] __________________________________________________________________________________________________ block_15_depthwise (DepthwiseCo (None, 7, 7, 960) 8640 block_15_expand_relu[0][0] __________________________________________________________________________________________________ block_15_depthwise_BN (BatchNor (None, 7, 7, 960) 3840 block_15_depthwise[0][0] __________________________________________________________________________________________________ block_15_depthwise_relu (ReLU) (None, 7, 7, 960) 0 block_15_depthwise_BN[0][0] __________________________________________________________________________________________________ block_15_project (Conv2D) (None, 7, 7, 160) 153600 block_15_depthwise_relu[0][0] __________________________________________________________________________________________________ block_15_project_BN (BatchNorma (None, 7, 7, 160) 640 block_15_project[0][0] __________________________________________________________________________________________________ block_15_add (Add) (None, 7, 7, 160) 0 block_14_add[0][0] block_15_project_BN[0][0] __________________________________________________________________________________________________ block_16_expand (Conv2D) (None, 7, 7, 960) 153600 block_15_add[0][0] __________________________________________________________________________________________________ block_16_expand_BN (BatchNormal (None, 7, 7, 960) 3840 block_16_expand[0][0] __________________________________________________________________________________________________ block_16_expand_relu (ReLU) (None, 7, 7, 960) 0 block_16_expand_BN[0][0] __________________________________________________________________________________________________ block_16_depthwise (DepthwiseCo (None, 7, 7, 960) 8640 block_16_expand_relu[0][0] __________________________________________________________________________________________________ block_16_depthwise_BN (BatchNor (None, 7, 7, 960) 3840 block_16_depthwise[0][0] __________________________________________________________________________________________________ block_16_depthwise_relu (ReLU) (None, 7, 7, 960) 0 block_16_depthwise_BN[0][0] __________________________________________________________________________________________________ block_16_project (Conv2D) (None, 7, 7, 320) 307200 block_16_depthwise_relu[0][0] __________________________________________________________________________________________________ block_16_project_BN (BatchNorma (None, 7, 7, 320) 1280 block_16_project[0][0] __________________________________________________________________________________________________ Conv_1 (Conv2D) (None, 7, 7, 1280) 409600 block_16_project_BN[0][0] __________________________________________________________________________________________________ Conv_1_bn (BatchNormalization) (None, 7, 7, 1280) 5120 Conv_1[0][0] __________________________________________________________________________________________________ out_relu (ReLU) (None, 7, 7, 1280) 0 Conv_1_bn[0][0] __________________________________________________________________________________________________ global_average_pooling2d (Globa (None, 1280) 0 out_relu[0][0] __________________________________________________________________________________________________ predictions (Dense) (None, 1000) 1281000 global_average_pooling2d[0][0] ================================================================================================== Total params: 3,538,984 Trainable params: 3,504,872 Non-trainable params: 34,112 __________________________________________________________________________________________________ In [5]: ### edTest(test_chow1) ### # Find the last convolutional layer # Inspect the model summary and find the last convolution layer # Get the name of the last convolution layer conv_layer_name = model . layers [ - 3 ] . name print ( conv_layer_name ) out_relu In [6]: # Take a sample image to find the saliency map img_path = './cat.png' # Load the image with the target_size for mobilenet img = image . load_img ( img_path , target_size = ( 224 , 224 )) # Convert the image to a numpy array x = image . img_to_array ( img ) # Add an extra dimension for batch size # to change it to (1,224,224,3) x = np . expand_dims ( x , axis = 0 ) # Use the MobileNetV2 preprocess_input function on the image x = preprocess_input ( x ) In [7]: # Use the pretrained model to make a prediction preds = model . predict ( x ) # Useful dictionary to go from label index to actual label with open ( 'idx2name.pkl' , 'rb' ) as handle : keras_idx_to_name = pickle . load ( handle ) In [8]: # See what the output predictions is: prediction_class = keras_idx_to_name [ np . argmax ( preds , axis = 1 ) . item ( 0 )] print ( f 'Prediction class is { prediction_class } ' ) Prediction class is Egyptian cat In [9]: # We use the tf.keras Functional API to get # 1. The model prediction probabilities # 2. The feature maps after the last convolution in the model # Get the last convolution layer in the network last_conv_layer = model . get_layer ( conv_layer_name ) # Get the output predictions and the last_conv_layer # Using tf.keras functional API get_maps = Model ( inputs = [ model . inputs ], outputs = [ model . output , last_conv_layer . output ]) In [10]: # Now we perform the Grad-CAM, # We take the gradient of the output with respect to the feature maps # after the convolution with tf . GradientTape () as tape : # Getting the required outputs model_out , last_conv_layer = get_maps ( x ) # We choose the output with maximum probability # But this can be different depending on your choice # For eg. you could select the second highest probability value class_out = tf . reduce_max ( model_out ) ‚è∏ Take the gradients In [11]: ### edTest(test_chow2) ### # We take the gradients # tape.gradient() takes the gradient of something with respect to # something else. Here we want the derivative of the output class # with respect to to the last conv layer grads = tape . gradient ( class_out , last_conv_layer ) In [12]: # Here we combine all the gradients for each feature map pooled_grads = tf . reduce_mean ( grads , axis = ( 0 , 1 , 2 )) # As per grad-CAM literature, here we need to multiply # the pooled grads with each feature map and take the average across # all the feature maps to make the heat map heatmap = tf . reduce_mean ( tf . multiply ( pooled_grads , last_conv_layer ), axis =- 1 ) In [13]: # Below we convert heatmap to numpy # Make all values positive # and reshape from (1,7,7) to (7,7) for ease of plotting heatmap = heatmap . numpy () heatmap [ heatmap < 0 ] = 0 #relu heatmap = ( heatmap - heatmap . min ()) / ( heatmap . max () - heatmap . min ()) heatmap = heatmap . reshape (( 7 , 7 )) # We plot the (7,7) heatmap plt . imshow ( heatmap , cmap = 'jet' ) plt . show () In [14]: # Inorder to map to the original image # This heatmap has to be be resized resized_heatmap = np . uint8 ( cv2 . resize ( heatmap ,( 224 , 224 )) * 255 ) In [15]: # We need to add a pre-processing step # to convert the grayscale heatmap # to a true JET colormap of 3 channels # for ease of viewing val = np . uint8 ( 256 - resized_heatmap ) heatmap_final = cv2 . applyColorMap ( val , cv2 . COLORMAP_JET ) In [16]: # We also prepare the image for plotting # by converting to tensor # and converting dtype to int8 img = image . img_to_array ( img ) img = np . uint8 ( img ) In [17]: # Finally, we use the cv2.addweighted function # to superimpose the heatmap on the original image # Use the helper code below to do the same fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 6 )) ax . imshow ( cv2 . addWeighted ( heatmap_final , 0.5 , img , 0.5 , 0 )) ax . axis ( 'off' ); fig . suptitle ( f 'Predicted class: { prediction_class } ' , y = 0.92 , fontsize = 14 ); plt . show (); ‚è∏ Will Grad-CAM work if we took the output from the last ReLU instead ? (True or False) In [18]: ### edTest(test_chow3) ### # Type your answer within in the quotes given answer3 = 'True' ‚è∏ The heatmap output is displaying: A: The weights of the layer B: A 7x7 mask from the input image C: The pixels that activates the most in red and the least in blues D: A feature map of the input image In [20]: ### edTest(test_chow4) ### # Type your answer within in the quotes given answer4 = 'C' In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture17/notebook/"
    }, {
        "title": "Lecture 16: œç Backprop max pooling, Receptive Fields and feature map viz",
        "text": "Slides Lecture 16: CNN2 (PDF) Exercises Lecture 16: Investigating CNNs (Notebook) Lecture 16: Image Occlusion (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture16/"
    }, {
        "title": "Image Occlusion",
        "text": "Title : Image Occlusion Description : The aim of this exercise is to understand occlusion. Each pixel in an image has varying importance for the classification of the image. Occlusion involves running a patch over the entire image to see which pixels affect the classification the most. Instructions: Define a convolutional neural network based on the architecture mentioned in the scaffold. Load the trained model weights given in the occlusion_model_weights.h5 file. Take a quick look at the model architecture using model.summary() . Use the helper function occlusion to visualize the delta loss as a mask moves across the image. The output will look similar to the one shown below. Hints: MaxPooling2D() Max pooling operation for 2D spatial data. compile() Configures the model for training. Conv2D() 2D convolution layer (e.g. spatial convolution over images). flatten() Flattens the input. Dense() A regular densely-connected NN layer. NOTE - To run the test, comment the cells that call the occlusion function and then click on Mark . In [1]: # Import necessary libraries import os import random import numpy as np import pandas as pd import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras import backend as K from sklearn.metrics import accuracy_score from helper import occlusion , load_dataset from tensorflow.keras.optimizers import SGD from tensorflow.keras.datasets import cifar10 from tensorflow.keras.utils import to_categorical from tensorflow.keras.applications import MobileNet from sklearn.model_selection import train_test_split from tensorflow.keras.models import Sequential , Model from tensorflow.keras.applications.mobilenet import preprocess_input from tensorflow.keras.layers import Dense , Dropout , Flatten , Activation , Input , Conv2D , MaxPooling2D , InputLayer , ReLU % matplotlib inline In [2]: # Initialize a sequential model model = Sequential ( name = \"Occlusion\" ) # First convolution layer model . add ( Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 32 , 32 , 3 ))) # Second convolution layer model . add ( Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' )) # First max-pooling layer model . add ( MaxPooling2D (( 2 , 2 ))) # Third convolution layer model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' )) # Fourth convolution layer model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' )) # Second max-pooling layer model . add ( MaxPooling2D (( 2 , 2 ))) # Fifth convolution layer model . add ( Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' )) # Sixth convolution layer model . add ( Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' )) # Third max-pooling layer model . add ( MaxPooling2D (( 2 , 2 ))) # Flatten layer model . add ( Flatten ()) # Fully connected dense layer model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' )) # Output layer model . add ( Dense ( 10 , activation = 'softmax' )) # Compiling the model model . compile ( optimizer = SGD ( lr = 0.001 , momentum = 0.9 ), loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) In [0]: # Take a quick look at the model summary model . summary () In [4]: # Load the weights of the pre-trained model model . load_weights ( \"occlusion_model_weights.h5\" ) ‚è∏ Call the function occlusion (below) with image numbers 10, 12 and 35. What do you observe based on the occlusion map plotted for each image? A. The images are blurred more as compared to other images in the set. B. The images are incorrectly predicted because the model weights the wrong parts of the image to make the prediction. C. The images are correctly predicted as the network is giving high importance to the most telling features of the images. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' In [0]: # Call the helper function occlusion with # the trained model, a valid image number within 50, occlusion # patch size img_num = ___ patch_size = ___ occlusion ( model , img_num , patch_size ) ‚è∏ Call the occlusion function (below) with images 1, 15 and 30. What do you observe based on the plots? In [0]: ### edTest(test_chow2) ### # Type your answer here answer2 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture16/notebook1/"
    }, {
        "title": "Investigating CNNs",
        "text": "Title : Investigating CNNs Description : The goal of the exercise is to investigate the building blocks of a CNN, such as kernels, filters, and feature maps using a CNN model trained on the CIFAR-10 dataset . Instructions: Import the CIFAR-10 dataset, and the pre-trained model from the helper file by calling the get_cifar10() function. Evaluate the model on the test set in order to verify if the selected model has trained weights. You should get a test set accuracy of about 75% . Take a quick look at the model architecture using model.summary() . Investigate the weights of the pre-trained model and plot the weights of the 1st filter of the 1st convolution layer. Plot all the filters of the first convolution layer. Use the helper code give to visualize the feature maps of the first convolution layer along with the input image. Use the helper code give to visualize the activations of the first convolution layer along with the input image. Hints: model.layers Accesses various layers of the model model.predict() Used to predict the values given the model model.layers.get_weights() Get the weights of a particular layer tensorflow.keras.Model() Functional API to group layers into an object with training and inference features. Visual Demonstration of CNNs In [0]: # Import necessary libraries import numpy as np import random import pandas as pd import matplotlib.pyplot as plt % matplotlib inline import tensorflow as tf from tensorflow.keras.models import Sequential , Model from matplotlib import cm import helper from helper import cnn_model , get_cifar10 , plot_featuremaps In [0]: # As we are using a pre-trained model, # we will only use 1000 images from the 'unseen' test data # The get_cifar10() function will load 1000 cifar10 images ( x_test , y_test ) = get_cifar10 () # We also provide a handy dictionary to map response values to image labels cifar10dict = helper . cifar10dict cifar10dict In [0]: # Let's look at some sample images with their labels # Run the helper code below to plot the image and its label num_images = 5 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 12 )) for i in range ( num_images ): image_index = random . randint ( 0 , 1000 ) img = ( x_test [ image_index ] + 0.5 ) ax [ i ] . imshow ( img ) label = cifar10dict [ np . argmax ( y_test [ image_index ])] ax [ i ] . set_title ( f 'Actual: { label } ' ) ax [ i ] . axis ( 'off' ) In [0]: # For this exercise we use a pre-trained network by calling # the cnn_model() function model = cnn_model () model . summary () In [0]: # Evaluate the pretrained model on the test set model_score = model . evaluate ( x_test , y_test ) print ( f 'The test set accuracy for the pre-trained model is { 100 * model_score [ 1 ] : .2f } %' ) In [0]: # Visualizing the predictions on 5 randomly selected images num_images = 5 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 12 )) for i in range ( num_images ): image_index = random . randint ( 0 , 1000 ) prediction = cifar10dict [ int ( np . squeeze ( np . argmax ( model . predict ( x_test [ image_index : image_index + 1 ]), axis = 1 ), axis = 0 ))] img = ( x_test [ image_index ] + 0.5 ) ax [ i ] . imshow ( img ) ax [ i ] . set_title ( f 'Predicted: { prediction } \\n Actual: { cifar10dict [ np . argmax ( y_test [ image_index : image_index + 1 ])] } ' ) ax [ i ] . axis ( 'off' ) Visualize kernels corresponding to the filters for the 1st layer In [0]: # The 'weights' variable is of the form # [height, width, channel, number of filters] # Use .get_weights() with the appropriate layer number # to get the weights and bias of the first layer i.e. layer number 0 weights , bias = ___ assert weights . shape == ( 3 , 3 , 3 , 32 ), \"Computed weights are incorrect\" In [0]: # How many filters are in the first convolution layer? n_filters = ___ print ( f 'Number of filters: { n_filters } ' ) # Print the filter size filter_channel = ___ filter_height = ___ filter_width = ___ print ( f 'Number of channels { filter_channel } ' ) print ( f 'Filter height { filter_height } ' ) print ( f 'Filter width { filter_width } ' ) ‚è∏ Based on the dimensions of the input image given to the defined model, how many kernels constitute the first filter? A. $3$ B. $32$ C. $1$ D. $24$ In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' In [0]: # The 'weights' variable (defined above) is of the form # [height, width, channel, number of filters] # From this select all three channels, the entire length and width # and the first filter kernels_filter1 = ___ # Test case to check if you have indexed correctly assert kernels_filter1 . shape == ( 3 , 3 , 3 ) In [0]: # Use the helper code below to plot each kernel of the choosen filter fig , axes = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) colors = [ 'Reds' , 'Greens' , 'Blues' ] for num , i in enumerate ( axes ): i . imshow ( kernels_filter1 [ num ], cmap = colors [ num ]) i . set_title ( f 'Kernel for { colors [ num ] } channel' ) Visualizing one filter for the first convolutional layer Each of the above kernels stacked together forms a filter, which interacts with the input. In [0]: # For the same filter above, we perform normalization because the current # values are between -1 and 1 and the imshow function would truncate all values # less than 0 making the visual difficult to infer from. kernels_filter1 = ( kernels_filter1 - kernels_filter1 . min ()) / ( kernels_filter1 . max () - kernels_filter1 . min ()) # Plotting the filter fig , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 )) ax . imshow ( kernels_filter1 ) ax . set_title ( f '1st Filter of convolution' ) Visualizing all the filters (32) for the first convolutional layer In [0]: # Use the helper code below to visualize all filters for the first layer fig , ax = plt . subplots ( 4 , 8 , figsize = ( 14 , 14 )) fig . subplots_adjust ( bottom = 0.2 , top = 0.5 ) for i in range ( 4 ): for j in range ( 8 ): filters = weights [:,:,:,( 8 * i ) + j ] filters = ( filters - filters . min ()) / ( filters . max () - filters . min ()) ax [ i , j ] . imshow ( filters ) fig . suptitle ( 'All 32 filters for 1st convolution layer' , fontsize = 20 , y = 0.53 ); Visualize Feature Maps & Activations ‚è∏ Which of the following statements is true? A. Feature maps are a collection of weights, and filters are outputs of convolved inputs. B. Filters are a collection of learned weights, and feature maps are outputs of convolved inputs. C. Feature maps are learned features of a trained CNN model. D. Filters are the outputs of an activation layer on a feature map. In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer2 = '___' In [0]: # Use model.layers to get a list of all the layers in the model layers_list = ___ print ( ' \\n ' . join ([ layer . name for layer in layers_list ])) For this exercise, we take a look at only the first convolution layer and the first activation layer. In [0]: # Get the output of the first convolution layer layer0_output = model . layers [ 0 ] . output In [0]: # Use the tf.keras functional API : Model(inputs= , outputs = ) where # the input will come from model.input and output will be layer0_output feature_model = ___ # Use a sample image from the test set to visualize the feature maps img = x_test [ 16 ] . reshape ( 1 , 32 , 32 , 3 ) # NOTE: We have to reshape the image to 4-d tensor so that # it can input to the trained model In [0]: # Use the helper code below to plot the feature maps features = feature_model . predict ( img ) plot_featuremaps ( img , features ,[ model . layers [ 0 ] . name ]) Visualizing the first activation In [0]: # Get the output of the first activation layer layer1_output = model . layers [ 1 ] . output In [0]: # Follow the same steps as above for the next layer activation_model = ___ In [0]: # Use the helper code to again visualize the outputs img = x_test [ 16 ] . reshape ( 1 , 32 , 32 , 3 ) activations = activation_model . predict ( img ) # You can download the plot_featuremaps helper file # to see how exactly do we make this nice plot below plot_featuremaps ( img , activations ,[ model . layers [ 1 ] . name ]) ‚è∏ Using the feature maps, is it possible to locate the part of the image that is most responsible for predicting the output class? A. Yes B. No In [0]: ### edTest(test_chow3) ### # Submit an answer choice as a string below # (eg. if you choose option B, put 'B') answer3 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture16/notebook2/"
    }, {
        "title": "Lecture 15: ‚ç∫ CNNs Pooling and CNNs Structure",
        "text": "Slides Lecture 15: CNN2 (PDF) Exercises Lecture 15: Pooling Mechanics (Notebook) Lecture 15: Avg vs Max Pooling (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture15/"
    }, {
        "title": "Avg vs Max Pooling",
        "text": "Title : Pooling Mechanics Description : The aim of this exercise is to understand the tensorflow.keras implementation of: Max Pooling Average Pooling Instructions : First, implement Max Pooling by building a model with a single MaxPooling2D layer. Print the output of this layer by using model.predict() to show the output. Next, implement Average Pooling by building a model with a single AvgPooling2D layer. Print the output of this layer by using model.predict() to show the output. Hints: tf.keras.layers.MaxPooling2D() Max pooling operation for 2D spatial data. tf.keras.layers.AveragePooling2D() Average pooling operation for spatial data. np.squeeze() Remove single-dimensional entries from the shape of an array. np.expand_dims() Add single-dimensional entries from the shape of an array. Example: np.expand_dims (img, axis=(0,3)) In [2]: # Import necessary libraries import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import MaxPool2D , AveragePooling2D , Input from helper import plot_pool --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) in 2 import matplotlib . pyplot as plt 3 import numpy as np ----> 4 import tensorflow as tf 5 from tensorflow . keras . models import Sequential 6 from tensorflow . keras . layers import MaxPool2D , AveragePooling2D , Input ModuleNotFoundError : No module named 'tensorflow' In [20]: # Load the 7x7 mnist image img = np . load ( '3.npy' ) plt . imshow ( img , cmap = 'bone' , alpha = 0.5 ); plt . axis ( 'off' ); plt . title ( 'MNIST image of 3' , fontsize = 20 ); ‚è∏ Consider an input of size $(7,7)$ pixels.What will be the dimensions of the output if you use pool_size=2 , strides = 1 & padding='valid' ? A. $(5,5)$ B. $(6,6)$ C. $(4,4)$ D. $(7,7)$ In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' Max Pooling In [16]: # Specify the variables for pooling pool_size = ___ strides = ___ # Padding parameter can be 'valid', 'same', etc. padding = '___' # Build the model to perform maxpooling operation model_1 = Sequential ( name = 'MaxPool' ) model_1 . add ( Input ( shape = np . expand_dims ( img , axis = 2 ) . shape )) model_1 . add ( MaxPool2D ( pool_size = pool_size , strides = strides , padding = padding )) # Take a look at the summary to see the output shape model_1 . summary () Model: \"MaxPool\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= max_pooling2d_3 (MaxPooling2 (None, 6, 6, 1) 0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ In [17]: # Output the image using the model above # Remember to use np.expand_dims to change input image dimensions # to 4-d tensor because model_1.predict will not work on 2-d tensor pooled_img = model_1 . predict ( ___ ) # Use the helper code to visualize the pooling operation # np.squeeze() is used to bring the image to 2-dimension # to use matplotlib to plot it pooled_img = pooled_img . squeeze () # plot_pool is a function that will return 3 plots to help visualize # the pooling operation plot_pool ( img , pooled_img ) WARNING:tensorflow:5 out of the last 5 calls to .predict_function at 0x1442bb700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. ‚è∏ What if your stride is larger than your pool size? A. Operation is invalid B. Operation is valid but you will have an output larger than the input C. Operation is valid but you will miss out on some pixels D. Operation is valid but you will have an output as the same size as the input In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below # (eg. if you choose option C, put 'C') answer2 = '___' Average Pooling In [18]: # Specify the variables for pooling pool_size = ___ strides = ___ # Padding parameter can be 'valid', 'same', etc. padding = '___' # Build the model to perform average pooling operation model_2 = Sequential ( name = 'AveragePool' ) model_2 . add ( Input ( shape = np . expand_dims ( img , axis = 2 ) . shape )) model_2 . add ( AveragePooling2D ( pool_size = pool_size , strides = strides , padding = padding )) model_2 . summary () Model: \"AveragePool\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= average_pooling2d_1 (Average (None, 2, 2, 1) 0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ In [19]: # Output the image using the model above # Remember to use np.expand_dims to change input image dimensions # to 4-d tensor because model_1.predict will not work on 2-d tensor pooled_img = model_2 . predict ( ___ ) # Use the helper code to visualize the pooling operation pooled_img = pooled_img . squeeze () plot_pool ( img , pooled_img ) WARNING:tensorflow:6 out of the last 6 calls to .predict_function at 0x1091618b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. ‚è∏ Which among the following 2 pooling operation activates the input image more? Answer based on your results above. A. Average pooling B. Max pooling In [1]: ### edTest(test_chow3) ### # Submit an answer choice as a string below # (eg. if you choose option A, put 'a') answer3 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture15/notebook1/"
    }, {
        "title": "Pooling Mechanics",
        "text": "Title : Pooling Mechanics Description : The aim of this exercise is to understand the difference between average and max pooling by comparing the accuracy and number of parameters for the classification of MNIST digits. Instructions : Use the helper function get_data() to get the train and test data. Define a function cnn_model that returns a Convolutional Neural Network whose architecture varies based on a variable pool_type: When pool_type is no_pooling the model does not have any pooling layers. When pool_type is max_pooling add a max-pooling layer to the model. When pool_type is avg_pooling add an average-pooling layer to the model. Compile the model and fit it on the training data. Call the function thrice: Once for a model with no pooling layer. Once for a model with average pooling. Once for a model with max pooling. For each of the above mentioned calls, compute the number of parameters in the model and the accuracy of the model on the test data. Use the helper code given to visualise the computed accuracy, loss and number of parameters of all 3 models. Hints: MaxPooling2D() Max pooling operation for 2D spatial data. AveragePooling2D() Average pooling operation for spatial data. NOTE - In the case of pooling layers, if no stride size is mentioned the default size is the size of the pooling. compile() Configures the model for training. Conv2D() 2D convolution layer (e.g. spatial convolution over images). flatten() Flattens the input. Dense() A regular densely-connected NN layer. Dropout() Applies Dropout to the corresponding input In [0]: # Import necessary libraries import numpy as np import tensorflow as tf from numpy.random import seed import matplotlib.pyplot as plt from prettytable import PrettyTable from helper import get_data , plot_activation from tensorflow.keras.models import Sequential from sklearn.model_selection import train_test_split from tensorflow.keras.layers import Dense , Conv2D , Dropout , Flatten , MaxPooling2D , AveragePooling2D # Set random seed seed ( 1 ) tf . random . set_seed ( 1 ) % matplotlib inline In [0]: # Use the helper function get_data to get the train and # test MNIST dataset x_train , y_train , x_test , y_test = get_data () In [0]: # Setting the random seed seed ( 1 ) tf . random . set_seed ( 1 ) # Function to define the CNN model for MNIST classification def cnn_model ( pool_type = \"no_pooling\" ): # Intialize a sequential model model = Sequential ( name = pool_type ) # Define the input shape input_shape = ( 28 , 28 , 1 ) # Add a convolutional layer with 28 filters, kernel size of 3, # input_shape as input_shape defined above and tanh activation model . add ( ___ ) # Define size of the pooling operation pool_size = ( 3 , 3 ) # Add an average pooling layer with pool size value as defined # above by pool_size if pool_type == \"avg_pooling\" : model . add ( ___ ) # Add a max pooling layer based with pool size value as defined # above by pool_size if pool_type == \"max_pooling\" : model . add ( ___ ) # Add a flatten layer model . add ( ___ ) # Add a dense layer with ReLU activation with 16 nodes model . add ( ___ ) # Add a dropout layer with 0.3 as the dropout percentage model . add ( ___ ) # Add an output layer with 10 nodes and softmax activation model . add ( ___ ) # Compile the model with adam optimizer, # sparse_categorical_crossentropy as the loss # and accuracy as the metric model . compile ( ___ ) # Fit the model on the train data with 8 epochs model . fit ( x_train , y_train , epochs = 8 , verbose = 0 , shuffle = False , workers = 0 , use_multiprocessing = False ) return model In [0]: ### edTest(test_no_pool) ### # Call the cnn_model function with pool_type as no_pooling # to get the trained model without pooling model = cnn_model ( pool_type = \"no_pooling\" ) # Evaluate on the test data no_pool_acc = model . evaluate ( x_test , y_test ) print ( \"The accuracy of the model with no pooling is\" , no_pool_acc [ 1 ]) # Get the number of parameters of the network no_pool_params = model . count_params () In [0]: ### edTest(test_avg_pool) ### # Call the cnn_model function with pool_type as avg_pooling # to get the trained model with avg pooling model = cnn_model ( pool_type = \"avg_pooling\" ) # Evaluate on the test data avg_pool_acc = model . evaluate ( x_test , y_test ) print ( \"The accuracy of the model with average pooling is\" , avg_pool_acc [ 1 ]) # Get the number of parameters of the network avg_pool_params = model . count_params () In [0]: ### edTest(test_max_pool) ### # Call the cnn_model function with pool_type as max_pooling # to get the trained model with max pooling model = cnn_model ( pool_type = \"max_pooling\" ) # Evaluate on the test data max_pool_acc = model . evaluate ( x_test , y_test ) print ( \"The accuracy of the model with max pooling is\" , max_pool_acc [ 1 ]) # Get the number of parameters of the network max_pool_params = model . count_params () ‚è∏ Based on the results seen here, which of the following is the most true? A. The average pooling provides no advantage over no pooling models. B. The no pooling model is more robust and reliable for all datasets. C. The max pooling and average pooling though have lower number of parameters takes longer time to train than the no pooling model. D. The max pooling model performs better as MNIST is made up of mostly edges and high contrasts which provide for max pooling to easily identify the sharp edges. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option C, put 'C') answer1 = '___' In [0]: ### edTest(test_accuracy) ### # Display the models with their accuracy score and parameters table = PrettyTable () table . field_names = [ \"Model Type\" , \"Test Accuracy\" , \"Test Loss\" , \"Number of Parameters\" ] table . add_row ([ \"Without pooling\" , round ( no_pool_acc [ 1 ], 4 ), round ( no_pool_acc [ 0 ], 4 ), no_pool_params ]) table . add_row ([ \"With avg pooling\" , round ( avg_pool_acc [ 1 ], 4 ), round ( avg_pool_acc [ 0 ], 4 ), avg_pool_params ]) table . add_row ([ \"With max pooling\" , round ( max_pool_acc [ 1 ], 4 ), round ( max_pool_acc [ 0 ], 4 ), max_pool_params ]) print ( table ) ‚è∏ How does the accuracy and loss of the model vary by increasing the pool_size to (5x5)? Why does this happen? In [0]: ### edTest(test_chow2) ### # Type your answer within in the quotes given answer2 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture15/notebook2/"
    }, {
        "title": "Lecture 14: Œ† CNNs basics",
        "text": "Slides Lecture 14 : CNNs1 - Basics (PDF) Exercises Lecture 14: Feed-Forward Neural Networks vs Convolution Neural Networks (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture14/"
    }, {
        "title": "Lecture 13: Hierarchical Models (Lab)",
        "text": "Exercises Lecture 13: Bayes - Exercise 1 (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture13/"
    }, {
        "title": "Lecture 13 - Hierarchical Models (Lab)",
        "text": "Title : MCMC and PyMC3 Hierarchical Models Description : Download this notebook on your local environment. Some parts of it are time-consuming, so we are providing the output cells as well. Once you have gone through them, make changes and run the cells again. CS109B Data Science 2: Advanced Topics in Data Science Lecture 13 - MCMC and Hierarchical Models in PyMC3 Harvard University Spring 2021 Instructors: Pavlos Protopapas, Mark Glickman, and Chris Tanner Additional Instructor: Eleni Angelaki Kaxiras Content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import pymc3 as pm from pymc3 import summary import arviz as az from matplotlib import gridspec # Ignore a common pymc3 warning that comes from library functions, not our code. # Pymc3 may throw additional warnings, but other warnings should be manageable # by following the instructions included within the warning messages. import warnings messages = [ \"Using `from_pymc3` without the model will be deprecated in a future release\" , ] # or silence all warnings (not recommended) # warnings.filterwarnings('ignore') for m in messages : warnings . filterwarnings ( \"ignore\" , message = m ) print ( f \"Using PyMC3 version: { pm . __version__ } \" ) print ( f \"Using ArviZ version: { az . __version__ } \" ) Using PyMC3 version: 3.8 Using ArviZ version: 0.7.0 In [3]: import pymc3 as pm from pymc3 import summary #import arviz as az from matplotlib import gridspec In [4]: import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline In [5]: # from pymc3 import Model, Normal, HalfNormal, model_to_graphviz # from pymc3 import NUTS, sample, find_MAP from scipy import optimize In [6]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; In [7]: #pandas trick pd . options . display . max_columns = 50 # None -> No Restrictions pd . options . display . max_rows = 200 # None -> Be careful with this pd . options . display . max_colwidth = 100 pd . options . display . precision = 3 Learning Objectives By the end of this lab, you should be able to understand how to: run a PyMC3 bayesian model using MCMC. implement hierarchical models in PyMC3 . Table of Contents Markov Chain Monte Carlo (MCMC) Simulations in PyMC3 . Hierarchical models . Top 1. Markov Chain Monte Carlo (MCMC) Simulations PyMC3 is a Python library for programming Bayesian analysis, and more specifically, data creation, model definition, model fitting, and posterior analysis. It uses the concept of a model which contains assigned parametric statistical distributions to unknown quantities in the model. Within models we define random variables and their distributions. A distribution requires at least a name argument, and other parameters that define it. You may also use the logp() method in the model to build the model log-likelihood function. We define and fit the model. PyMC3 includes a comprehensive set of pre-defined statistical distributions that can be used as model building blocks. Although they are not meant to be used outside of a model , you can invoke them by using the prefix pm , as in pm.Normal . Monte Carlo methods are rooted in the 1940s when Enrico Fermi, John von Neumann, Stan Ulam, Nicolas Metropolis, and others, employed the use of random numbers to study physics from a stochastic point of view. The name is attributed to Metropolis 1 . For a history of the Monte Carlo Method first used in Physics see: The beginning of the Monte Carlo Method PyMC3 uses the No-U-Turn Sampler (NUTS) and the Random Walk Metropolis , two Markov chain Monte Carlo (MCMC) algorithms for sampling in posterior space. Monte Carlo gets its name because when we sample in posterior space, we choose our next move via a pseudo-random process. NUTS is a sophisticated algorithm that can handle a large number of unknown (albeit continuous) variables. In [8]: #help(pm.Poisson) Bayesian Linear Regression We will artificially create the data to predict on. We will then see if our model predicts them correctly. In [9]: np . random . seed ( 123 ) ######## True parameter values ##### our model does not see these sigma = 1 beta0 = 1 beta = [ 1 , 2.5 ] ############################### # Size of dataset size = 100 # Feature variables x1 = np . linspace ( 0 , 1. , size ) x2 = np . linspace ( 0 , 2. , size ) # Create outcome variable with random noise Y = beta0 + beta [ 0 ] * x1 + beta [ 1 ] * x2 + np . random . randn ( size ) * sigma In [10]: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () fontsize = 14 labelsize = 8 title = 'Observed Data (created artificially by ' + r '$Y(x_1,x_2)$)' ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , Y ) ax . set_xlabel ( r '$x_1$' , fontsize = fontsize ) ax . set_ylabel ( r '$x_2$' , fontsize = fontsize ) ax . set_zlabel ( r '$Y$' , fontsize = fontsize ) ax . tick_params ( labelsize = labelsize ) fig . suptitle ( title , fontsize = fontsize ) fig . tight_layout ( pad =. 1 , w_pad = 10.1 , h_pad = 2. ) #fig.subplots_adjust(); #top=0.5 plt . tight_layout plt . show () Now let's see if our model will correctly predict the values for our unknown parameters, namely $b_0$, $b_1$, $b_2$ and $\\sigma$. Defining the Problem Our problem is the following: we want to perform multiple linear regression to predict an outcome variable $Y$ which depends on variables $\\bf{x}_1$ and $\\bf{x}_2$. We will model $Y$ as normally distributed observations with an expected value $mu$ that is a linear function of the two predictor variables, $\\bf{x}_1$ and $\\bf{x}_2$. \\begin{equation} Y \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} \\begin{equation} \\mu = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 \\end{equation} where $\\sigma&#94;2$ represents the measurement error (in this example, we will use $\\sigma&#94;2 = 10$) We also choose the parameters to have normal distributions with those parameters set by us. \\begin{eqnarray} \\beta_i \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} Defining a Model in PyMC3 In [11]: with pm . Model () as my_linear_model : # Priors for unknown model parameters, specifically created stochastic random variables # with Normal prior distributions for the regression coefficients, # and a half-normal distribution for the standard deviation of the observations. # These are our parameters. P(theta) beta0 = pm . Normal ( 'beta0' , mu = 0 , sd = 10 ) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] betas = pm . Normal ( 'betas' , mu = 0 , sd = 10 , shape = 2 ) sigma = pm . HalfNormal ( 'sigma' , sd = 1 ) # mu is what is called a deterministic random variable, which implies that its value is completely # determined by its parents' values (betas and sigma in our case). # There is no uncertainty in the variable beyond that which is inherent in the parents' values mu = beta0 + betas [ 0 ] * x1 + betas [ 1 ] * x2 # Likelihood function = how probable is my observed data? # This is a special case of a stochastic variable that we call an observed stochastic. # It is identical to a standard stochastic, except that its observed argument, # which passes the data to the variable, indicates that the values for this variable were observed, # and should not be changed by any fitting algorithm applied to the model. # The data can be passed in the form of either a numpy.ndarray or pandas.DataFrame object. Y_obs = pm . Normal ( 'Y_obs' , mu = mu , sd = sigma , observed = Y ) Note : If our problem was a classification for which we would use Logistic regression see below Python Note : pm.Model is designed as a simple API that abstracts away the details of the inference. For the use of with see Compounds statements in Python. . In [12]: ## do not worry about this, it's just a nice graph to have ## you need to install python-graphviz first # conda install -c conda-forge python-graphviz pm . model_to_graphviz ( my_linear_model ) Out[12]: %3 cluster2 2 cluster100 100 sigma sigma ~ HalfNormal Y_obs Y_obs ~ Normal sigma->Y_obs beta0 beta0 ~ Normal beta0->Y_obs betas betas ~ Normal betas->Y_obs Fitting the Model with Sampling - Doing Inference See below for PyMC3's sampling method. As you can see it has quite a few parameters. Most of them are set to default values by the package. For some, it's useful to set your own values. pymc3.sampling.sample(draws=500, step=None, n_init=200000, chains=None, cores=None, tune=500, random_seed=None) Parameters to set: draws : (int): Number of samples to keep when drawing, defaults to 500. Number starts after the tuning has ended. tune : (int): Number of iterations to use for tuning the model, also called the burn-in period, defaults to 500. Samples from the tuning period will be discarded. target_accept (float in $[0, 1]$). The step size is tuned such that we approximate this acceptance rate. Higher values like 0.9 or 0.95 often work better for problematic posteriors. (optional) chains (int) number of chains to run in parallel, defaults to the number of CPUs in the system, but at most 4. pm.sample returns a pymc3.backends.base.MultiTrace object that contains the samples. We usually name it a variation of the word trace . All the information about the posterior is in trace , which also provides statistics about the sampler. How does a good trace plot look like? Is this a good one? In [13]: ## uncomment this to see more about pm.sample #help(pm.sample) In [14]: #help(pm.backends.base.MultiTrace) In [15]: with my_linear_model : print ( f 'Starting MCMC process' ) # draw nsamples posterior samples and run the default number of chains = 4 nsamples = 1000 # number of samples to keep burnin = 100 # burnin period trace = pm . sample ( nsamples , tune = burnin , target_accept = 0.8 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Starting MCMC process Multiprocess sampling (4 chains in 4 jobs) NUTS: [sigma, betas, beta0] Sampling 4 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4400/4400 [00:12<00:00, 352.64draws/s] The acceptance probability does not match the target. It is 0.8975560054708723, but should be close to 0.8. Try to increase the number of tuning steps. The acceptance probability does not match the target. It is 0.9136382172502662, but should be close to 0.8. Try to increase the number of tuning steps. The acceptance probability does not match the target. It is 0.9276833880914027, but should be close to 0.8. Try to increase the number of tuning steps. The number of effective samples is smaller than 25% for some parameters. DONE In [16]: var_names = trace . varnames var_names = var_names . remove ( 'sigma_log__' ) var_names Model Plotting PyMC3 provides a variety of visualizations via plots: https://docs.pymc.io/api/plots.html . arviz is another library that you can use. In [17]: title = f 'Traceplots for our artificial dataset with parameters: { var_names } ' pm . traceplot ( trace ); plt . suptitle ( title , fontsize = 20 ) plt . show () In [18]: az . plot_trace ( trace , var_names = var_names ); plt . suptitle ( title , fontsize = 20 ) plt . show () In [19]: # generate results table from trace samples # remember our true hidden values sigma = 1, beta0 = 1, beta = [1, 2.5] # We want R_hat < 1.1 results = az . summary ( trace , var_names = var_names ) display ( results ) mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat beta0 1.016 0.232 0.572 1.446 0.005 0.003 2313.0 2214.0 2327.0 1608.0 1.00 betas[0] 1.439 8.795 -15.281 17.336 0.366 0.259 576.0 576.0 577.0 687.0 1.00 betas[1] 2.292 4.402 -5.836 10.352 0.184 0.130 572.0 572.0 573.0 684.0 1.01 sigma 1.147 0.082 1.005 1.311 0.001 0.001 3021.0 2964.0 3091.0 2468.0 1.00 In [20]: #help(pm.Normal) $\\hat{R}$ is a metric for comparing how well a chain has converged to the equilibrium distribution by comparing its behavior to other randomly initialized Markov chains. Multiple chains initialized from different initial conditions should give similar results. If all chains converge to the same equilibrium, $\\hat{R}$ will be 1. If the chains have not converged to a common distribution, $\\hat{R}$ will be > 1.01. $\\hat{R}$ is a necessary but not sufficient condition. For details on the $\\hat{R}$ see Gelman and Rubin (1992) . This linear regression example is from the original paper on PyMC3: Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55 Top 2. Hierarchical Models Gelman et al. 's famous radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in counties in several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil. Here we'll investigate this differences and try to make predictions of radonlevels in different county's based on the county itself and the presence of a basement. We'll look at Minnesota, a state that contains 85 counties in which different measurements are taken, ranging from 2 to 116 measurements. In [21]: df = pd . read_csv ( 'data/radon.csv' , index_col = [ 0 ]) df [ 'log_radon' ] = df [ 'log_radon' ] . astype ( 'float' ) county_names = df . county . unique () county_idx = df . county_code . values n_counties = len ( df . county . unique ()) df . head () Out[21]: idnum state state2 stfips zip region typebldg floor room basement windoor rep stratum wave starttm stoptm startdt stopdt activity pcterr adjwt dupflag zipflag cntyfips county fips Uppm county_code log_radon 0 5081.0 MN MN 27.0 55735 5.0 1.0 1.0 3.0 N 2 4.0 41 930.0 930.0 12088.0 12288.0 2.2 9.7 1146.499 1.0 0.0 1.0 AITKIN 27001.0 0.502 0 0.833 1 5082.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 Y 5 2.0 40 1615.0 1615.0 11888.0 12088.0 2.2 14.5 471.366 0.0 0.0 1.0 AITKIN 27001.0 0.502 0 0.833 2 5083.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 Y 3 2.0 42 1030.0 1515.0 20288.0 21188.0 2.9 9.6 433.317 0.0 0.0 1.0 AITKIN 27001.0 0.502 0 1.099 3 5084.0 MN MN 27.0 56469 5.0 1.0 0.0 4.0 Y 2 2.0 24 1410.0 1410.0 122987.0 123187.0 1.0 24.3 461.624 0.0 0.0 1.0 AITKIN 27001.0 0.502 0 0.095 4 5085.0 MN MN 27.0 55011 3.0 1.0 0.0 4.0 Y 3 2.0 40 600.0 600.0 12888.0 13088.0 3.1 13.8 433.317 0.0 0.0 3.0 ANOKA 27003.0 0.429 1 1.163 In [22]: df . shape Out[22]: (919, 29) Each row in the dataframe represents the radon measurements for one house in a specific county including whether the house has a basement (floor = 0) or not (floor = 1). We are interested in whether having a basement increases the radon measured in the house. To keep things simple let's keep only the following three variables: county , log_radon , and floor Let's check how many different counties we have. We also notice that they have a different number of houses. Some have a large number of houses measured, some only 1. In [23]: print ( f 'We have { n_counties } counties in { len ( df . state . value_counts ()) } state.' ) df [ 'county' ] . value_counts () . head ( 5 ) We have 85 counties in 1 state. Out[23]: ST LOUIS 116 HENNEPIN 105 DAKOTA 63 ANOKA 52 WASHINGTON 46 Name: county, dtype: int64 In [24]: # keep only these variables data = df [[ 'county' , 'log_radon' , 'floor' ]] data . head () Out[24]: county log_radon floor 0 AITKIN 0.833 1.0 1 AITKIN 0.833 0.0 2 AITKIN 1.099 0.0 3 AITKIN 0.095 0.0 4 ANOKA 1.163 0.0 In [25]: data [ 'county' ] . value_counts ()[ - 5 :] Out[25]: ROCK 2 STEVENS 2 MURRAY 1 WILKIN 1 MAHNOMEN 1 Name: county, dtype: int64 Pooling: Same Linear Regression for all We can just pool all the data and estimate one big regression to asses the influence of having a basement on radon levels across all counties. Our model would be: \\begin{equation} y_{i} = \\alpha + \\beta*floor_{i} \\end{equation} Where $i$ represents the measurement (house), and floor contains a 0 or 1 if the house has a basement or not. By ignoring the county feature, we do not differenciate on counties. In [26]: with pm . Model () as pooled_model : # common priors for all a = pm . Normal ( 'a' , mu = 0 , sigma = 100 ) b = pm . Normal ( 'b' , mu = 0 , sigma = 100 ) # radon estimate radon_est = a + b * data [ 'floor' ] . values # likelihood after radon observations radon_obs = pm . Normal ( 'radon_obs' , mu = radon_est , observed = data [ 'log_radon' ]) # note here we enter the whole dataset In [27]: pm . model_to_graphviz ( pooled_model ) Out[27]: %3 cluster919 919 a a ~ Normal radon_obs radon_obs ~ Normal a->radon_obs b b ~ Normal b->radon_obs In [28]: with pooled_model : pooled_trace = pm . sample ( 2000 , tune = 1000 , target_accept = 0.9 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a] Sampling 4 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12000/12000 [00:02<00:00, 4005.26draws/s] DONE In [29]: pm . traceplot ( pooled_trace ); Remember, with the pooled model we have only one intercept, $\\alpha$, and only one slope, $\\beta$ for all the counties. Let's plot the regression lines. In [30]: # plot just a subset of the countries, the five most counted and the 5 less counted counties_by_counts = data [ 'county' ] . value_counts () counties = counties_by_counts . index [: 5 ] . tolist () + counties_by_counts . index [ 75 : - 5 ] . tolist () counties Out[30]: ['ST LOUIS', 'HENNEPIN', 'DAKOTA', 'ANOKA', 'WASHINGTON', 'MILLE LACS', 'FILLMORE', 'COOK', 'LAC QUI PARLE', 'POPE'] In [31]: # plot just a subset of the countries #counties = ['HENNEPIN','AITKIN','WASHINGTON', 'MURRAY', 'YELLOW MEDICINE', 'MAHNOMEN'] plt . figure ( figsize = ( 15 , 5 )) rows = 2 gs = gridspec . GridSpec ( rows , len ( counties ) // rows ) for i , county in enumerate ( counties ): county_data = data . loc [ data [ 'county' ] == county ] x = np . linspace ( - 0.2 , 1.2 ) radon_est = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * x subplt = plt . subplot ( gs [ i ]) subplt . set_ylim ( 0. , 4. ) subplt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) subplt . plot ( x , radon_est , c = 'r' , label = 'pooled line' ); subplt . set_xlabel ( 'floor' , fontsize = 10 ) subplt . set_ylabel ( 'radon level' , fontsize = 10 ) subplt . set_title ( str ( county ) + ' County' ) subplt . legend () plt . tight_layout () Unpooling: Separate Linear Regression for each county We believe that different counties have different relationships of radon and basements. Our model would be: \\begin{equation} \\textbf{radon}_{i,c} = \\alpha_c + \\beta_c*\\textbf{floor}_{i,c} \\end{equation} Where $\\textbf{i}$ represents the measurement, $\\textbf{c}$ the county, and floor contains a 0 or 1 if the house has a basement or not. Notice we have separate coefficients for each county in $a_c$ and $b_c$. They are totally different, they could even come from different distributions. We will do this for only one county , since this is very time consuming, as an example. In [32]: # chose a county county = 'YELLOW MEDICINE' county_data = data . loc [ data [ 'county' ] == county ] county_data Out[32]: county log_radon floor 917 YELLOW MEDICINE 1.335 0.0 918 YELLOW MEDICINE 1.099 0.0 In [33]: #help(pm.Normal) In [34]: with pm . Model () as unpooled_model : mu_a = pm . Normal ( 'mu_a' , mu = 0. , sigma = 100 ) sigma_a = pm . HalfNormal ( 'sigma_a' , 5. ) mu_b = pm . Normal ( 'mu_b' , mu = 0. , sigma = 100 ) sigma_b = pm . HalfNormal ( 'sigma_b' , 5. ) a = pm . Normal ( 'a' , mu = mu_a , sigma = sigma_a ) b = pm . Normal ( 'b' , mu = mu_b , sigma = sigma_b ) radon_est = a + b * county_data [ 'floor' ] . values radon_obs = pm . Normal ( 'radon_like' , mu = radon_est , observed = county_data [ 'log_radon' ]) In [35]: pm . model_to_graphviz ( unpooled_model ) Out[35]: %3 cluster2 2 mu_b mu_b ~ Normal b b ~ Normal mu_b->b radon_like radon_like ~ Normal b->radon_like mu_a mu_a ~ Normal a a ~ Normal mu_a->a a->radon_like sigma_a sigma_a ~ HalfNormal sigma_a->a sigma_b sigma_b ~ HalfNormal sigma_b->b In [36]: with unpooled_model : unpooled_trace = pm . sample ( 5000 , tune = 1000 , target_accept = 0.99 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a, sigma_b, mu_b, sigma_a, mu_a] Sampling 4 chains, 238 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24000/24000 [10:05<00:00, 39.65draws/s] There were 53 divergences after tuning. Increase `target_accept` or reparameterize. There were 53 divergences after tuning. Increase `target_accept` or reparameterize. There were 61 divergences after tuning. Increase `target_accept` or reparameterize. The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. There were 71 divergences after tuning. Increase `target_accept` or reparameterize. The number of effective samples is smaller than 25% for some parameters. DONE In [37]: pm . traceplot ( unpooled_trace ); Print the regression line for our chosen county alone. In [38]: county_data = data . loc [ data [ 'county' ] == county ] print ( len ( county_data )) x = np . linspace ( - 0.2 , 1.2 ) radon_est_unpooled = unpooled_trace [ 'a' ] . mean () + unpooled_trace [ 'b' ] . mean () * x xx = np . linspace ( - 0.2 , 1. ) radon_est_pooled = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * xx plt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) plt . xlim ( - 0.1 , 1.1 ) plt . xlabel ( 'floor' , fontsize = 10 ) plt . ylabel ( 'radon level' , fontsize = 10 ) plt . title ( f ' { str ( county ) } county Radon levels' ) plt . plot ( x , radon_est_unpooled , c = 'g' , label = 'unpooled line' ); plt . plot ( xx , radon_est_pooled , c = 'r' , label = 'pooled line' ); plt . legend (); 2 Partial pooling: Hierarchical Regression (Varying-Coefficients Model) Counties, of course, have similarities, so there is a middle ground to both of these extremes. Specifically, we may assume that while $\\alpha_c$ and $\\beta_c$are different for each county as in the unpooled case, the coefficients are all drawn from the same distribution: \\begin{equation} radon_{i,c} = \\alpha_c + \\beta_c*floor_{i,c} \\end{equation} \\begin{equation} a_c \\sim \\mathcal{N}(\\mu_a,\\,\\sigma_a&#94;{2}) \\end{equation} \\begin{equation} b_c \\sim \\mathcal{N}(\\mu_b,\\,\\sigma_b&#94;{2}) \\end{equation} where the common parameters are: \\begin{eqnarray} \\mu_a \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma_a&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\\\ \\mu_b \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma_b&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} The different counties are effectively sharing information through the common priors. We are thus observing what is known as shrinkage; modeling the groups not as independent from each other, neither as a single group but rather as related. Discussion: how can we best handle this data? Does it make sense to make inferences without taking into account the county? TALKING POINTS We saw that some counties had only one sample, so if that house is a really old with old lead pipes, our prediction will be that all houses in this county have radon. On the other extreme, if we have a newer house with no radon then again we will have missleading results. In one case, you will overestimate the bad quality and in the other underestimate it. Under a hierarchical model, the miss-estimation of one group will be offset by the information provided by the other groups. As always, gathering more data helps, if this is an option. In [39]: with pm . Model () as hierarchical_model : # Hyperpriors for group nodes mu_a = pm . Normal ( 'mu_a' , mu = 0. , sigma = 100 ) sigma_a = pm . HalfNormal ( 'sigma_a' , 5. ) mu_b = pm . Normal ( 'mu_b' , mu = 0. , sigma = 100 ) sigma_b = pm . HalfNormal ( 'sigma_b' , 5. ) # Above we just set mu and sd to a fixed value while here we # plug in a common group distribution for all a and b (which are # vectors of length n_counties). # Intercept for each county, distributed around group mean mu_a a = pm . Normal ( 'a' , mu = mu_a , sigma = sigma_a , shape = n_counties ) # beta for each county, distributed around group mean mu_b b = pm . Normal ( 'b' , mu = mu_b , sigma = sigma_b , shape = n_counties ) # Model error #eps = pm.HalfCauchy('eps', 5.) radon_est = a [ county_idx ] + b [ county_idx ] * data [ 'floor' ] . values # Data likelihood with sigma for random error # radon_like = pm.Normal('radon_like', mu=radon_est, # sigma=eps, observed=data['log_radon']) # Data likelihood with sigma without random error radon_like = pm . Normal ( 'radon_like' , mu = radon_est , #sigma=eps, observed = data [ 'log_radon' ]) In [40]: # uncomment to create graph pm . model_to_graphviz ( hierarchical_model ) Out[40]: %3 cluster85 85 cluster919 919 sigma_a sigma_a ~ HalfNormal a a ~ Normal sigma_a->a mu_b mu_b ~ Normal b b ~ Normal mu_b->b mu_a mu_a ~ Normal mu_a->a sigma_b sigma_b ~ HalfNormal sigma_b->b radon_like radon_like ~ Normal a->radon_like b->radon_like Inference In [41]: %%time with hierarchical_model : hierarchical_trace = pm . sample ( 10000 , tune = 5000 , target_accept =. 9 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a, sigma_b, mu_b, sigma_a, mu_a] Sampling 4 chains, 6,158 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [02:31<00:00, 397.35draws/s] There were 1194 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.7248100688488703, but should be close to 0.9. Try to increase the number of tuning steps. There were 503 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.7737296457857813, but should be close to 0.9. Try to increase the number of tuning steps. There were 1450 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.7519143067834009, but should be close to 0.9. Try to increase the number of tuning steps. There were 3011 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.5847316188952049, but should be close to 0.9. Try to increase the number of tuning steps. The estimated number of effective samples is smaller than 200 for some parameters. CPU times: user 44.8 s, sys: 5.47 s, total: 50.3 s Wall time: 2min 47s In [42]: pm . traceplot ( hierarchical_trace , var_names = [ 'mu_a' , 'mu_b' , 'sigma_a' , 'sigma_b' ]); In [43]: results = pm . summary ( hierarchical_trace ) In [44]: results [: 20 ] # printing only 20 of them Out[44]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat mu_a 1.469 0.056 1.362 1.574 0.003 0.002 444.0 435.0 451.0 235.0 1.00 mu_b -0.642 0.095 -0.818 -0.457 0.003 0.002 866.0 857.0 829.0 2698.0 1.01 a[0] 1.354 0.231 0.920 1.780 0.006 0.005 1321.0 1264.0 1279.0 1527.0 1.00 a[1] 1.052 0.133 0.825 1.322 0.010 0.007 164.0 164.0 163.0 417.0 1.02 a[2] 1.474 0.239 1.027 1.927 0.009 0.007 676.0 676.0 675.0 839.0 1.01 a[3] 1.496 0.209 1.082 1.882 0.005 0.003 1851.0 1851.0 1822.0 2660.0 1.01 a[4] 1.440 0.223 1.023 1.870 0.005 0.004 1672.0 1596.0 1630.0 4032.0 1.00 a[5] 1.496 0.237 1.040 1.932 0.010 0.007 586.0 576.0 577.0 1197.0 1.01 a[6] 1.723 0.190 1.363 2.072 0.008 0.005 603.0 603.0 598.0 2845.0 1.01 a[7] 1.562 0.231 1.134 2.005 0.012 0.008 394.0 394.0 389.0 2584.0 1.01 a[8] 1.303 0.198 0.929 1.666 0.005 0.004 1632.0 1577.0 1636.0 2268.0 1.00 a[9] 1.481 0.211 1.086 1.890 0.004 0.003 3168.0 3037.0 2949.0 15587.0 1.00 a[10] 1.463 0.220 1.051 1.883 0.005 0.004 1900.0 1756.0 1922.0 3058.0 1.00 a[11] 1.515 0.230 1.072 1.930 0.007 0.005 1228.0 1228.0 1206.0 2406.0 1.00 a[12] 1.365 0.214 0.953 1.759 0.009 0.007 510.0 494.0 500.0 2764.0 1.01 a[13] 1.711 0.192 1.368 2.092 0.006 0.004 1017.0 1017.0 1006.0 2924.0 1.00 a[14] 1.446 0.230 1.009 1.874 0.007 0.005 1111.0 1015.0 1107.0 974.0 1.00 a[15] 1.386 0.243 0.934 1.848 0.008 0.006 886.0 815.0 873.0 800.0 1.00 a[16] 1.429 0.224 0.997 1.849 0.005 0.004 1975.0 1828.0 1939.0 2158.0 1.01 a[17] 1.323 0.192 0.957 1.665 0.005 0.004 1394.0 1394.0 1412.0 4408.0 1.00 In [45]: counties Out[45]: ['ST LOUIS', 'HENNEPIN', 'DAKOTA', 'ANOKA', 'WASHINGTON', 'MILLE LACS', 'FILLMORE', 'COOK', 'LAC QUI PARLE', 'POPE'] In [46]: # use counties from before or choose new ones #counties = ['HENNEPIN','AITKIN','WASHINGTON', 'LAKE OF THE WOODS', 'YELLOW MEDICINE', 'ANOKA'] #counties = ['HENNEPIN', 'DAKOTA', 'ANOKA', 'WILKIN', 'MURRAY'] plt . figure ( figsize = ( 15 , 5 )) rows = 2 gs = gridspec . GridSpec ( rows , len ( counties ) // rows ) for i , county in enumerate ( counties ): county_data = data . loc [ data [ 'county' ] == county ] if county_data . shape [ 0 ] == 1 : break ; subplt = plt . subplot ( gs [ i ]) # pooled line (single values coeff for all) xx = np . linspace ( - 0.2 , 1.2 ) radon_est = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * xx radon_est_hier = np . mean ( hierarchical_trace [ 'a' ][ i ]) + \\ np . mean ( hierarchical_trace [ 'b' ][ i ]) * xx # hierarchical line subplt . set_ylim ( 0. , 4. ) subplt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) subplt . plot ( xx , radon_est , c = 'r' , label = 'pooled' ); # plot the hierarchical, varying coefficient model subplt . plot ( xx , radon_est_hier , c = 'g' , label = 'hierarchical' ); subplt . set_xlabel ( 'floor' , fontsize = 10 ) subplt . set_ylabel ( 'radon level' , fontsize = 10 ) subplt . set_title ( str ( county ) + ' County' ) subplt . legend () plt . tight_layout () Adapted from PyMC3 docs: https://docs.pymc.io/notebooks/GLM-hierarchical.html Appendix A: Is this a fair coin? Can we find out if this a fair coin without using sampling? Let's say you visit the casino. You want to test your theory that casinos are dubious places where coins have been manipulated to have a larger probability for tails. So you will try to estimate how fair a coin is based on a certain amount of flips. You have no prior opinion on the coin's fairness (i.e. what $\\theta$ might be ), and begin flipping the coin. You get either Heads ($H$) or Tails ($T$) as our observed data and want to see if your posterior probabilities change as you obtain more data, that is, more coin flips. A nice way to visualize this is to plot the posterior probabilities as we observe more data. Your data is the pair ($n,k$) where $n$ is the number of flips and $k$ is the number of successes. We will be using Bayes rule. $\\textbf{D}$ is our data. Posterior $\\propto$ Prior $\\times$ Likelihood \\begin{equation} P(\\theta|\\textbf{D}) \\propto P(\\theta) \\times P(\\textbf{D} |\\theta) \\end{equation} a) Let's say we believe that our experiment is governed by a $\\text{Binomial}$ distribution (this will be our Likelihood). \\begin{equation} P(\\theta|\\textbf{D}) \\propto \\text{Beta(a,b)} \\times \\text{Binomial}(\\textbf{k},\\textbf{n}|\\theta) \\end{equation} b) We start with a non-informative prior, a $\\text{Beta}$ distribution with (a=b=1.) \\begin{equation} P(\\theta|1.,1.) = \\text{Beta(1., 1.)} \\end{equation} c) We enter the data as pairs of ($\\textbf{k},\\textbf{n}$) (we observe $\\textbf{k}$ heads in $\\textbf{n}$ tosses), and we update our Beta with new a,b as follows $&#94;*$: \\begin{equation} P(\\theta|\\textbf{k}) = Beta(\\alpha + \\textbf{k}, \\beta + (n - \\textbf{k})) \\end{equation} d) We are done! That is why using a congugate prior helps you find your parameters analytically. $&#94;*$ (the proof for this formula is beyond our scope, if interested, see this Wikipedia article ) In [47]: # change the data trials = np . array ([ 0 , 1 , 3 , 5 , 10 , 15 , 20 , 500 , 200 , 400 ]) heads = np . array ( [ 0 , 1 , 2 , 4 , 8 , 10 , 10 , 250 , 180 , 10 ]) # non-informative prior Beta(a,b=1), shows our ignorance about the coin # informative prior Beta(a,b=10.), shows our belief that this is a fair coin # informative prior Beta(a=20.,b=2.), shows our belief that this is NOT a fair coin alphas = [ 1. , 10. , 20. ] betas = [ 1. , 10. , 2. ] colors = [ '#348ABD' , '#000BCC' , '#999BCC' ] opacity = [ 0.4 , 0.2 , 0.1 ] plt . figure ( figsize = ( 10 , 12 )) x = np . linspace ( 0 , 1 , 100 ) for k , N in enumerate ( trials ): sublabel = f 'n trials = { trials [ k ] } , k = { heads [ k ] } ' sx = plt . subplot ( len ( trials ) / 2 , 2 , k + 1 ) for i in range ( len ( alphas )): posterior = stats . beta . pdf ( x , alphas [ i ] + heads [ k ], betas [ i ] + trials [ k ] - heads [ k ]) plt . plot ( x , posterior , alpha = 0.5 , c = colors [ i ], label = f 'a= { alphas [ i ] } ,b= { betas [ i ] } ' ); plt . fill_between ( x , 0 , posterior , color = colors [ i ], alpha = opacity [ i ]) plt . legend ( loc = 'upper left' , fontsize = 10 ) plt . title ( sublabel ) plt . legend () plt . autoscale ( tight = True ) plt . suptitle ( \"Posterior probabilities for coin flips\" , fontsize = 15 ); plt . tight_layout () plt . subplots_adjust ( top = 0.88 ) Appendix B: MAP Fitting the Model with MAP (FYI, we will not directly use this method) In Bayesian analysis we have our prior(s) , we define our likelihood , and, having specified our model , we try to calculate posterior estimates for the unknown variables in the model. We could try to calculate the posterior estimates analytically, but for most the models, this is not feasible. What we do then is compute summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods. \\begin{equation} P(\\theta|\\textbf{D}) \\rightarrow \\{\\theta_1,....\\theta_n\\} \\end{equation} Then we can find any estimate we want by using these samples, for example: \\begin{equation} \\mathbb{E}[f(\\theta] = \\int d\\theta{p(\\theta) f(\\theta)} \\end{equation} So we calculate the maximum a posteriori (MAP) point using optimization methods. \\begin{equation} f(\\hat{\\theta}), \\hat{\\theta} = argmax ({p(\\theta))} \\end{equation} The maximum a posteriori (MAP) estimate for a model, is the mode of the posterior distribution and is generally found using numerical optimization methods. This is often fast and easy to do, but only gives a point estimate for the parameters and can be biased if the mode isn't representative of the distribution. PyMC3 provides this functionality with the find_MAP function. MAP estimate is not always reasonable, especially if the mode is at an extreme or we have a multimodal distribution, or we have high dimensional posteriors. This will often occur in hierarchical models with the variance parameter for the random effect. If the individual group means are all the same, the posterior will have near infinite density if the variance parameter for the group means is almost zero. Most techniques for finding the MAP estimate only find a local optimium (which is often good enough), and can therefore fail badly for multimodal posteriors, as mentioned above. To solve these issues we turn to sampling as our method for finding the posterior. You do not have to worry about MAP in our problems. Our pyMC3 models use the MAP method to initialize the variables under the hood and we do not have to explicitly set this. References : Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 (https://doi.org/10.7717/peerj-cs.55) Distributions in PyMC3 More Details on Distributions Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the help command.",
        "tags": "lectures",
        "url": "lectures/lecture13/notebook/"
    }, {
        "title": "Feed-Forward Neural Networks vs Convolution Neural Networks",
        "text": "Title : Feed-Forward Neural Networks vs Convolution Neural Networks Description : The aim of this exercise is to train a feed-forward neural network and a convolutional neural network and compare the number of parameters between them on the following image classification task Instructions: Since we have only one 'Pavlos' and one 'Not Pavlos' image, we will need to augment our dataset. We use an image generator to create 'translated' versions of our two images. The training is performed on these translated images given in the data folder. Feed-Forward Neural Network: Build a simple Feed-Forward Neural Network and compile the model with binary cross entropy as the loss. Fit the model on the training data and save the history. Predict on the entire data. Visualize the loss and accuracy on train and validation data with respect to the epochs. Convolutional Neural Network: Build a Convolution Neural Networks and compile the model with binary cross-entropy as the loss. Fit the model on the training data and save the history. Predict on the entire data. Visualize the loss and accuracy on train and validation data with respect to the epochs. Compare the accuracy and the number of parameters of both the models. Hints: keras.Sequential() Creates a sequential model. A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. keras.compile() Configures the model for training. keras.fit() Trains the model for a fixed number of epochs. history.history[] The returned \"history\" object from model.fit() holds a dictionary of the loss values and metric values during training. keras.evaluate() Returns the loss value & metrics values for the model in test mode. tf.keras.preprocessing.image.ImageDataGenerator() Generate batches of tensor image data with real-time data augmentation. This function is used in our helper code. tf.keras.layers.Flatten() Flattens the input. Does not affect the batch size. tf.keras.layers.Conv2D() 2D convolution layer (e.g. spatial convolution over images). tf.keras.layers.Dense() A regular densely-connected NN layer. NOTE - The accuracy testing is done on the original network. Ensure to reset to the original parameters after answering the pause and think questions to pass the tests. Image Classification: FFNN vs CNN In [1]: # Importing necessary libraries import numpy as np import tensorflow as tf from numpy.random import seed seed ( 1 ) tf . random . set_seed ( 1 ) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Conv2D , MaxPool2D , Flatten , Input from matplotlib import pyplot as plt % matplotlib inline from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from keras.preprocessing.image import ImageDataGenerator from PIL import Image from numpy import asarray from helper import plot_history In [2]: # Initialise an image generator object generator = ImageDataGenerator ( rescale = 1. / 255 ) # Initialising number of data images num_data = len ( os . listdir ( 'data/pavlos' ) + os . listdir ( 'data/not_pavlos' )) # Read the image data from the directory using the generator object img_generator = generator . flow_from_directory ( directory = \"data/\" , color_mode = 'rgb' , seed = 1 , batch_size = 16 , target_size = ( 150 , 150 ), class_mode = 'binary' ) # Print the target size i.e. the total dataset size TARGET_SIZE = img_generator . target_size print ( f 'Generator produces images of size { TARGET_SIZE } (with 3 color channels)' ) # Print the batch size BATCH_SIZE = img_generator . batch_size print ( f 'Images are generated in batches of size { BATCH_SIZE } ' ) Found 100 images belonging to 2 classes. Generator produces images of size (150, 150) (with 3 color channels) Images are generated in batches of size 16 In [3]: # Plotting a sample of the generated images sample_batch = img_generator . next ()[ 0 ] fig , ax = plt . subplots ( 4 , 4 ) ax = ax . ravel () for i , img in enumerate ( sample_batch ): ax [ i ] . set_axis_off () ax [ i ] . imshow ( img ) plt . suptitle ( 'Sample Batch of Generated Images' , y = 1.05 ) plt . tight_layout () Feed-Forward Neural Network Our first network will be a feed-forward neural network. The only layers with learned parameters we will be using are dense layers. In [28]: # Fixing the random seed seed ( 1 ) tf . random . set_seed ( 1 ) # Creating a feed-forward Neural Network FFNN = Sequential () # Specify a layer that takes the input with input shape # the same as the size of the image defined during image generation # Remember to take into account that the image has 3 channels FFNN . add ( tf . keras . layers . Input ( shape = ( 150 , 150 , 3 ))) # Add a flatten layer to enable FFNN to process images FFNN . add ( tf . keras . layers . Flatten ()) # Specify a list of the number of nodes for each dense layer ffnn_filters = [ 6 , 4 , 2 ] # Add dense layers for the number of nodes in ffnn_filters with ReLU activation for n_nodes in ffnn_filters : FFNN . add ( tf . keras . layers . Dense ( n_nodes , activation = 'relu' )) # Add the final dense layer with 1 output node to differentiate # between the two classes and sigmoid activation FFNN . add ( tf . keras . layers . Dense ( 1 , activation = 'sigmoid' )) # Compile the model with bce as the loss, accuracy as the metric and adam optimizer FFNN . compile ( loss = 'binary_crossentropy' , metrics = [ 'accuracy' ], optimizer = 'adam' ) In [29]: # Print a summary of the model and observe the total number of parameters FFNN . summary () Model: \"sequential_5\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_3 (Flatten) (None, 67500) 0 _________________________________________________________________ dense_10 (Dense) (None, 6) 405006 _________________________________________________________________ dense_11 (Dense) (None, 4) 28 _________________________________________________________________ dense_12 (Dense) (None, 2) 10 _________________________________________________________________ dense_13 (Dense) (None, 1) 3 ================================================================= Total params: 405,047 Trainable params: 405,047 Non-trainable params: 0 _________________________________________________________________ In [30]: # Train the model FFNN_history = FFNN . fit ( img_generator , steps_per_epoch = num_data // BATCH_SIZE , epochs = 10 , shuffle = False , workers = 0 , validation_data = img_generator , validation_steps = num_data * 0.25 // BATCH_SIZE ) Epoch 1/10 6/6 [==============================] - 1s 46ms/step - loss: 1.4907 - accuracy: 0.5323 - val_loss: 0.6934 - val_accuracy: 0.4375 Epoch 2/10 6/6 [==============================] - 0s 19ms/step - loss: 0.6931 - accuracy: 0.5182 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 3/10 6/6 [==============================] - 0s 19ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 4/10 6/6 [==============================] - 0s 20ms/step - loss: 0.6931 - accuracy: 0.5132 - val_loss: 0.6921 - val_accuracy: 0.6250 Epoch 5/10 6/6 [==============================] - 0s 19ms/step - loss: 0.6933 - accuracy: 0.4854 - val_loss: 0.6937 - val_accuracy: 0.4375 Epoch 6/10 6/6 [==============================] - 0s 19ms/step - loss: 0.6928 - accuracy: 0.5424 - val_loss: 0.6921 - val_accuracy: 0.6250 Epoch 7/10 6/6 [==============================] - 0s 19ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 8/10 6/6 [==============================] - 0s 20ms/step - loss: 0.6929 - accuracy: 0.5344 - val_loss: 0.6914 - val_accuracy: 0.6875 Epoch 9/10 6/6 [==============================] - 0s 26ms/step - loss: 0.6936 - accuracy: 0.4563 - val_loss: 0.6937 - val_accuracy: 0.4375 Epoch 10/10 6/6 [==============================] - 0s 21ms/step - loss: 0.6933 - accuracy: 0.4893 - val_loss: 0.6937 - val_accuracy: 0.4375 ‚è∏ Enter the number of parameters in the given FFNN architecture. In [31]: ### edTest(test_chow1) ### # Enter the answer by typing in a number in the space provided answer1 = '405,047' In [32]: # Use the plot history function from the helper file to plot the data plot_history ( FFNN_history , 'Feed-Forward Neural Network' ) In [33]: ### edTest(test_ffnn_acc) ### # Evaluate your model FFNN_loss , FFNN_acc = FFNN . evaluate ( img_generator , steps = 2 ) print ( f 'FFNN Accuracy: { FFNN_acc } ' ) 2/2 [==============================] - 0s 12ms/step - loss: 0.6913 - accuracy: 0.7188 FFNN Accuracy: 0.71875 ‚è∏ Alter the network architecture by increasing the number of nodes and/or layers. Enter the number of parameters of the network that gives a validation accuracy of above 80%. In [42]: ### edTest(test_chow2) ### # Enter the answer by typing in a number in the space provided answer2 = '405,047' Convolutional Neural Network In [43]: ### edTest(test_cnn_count_param) ### # Fixing the random seed seed ( 1 ) tf . random . set_seed ( 1 ) # Creating a CNN CNN = Sequential () # Add a layer to take the input with shape (150,150,3) CNN . add ( Input ( shape = ( 150 , 150 , 3 ))) # Specify a list of the number of filters for each convolutional layer cnn_filters = [ 8 , 8 , 8 , 8 , 8 ] # Add convolutional layers with number of filters in cnn_filters # with kernel size as 3, stride of 2 and relu activation for n_filters in cnn_filters : CNN . add ( Conv2D ( n_filters , strides = ( 2 , 2 ), kernel_size = 3 , activation = 'relu' )) # Add the flatten layer between the CNN and dense layer CNN . add ( Flatten ()) # Add a dense layer with 64 nodes and relu activation CNN . add ( Dense ( 64 , activation = 'relu' )) # Specify the output layer with sigmoid activation and one node CNN . add ( Dense ( 1 , activation = 'sigmoid' )) # Compile the model with bce as the loss, accuracy as the metric and adam optimizer CNN . compile ( loss = 'binary_crossentropy' , metrics = [ 'accuracy' ], optimizer = 'adam' ) In [44]: # Print a summary of the model and observe the total number of parameters CNN . summary () Model: \"sequential_7\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_15 (Conv2D) (None, 74, 74, 8) 224 _________________________________________________________________ conv2d_16 (Conv2D) (None, 36, 36, 8) 584 _________________________________________________________________ conv2d_17 (Conv2D) (None, 17, 17, 8) 584 _________________________________________________________________ conv2d_18 (Conv2D) (None, 8, 8, 8) 584 _________________________________________________________________ conv2d_19 (Conv2D) (None, 3, 3, 8) 584 _________________________________________________________________ flatten_5 (Flatten) (None, 72) 0 _________________________________________________________________ dense_16 (Dense) (None, 64) 4672 _________________________________________________________________ dense_17 (Dense) (None, 1) 65 ================================================================= Total params: 7,297 Trainable params: 7,297 Non-trainable params: 0 _________________________________________________________________ ‚è∏ Enter the number of parameters in the given CNN architecture. In [45]: ### edTest(test_chow3) ### # Enter the answer by typing in a number in the space provided answer3 = '7,297' In [46]: # Fit the model on the image generator CNN_history = CNN . fit ( img_generator , steps_per_epoch = num_data // BATCH_SIZE , epochs = 10 , shuffle = False , workers = 0 , validation_data = img_generator , validation_steps = num_data * 0.25 // BATCH_SIZE ) Epoch 1/10 6/6 [==============================] - 1s 50ms/step - loss: 0.6937 - accuracy: 0.4350 - val_loss: 0.6906 - val_accuracy: 0.6250 Epoch 2/10 6/6 [==============================] - 0s 23ms/step - loss: 0.6899 - accuracy: 0.6763 - val_loss: 0.6850 - val_accuracy: 0.7500 Epoch 3/10 6/6 [==============================] - 0s 33ms/step - loss: 0.6834 - accuracy: 0.8830 - val_loss: 0.6754 - val_accuracy: 1.0000 Epoch 4/10 6/6 [==============================] - 0s 23ms/step - loss: 0.6721 - accuracy: 0.9557 - val_loss: 0.6581 - val_accuracy: 0.9375 Epoch 5/10 6/6 [==============================] - 0s 23ms/step - loss: 0.6460 - accuracy: 0.9808 - val_loss: 0.6095 - val_accuracy: 1.0000 Epoch 6/10 6/6 [==============================] - 0s 91ms/step - loss: 0.5915 - accuracy: 0.9826 - val_loss: 0.5142 - val_accuracy: 1.0000 Epoch 7/10 6/6 [==============================] - 0s 27ms/step - loss: 0.4830 - accuracy: 0.9737 - val_loss: 0.3584 - val_accuracy: 1.0000 Epoch 8/10 6/6 [==============================] - 0s 24ms/step - loss: 0.3068 - accuracy: 0.9900 - val_loss: 0.1658 - val_accuracy: 1.0000 Epoch 9/10 6/6 [==============================] - 0s 23ms/step - loss: 0.1193 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000 Epoch 10/10 6/6 [==============================] - 0s 23ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000 In [47]: # Plot the history of the model plot_history ( CNN_history , 'Convolutional Neural Network' ) In [48]: ### edTest(test_cnn_acc) ### # Evaluate the model on the entire data CNN_loss , CNN_acc = CNN . evaluate ( img_generator , steps = 2 ) print ( f 'CNN Test Accuracy: { CNN_acc } ' ) 2/2 [==============================] - 0s 15ms/step - loss: 0.0153 - accuracy: 1.0000 CNN Test Accuracy: 1.0 ‚è∏ Remove the last convolution layer in the Convolution Neural Network defined above. How does this affect the number of parameters? A. The number of parameters decrease. B. The number of parameters increase. C. The number of parameters remains the same. In [49]: ### edTest(test_chow4) ### # Enter the answer by typing in a number in the space provided answer4 = 'A' In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture14/notebook/"
    }, {
        "title": "Lecture 12: Bayesian statistics (part 4)",
        "text": "Exercises Lecture 12: Bayes - Exercise 1 (Notebook) Lecture 12: Bayes - Exercise 2 (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture12/"
    }, {
        "title": "Lecture 12 - Bayesian statistics (part 4)",
        "text": "Title : Bayes - Exercise 1 Description : Model y as normal distribution with unknown mean and std dev, ignoring x After completing this exercise you should see following trace plots: Hints: pymc3 Normal Refer to lecture notebook. Do not change any other code except the blanks. In [1]: import pandas as pd import numpy as np import pymc3 as pm import warnings warnings . filterwarnings ( 'ignore' ) from matplotlib import pyplot % matplotlib inline In [2]: df = pd . read_csv ( 'data3.csv' ) df . head () Out[2]: x y 0 2.875775 0.223825 1 7.883051 6.118438 2 4.089769 2.167730 3 8.830174 5.868317 4 9.404673 7.333365 In [0]: ### edTest(test_pm_model) ### np . random . seed ( 109 ) with pm . Model () as model : #Set priors for unknown model parameters alpha = pm . Normal ( 'alpha' , mu = 0 , tau = 1000 ) # Likelihood (sampling distribution) of observations tau_obs = pm . Gamma ( 'tau' , alpha = 0.001 , beta = 0.001 ) obs = pm . Normal ( ____________ ) #Parameters to set: name, mu, tau, observed # create trace plots trace = pm . sample ( 2000 , tune = 2000 ) pm . traceplot ( trace , compact = False ); In [0]: #posterior means np . mean ( trace [ 'alpha' ]) , np . mean ( trace [ 'tau' ]) In [0]: In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture12/notebook1/"
    }, {
        "title": "Lecture 12 - Bayesian statistics (part 4)",
        "text": "Title : Bayes - Exercise 2 Description : Model $y$ as a least-squares regression as $y = \\alpha + \\beta \\cdot x + \\epsilon$ After completing this exercise you should see following trace plots: Hints: pymc3 Normal Refer to lecture notebook. Do not change any other code except the blanks. In [0]: ! pip install xarray == 0 .16.0 In [1]: import pandas as pd import numpy as np import pymc3 as pm from matplotlib import pyplot % matplotlib inline In [2]: df = pd . read_csv ( 'data3.csv' ) In [0]: ### edTest(test_pm_model) ### np . random . seed ( 109 ) with pm . Model () as model : # prior alpha = pm . Normal ( 'alpha' , mu = 0 , tau = 1000 ) beta = pm . Normal ( 'beta' , mu = 0 , tau = 1000 ) # likelihood # Next statement creates the expected value of mu_vec of the # outcomes, specifying the linear relationship. # mu_vec is just the sum of the intercept alpha and the product of # the coefficient beta and the predictor variable. mu_vec = pm . Deterministic ( 'mu_vec' , ____ ) tau_obs = pm . Gamma ( 'tau_obs' , 0.001 , 0.001 ) obs = pm . Normal ( _______ ) #Parameters to set: name, mu, tau, observed trace = pm . sample ( 2000 , tune = 2000 , chains = 2 ) pm . traceplot ( trace , var_names = [ 'alpha' , 'beta' , 'tau_obs' ], compact = False ); In [0]: #posterior means np . mean ( trace [ 'alpha' ]), np . mean ( trace [ 'beta' ]), np . mean ( trace [ 'tau_obs' ]) In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture12/notebook2/"
    }, {
        "title": "Lecture 11: Bayesian statistics (part 3)",
        "text": "",
        "tags": "lectures",
        "url": "lectures/lecture11/"
    }, {
        "title": "Lecture 10: Bayes PyMC3",
        "text": "Exercises Lecture 10 : Bayes PyMC3 (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture10/"
    }, {
        "title": "Lecture 10 - Bayes. PyMC3",
        "text": "Title : Bayes PyMC3 Description : Run this notebook in your own environment. Do not forget to download the images/ and data/ folders. CS109B Data Science 2: Advanced Topics in Data Science Lecture 10 - Bayesian Analysis and Introduction to pyMC3 Harvard University Spring 2021 Instructors: Pavlos Protopapas, Mark Glickman, and Chris Tanner Additional Instructor and content: Eleni Angelaki Kaxiras In [0]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) In [0]: import pymc3 as pm # from pymc3 import summary import arviz as az from matplotlib import gridspec # Ignore a common pymc3 warning that comes from library functions, not our code. # Pymc3 may throw additional warnings, but other warnings should be manageable # by following the instructions included within the warning messages. import warnings messages = [ \"Using `from_pymc3` without the model will be deprecated in a future release\" , ] # or silence all warnings (not recommended) # warnings.filterwarnings('ignore') for m in messages : warnings . filterwarnings ( \"ignore\" , message = m ) print ( f \"Using PyMC3 version: { pm . __version__ } \" ) print ( f \"Using ArviZ version: { az . __version__ } \" ) In [0]: import pymc3 as pm In [0]: from scipy import optimize In [0]: import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline In [0]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; In [0]: # pandas trick pd . options . display . max_columns = 50 # or =None -> No Restrictions pd . options . display . max_rows = 200 # or =None -> *Be careful with this* pd . options . display . max_colwidth = 100 pd . options . display . precision = 3 Learning Objectives By the end of this lab, you should be able to: identify and describe some of the most important probability distributions. apply Bayes Rule in calculating probabilities (and would have had fun in the process). create probabilistic models in the PyMC3 library. Table of Contents Some common families of distributions (review) The Bayesian Way of Thinking Bayesian Regression with pyMC3 and Defining a model in pyMC3 Bayesian Logistic Regression with pyMC3 Appendix 1. Some common families of distributions Statistical distributions are characterized by one of more parameters, such as $\\mu$ and $\\sigma&#94;2$ for a Gaussian distribution. \\begin{equation} Y \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} Discrete Distributions The probability mass function (pmf) of a discrete random variable $Y$ is equal to the probability that our random variable will take a specific value $y$: $f_y=P(Y=y)$ for all $y$. The variable, most of the times, assumes integer values. Plots for pmf s are better understood as stem plots since we have discrete values for the probabilities and not a curve. Probabilities should add up to 1 for proper distributions. Bernoulli for a binary outcome, success has probability $\\theta$, and we only have $one$ trial: \\begin{equation} P(Y=k) = \\theta&#94;k(1-\\theta)&#94;{1-k} \\end{equation} Binomial for a binary outcome, success has probability $\\theta$, $k$ successes in $n$ trials: Our random variable is $Y$= number of successes in $n$ trials. \\begin{equation} P(Y=k|n,\\theta) = {{n}\\choose{k}} \\cdot \\theta&#94;k(1-\\theta)&#94;{n-k} \\quad k=0,1,2,...,n \\end{equation} As a reminder ${{n}\\choose{k}}$ is \"from $n$ choose $k$\": \\begin{equation} {{n}\\choose{k}} = \\frac{n!}{k!(n-k)!} \\end{equation} $EY=n\\theta$, $VarY = np(1-p)$ Note : Binomial (1,$p$) = Bernouli ($p$) Exercise : Run the code below (that plots the Binomial distribution using stats.binom.pmf ) for various values of the probability for a success $\\theta\\in [0,1]$. Look at the ordinate values to verify that they add up to 1. In [0]: # probabity of success theta = 0.5 n = 5 k = np . arange ( 0 , n + 1 ) print ( k ) pmf = stats . binom . pmf ( k , n , theta ) plt . style . use ( 'seaborn-darkgrid' ) plt . stem ( k , pmf , label = r 'n = {} , $\\theta$ = {} ' . format ( n , theta )) plt . xlabel ( 'Y' , fontsize = 14 ) plt . ylabel ( 'P(Y)' , fontsize = 14 ) plt . legend () plt . show () Negative Binomial for a binary outcome, success has probability $\\theta$, we have $r$ successes in $x$ trials: Our random variable is $X$= number of trials to get to $r$ successes. \\begin{equation} P(X=x|r,\\theta) = {{x-1}\\choose{r-1}} \\cdot \\theta&#94;r(1-\\theta)&#94;{x-r} \\quad x=r,r+1,... \\end{equation} Poisson (counts independent events and has a single parameter $\\lambda$) \\begin{equation} P\\left( Y=y|\\lambda \\right) = \\frac{{e&#94;{ - \\lambda } \\lambda &#94;y }}{{y!}} \\quad y = 0,1,2,... \\end{equation} Exercise : Change the value of $\\lambda$ in the Poisson pmf plot below and see how the plot changes. Remember that the y-axis in a discrete probability distribution shows the probability of the random variable having a specific value in the x-axis. We use stats.poisson.pmf(x, $\\lambda$) , where $\\lambda$ is the parameter. In [0]: plt . style . use ( 'seaborn-darkgrid' ) x = np . arange ( 0 , 10 ) lam = 4 pmf = stats . poisson . pmf ( x , lam ) plt . stem ( x , pmf , label = '$\\lambda$ = {} ' . format ( lam )) plt . xlabel ( 'Y' , fontsize = 12 ) plt . ylabel ( 'P(Y)' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . ylim = ( - 0.1 ) plt . show () Discrete Uniform for a random variable $X\\in(1,N)$: \\begin{equation} P(X=x|N) = \\frac{1}{N}, \\quad x=1,2,...,N \\end{equation} In [0]: # boring but useful as a prior plt . style . use ( 'seaborn-darkgrid' ) N = 40 x = np . arange ( 0 , N ) pmf = stats . randint . pmf ( x , 0 , N ) plt . plot ( x , pmf , label = '$N$ = {} ' . format ( N )) fontsize = 14 plt . xlabel ( 'X' , fontsize = fontsize ) plt . ylabel ( f 'P(X| { N } )' , fontsize = fontsize ) plt . legend () plt . show () Categorical, or Multinulli (random variables can take any of K possible categories, each having its own probability; this is a generalization of the Bernoulli distribution for a discrete variable with more than two possible outcomes, such as the roll of a die) \\begin{equation} f(x_1,x_2,...,x_n) = \\frac{m}{x_1!\\cdot x_2!\\dotsb x_n!} \\cdot p_1&#94;{x_1}\\cdot p_2&#94;{x_2}\\dotsb p_n&#94;{x_n} \\end{equation} Continuous Distributions The random variable has a probability density function (pdf) whose area under the curve equals to 1. Uniform (variable equally likely to be near each value in interval $(a,b)$) \\begin{equation} f(x|a,b) = \\frac{1}{b - a} \\quad x\\in [a,b] \\quad \\text{and 0 elsewhere}. \\end{equation} Exercise : Change the value of $\\mu$ in the Uniform PDF and see how the plot changes. Remember that the y-axis in a continuous probability distribution does not shows the actual probability of the random variable having a specific value in the x-axis because that probability is zero!. Instead, to see the probability that the variable is within a small margin we look at the integral below the curve of the PDF. The uniform is often used as a noninformative prior. Uniform - numpy.random.uniform(a=0.0, b=1.0, size) $\\alpha$ and $\\beta$ are our parameters. size is how many tries to perform. The mean is $\\mu = \\frac{(a+b)}{2}$ In [0]: from scipy.stats import uniform a = 0 b = 1 r = uniform . rvs ( loc = a , scale = b - a , size = 1000 ) pdf = uniform . pdf ( r , loc = a , scale = b - a ) plt . plot ( r , pdf , 'b-' , lw = 3 , alpha = 0.6 , label = 'uniform pdf' ) plt . hist ( r , density = True , histtype = 'stepfilled' , alpha = 0.2 ) plt . ylabel ( r 'pdf' ) plt . xlabel ( f 'x' ) plt . legend ( loc = 'best' , frameon = False ) plt . show () Normal (a.k.a. Gaussian) \\begin{equation} X \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma&#94;{2}$. The link between the two is given by \\begin{equation} \\tau = \\frac{1}{\\sigma&#94;{2}} \\end{equation} Expected value (mean) $\\mu$ Variance $\\frac{1}{\\tau}$ or $\\sigma&#94;{2}$ Parameters: mu: float , sigma: float or tau: float Range of values (-$\\infty$, $\\infty$) In [0]: plt . style . use ( 'seaborn-darkgrid' ) x = np . linspace ( - 5 , 5 , 1000 ) mus = [ 0. , 0. , 0. , - 2. ] sigmas = [ 0.4 , 1. , 2. , 0.4 ] for mu , sigma in zip ( mus , sigmas ): pdf = stats . norm . pdf ( x , mu , sigma ) plt . plot ( x , pdf , label = r '$\\mu$ = ' + f ' { mu } ,' + r '$\\sigma$ = ' + f ' { sigma } ' ) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability density' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . show () Beta (where the variable ($\\theta$) takes on values in the interval $(0,1)$, and is parametrized by two positive parameters, $\\alpha$ and $\\beta$ that control the shape of the distribution. Note that Beta is a good distribution to use for priors (beliefs) because its range is $[0,1]$ which is the natural range for a probability and because we can model a wide range of functions by changing the $\\alpha$ and $\\beta$ parameters. Its density (pdf) is: \\begin{equation} \\label{eq:beta} P(\\theta|a,b) = \\frac{1}{B(\\alpha, \\beta)} {\\theta}&#94;{\\alpha - 1} (1 - \\theta)&#94;{\\beta - 1} \\propto {\\theta}&#94;{\\alpha - 1} (1 - \\theta)&#94;{\\beta - 1} \\end{equation} where the normalisation constant, $B$, is a beta function of $\\alpha$ and $\\beta$, \\begin{equation} B(\\alpha, \\beta) = \\int_{x=0}&#94;1 x&#94;{\\alpha - 1} (1 - x)&#94;{\\beta - 1} dx. \\end{equation} 'Nice', unimodal distribution Range of values $[0, 1]$ $EX = \\frac{a}{a+b}$, $VarX = \\frac{ab}{(a+b)&#94;2(a+b+1)}$ Exercise : Try out various combinations of $a$ and $b$. We get an amazing set of shapes by tweaking the two parameters $a$ and $b$. Notice that for $a=b=1.$ we get the uniform distribution. As the values increase, we get a curve that looks more and more Gaussian. In [0]: from scipy.stats import beta fontsize = 15 alphas = [ 0.5 ] #, 0.5, 1., 3., 6.] betas = [ 0.5 ] #, 1., 1., 3., 6.] x = np . linspace ( 0 , 1 , 1000 ) colors = [ 'red' , 'green' , 'blue' , 'black' , 'pink' ] fig , ax = plt . subplots ( figsize = ( 8 , 5 )) for a , b , colors in zip ( alphas , betas , colors ): plt . plot ( x , beta . pdf ( x , a , b ), c = colors , label = f 'a= { a } , b= { b } ' ) ax . set_ylim ( 0 , 3 ) ax . set_xlabel ( r '$\\theta$' , fontsize = fontsize ) ax . set_ylabel ( r 'P ($\\theta|\\alpha,\\beta)$' , fontsize = fontsize ) ax . set_title ( 'Beta Distribution' , fontsize = fontsize * 1.2 ) ax . legend ( loc = 'best' ) fig . show (); At home : Prove the formula mentioned in class which gives the probability density for a Beta distribution with parameters $2$ and $5$: $p(\\theta|2,5) = 30 \\cdot \\theta(1 - \\theta)&#94;4$ Code Resources: Statistical Distributions in numpy/scipy: scipy.stats Top 2. The Bayesian way of Thinking Here is my state of knowledge about the situation. Here is some data, I am now going to revise my state of knowledge. Bayes Rule \\begin{equation} \\label{eq:bayes} P(A|\\textbf{B}) = \\frac{P(\\textbf{B} |A) P(A) }{P(\\textbf{B})} \\end{equation} $P(A|\\textbf{B})$ is the posterior distribution, probability(parameters| data). $P(\\textbf{B} |A)$ is the likelihood function, how probable is my data for different values of the parameters. $P(A)$ is the marginal probability to observe the data, called the prior , this captures our belief about the data before observing it. $P(\\textbf{B})$ is the marginal distribution (sometimes called marginal likelihood). This serves a normalization purpose. Let's Make a Deal The problem we are about to solve gained fame as part of a game show \"Let's Make A Deal\" hosted by Monty Hall, hence its name. It was first raised by Steve Selvin in American Statistician in 1975. The game is as follows: there are 3 doors behind one of which are the keys to a new car. There is a goat behind each of the other two doors. Let's assume your goal is to get the car and not a goat. You are asked to pick one door, and let's say you pick Door1 . The host knows where the keys are. Of the two remaining closed doors, he will always open the door that has a goat behind it. He'll say \"I will do you a favor and open Door2 \". So he opens Door2 inside which there is, of course, a goat. He now asks you, do you want to open the initial Door1 you chose or change to Door3 ? Generally, in this game, when you are presented with this choice should you swap the doors? Hint: Start by defining the events of this probabilities game. One definition is: $A_i$: car is behind door $i$ $B_i$ host opens door $i$ $i\\in[1,2,3]$ In more math terms, the question is: is the probability of the price is behind Door 1 higher than the probability of the price is behind Door2 , given that an event has occured ? Breakout Game : Solve the Monty Hall Paradox using Bayes Rule. Bayes rule revisited We have data that we believe come from an underlying distribution of unknown parameters. If we find those parameters, we know everything about the process that generated this data and we can make inferences (create new data). \\begin{equation} P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \\end{equation} But what is $\\theta \\;$? $\\theta$ is an unknown yet fixed set of parameters. In Bayesian inference we express our belief about what $\\theta$ might be and instead of trying to guess $\\theta$ exactly, we look for its probability distribution . What that means is that we are looking for the parameters of that distribution. For example, for a Poisson distribution our $\\theta$ is only $\\lambda$. In a normal distribution, our $\\theta$ is often just $\\mu$ and $\\sigma$. Top 3. Bayesian Regression with pyMC3 PyMC3 is a Python library for programming Bayesian analysis, and more specifically, data creation, model definition, model fitting, and posterior analysis. It uses the concept of a model which contains assigned parametric statistical distributions to unknown quantities in the model. Within models we define random variables and their distributions. A distribution requires at least a name argument, and other parameters that define it. You may also use the logp() method in the model to build the model log-likelihood function. We define and fit the model. PyMC3 includes a comprehensive set of pre-defined statistical distributions that can be used as model building blocks. They are not meant to be used outside of a model , and you can invoke them by using the prefix pm , as in pm.Normal . For more see: PyMC3 Quickstart Distributions in PyMC3 : Statistical Distributions in pyMC3 . Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the help command. In [0]: #help(pm.Beta) Defining a Model in PyMC3 Our problem is the following: we want to perform multiple linear regression to predict an outcome variable $Y$ which depends on variables $\\bf{x}_1$ and $\\bf{x}_2$. We will model $Y$ as normally distributed observations with an expected value $mu$ that is a linear function of the two predictor variables, $\\bf{x}_1$ and $\\bf{x}_2$. \\begin{equation} Y \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} \\begin{equation} \\mu = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 \\end{equation} where $\\sigma&#94;2$ represents the measurement error (in this example, we will use $\\sigma&#94;2 = 10$) We also choose the parameters to have normal distributions with those parameters set by us. \\begin{eqnarray} \\beta_i \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} We will artificially create the data to predict on. We will then see if our model predicts them correctly. Let's create some synthetic data In [0]: np . random . seed ( 123 ) ################################### ## Hidden true parameter values sigma = 1 beta0 = 1 beta = [ 1 , 2.5 ] #################################### # Size of dataset size = 100 # Predictor variable x1 = np . linspace ( 0 , 1. , size ) x2 = np . linspace ( 0 , 2. , size ) # Simulate outcome variable Y = beta0 + beta [ 0 ] * x1 + beta [ 1 ] * x2 + np . random . randn ( size ) * sigma In [0]: Y . shape , x1 . shape , x2 . shape In [0]: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () fontsize = 14 labelsize = 8 title = 'Observed Data (created artificially by ' + r '$Y(x_1,x_2)$)' ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , Y ) ax . set_xlabel ( r '$x_1$' , fontsize = fontsize ) ax . set_ylabel ( r '$x_2$' , fontsize = fontsize ) ax . set_zlabel ( r '$Y$' , fontsize = fontsize ) ax . tick_params ( labelsize = labelsize ) fig . suptitle ( title , fontsize = fontsize ) fig . tight_layout ( pad =. 1 , w_pad = 10.1 , h_pad = 2. ) fig . subplots_adjust (); #top=0.5 plt . tight_layout plt . show () Building the model Step1: Formulate the probability model for our data: $Y \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2})$ Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=Y) Step2: Choose a prior distribution for our unknown parameters. beta0 = Normal('beta0', mu=0, sd=10) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] betas = Normal('betas', mu=0, sd=10, shape=2) sigma = HalfNormal('sigma', sd=1) Step3: Construct the likelihood function. Step4: Determine the posterior distribution, this is our main goal . Step5: Summarize important features of the posterior and/or plot the parameters. In [0]: with pm . Model () as my_linear_model : # Priors for unknown model parameters, specifically created stochastic random variables # with Normal prior distributions for the regression coefficients, # and a half-normal distribution for the standard deviation of the observations. # These are our parameters. # I need to give the prior some initial values for the parameters mu0 = 0 sd0 = 10 beta0 = pm . Normal ( 'beta0' , mu = mu0 , sd = sd0 ) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] mub = 0 sdb = 10 betas = pm . Normal ( 'betas' , mu = mub , sd = sdb , shape = 2 ) sds = 1 sigma = pm . HalfNormal ( 'sigma' , sd = sds ) # mu is what is called a deterministic random variable, which implies that # its value is completely determined by its parents' values # (betas and sigma in our case). There is no uncertainty in the # variable beyond that which is inherent in the parents' values mu = beta0 + betas [ 0 ] * x1 + betas [ 1 ] * x2 # Likelihood function = how probable is my observed data? # This is an observed variable; it is identical to a standard # stochastic variable, except that its observed argument, # which passes the data to the variable, indicates that the values for this # variable were observed, and should not be changed by any # fitting algorithm applied to the model. # The data can be passed in the form of a numpy.ndarray or pandas.DataFrame object. Y_obs = pm . Normal ( 'Y_obs' , mu = mu , sd = sigma , observed = Y ) In [0]: my_linear_model . basic_RVs In [0]: my_linear_model . free_RVs Note : If our problem was a classification for which we would use Logistic regression. In [0]: ## do not worry if this does not work, it's just a nice graph to have ## you need to install python-graphviz first # conda install -c conda-forge python-graphviz pm . model_to_graphviz ( my_linear_model ) Now all we need to do is sample our model. ... to be continued Appendix A: Bayesian Logistic Regression with pyMC3 If the problem above was a classification that required a Logistic Regression, we would use the logistic function ( where $\\beta_0$ is the intercept, and $\\beta_i$ (i=1, 2, 3) determines the shape of the logistic function). \\begin{equation} Pr(Y=1|X_1,X_2,X3) = {\\frac{1}{1 + exp&#94;{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3)}}} \\end{equation} Since both $\\beta_0$ and the $\\beta_i$s can be any possitive or negative number, we can model them as gaussian random variables. \\begin{eqnarray} \\beta_0 \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;2) \\\\ \\beta_i \\sim \\mathcal{N}(\\mu_i,\\,\\sigma_i&#94;2) \\end{eqnarray} In PyMC3 we can model those as: pm.Normal('beta_0', mu=0, sigma=100) (where $\\mu$ and $\\sigma&#94;2$ can have some initial values that we assign them, e.g. 0 and 100) The dererministic variable would be: logitp = beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3 To connect this variable (logit-p) with our observed data, we would use a Bernoulli as our likelihood. our_likelihood = pm.Bernoulli('our_likelihood', logit_p=logitp, observed=our_data) Notice that the main difference with Linear Regression is the use of a Bernoulli distribution instead of a Gaussian distribution, and the use of the logistic function instead of the identity function. Breakout Room Exercise : Write the model above in code. Suppose that your training dataframe (df_train) has the following features: **numerical** - df_train['age'] - df_train['weight'] **categorical** - df_train['hypertension'] In [0]: # A reminder of what the logistic function looks like. # Change parameters a and b to see the shape of the curve change b = 5. x = np . linspace ( - 8 , 8 , 100 ) plt . plot ( x , 1 / ( 1 + np . exp ( - b * x ))) plt . xlabel ( 'y' ) plt . ylabel ( 'y=logistic(x)' ) Appendix B: Is this a fair coin? Is this a fair coin? Let's say you visit the casino in Monte Carlo . You want to test your theory that casinos are dubious places where coins have been manipulated to have a larger probability for tails. So you will try to estimate how fair a coin is based on a certain amount of flips. You have no prior opinion on the coin's fairness (i.e. what $p$ might be), and begin flipping the coin. You get either Heads ($H$) or Tails ($T$) as our observed data and want to see if your posterior probabilities change as you obtain more data, that is, more coin flips. A nice way to visualize this is to plot the posterior probabilities as we observe more flips (data). We will be using Bayes rule. $\\textbf{D}$ is our data. \\begin{equation} P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \\end{equation} We start with a non-informative prior, a Beta distribution with (a=b=1.) \\begin{equation} P(\\theta|\\textbf{k=0}) = Beta(1., 1.) \\end{equation} Then, as we get new data (say, we observe $k$ heads in $n$ tosses), we update our Beta with new a,b as follows: \\begin{equation} P(\\theta|\\textbf{k}) = Beta(\\alpha + \\textbf{k}, \\beta + (n - \\textbf{k})) \\end{equation} (the proof is beyond our scope, if interested, see this Wikipedia article ) we can say that $\\alpha$ and $\\beta$ play the roles of a \"prior number of heads\" and \"prior number of tails\". In [0]: # play with the priors - here we manually set them but we could be sampling from a separate Beta trials = np . array ([ 0 , 1 , 3 , 5 , 10 , 15 , 20 , 100 , 200 , 300 ]) heads = np . array ([ 0 , 1 , 2 , 4 , 8 , 10 , 10 , 50 , 180 , 150 ]) x = np . linspace ( 0 , 1 , 100 ) # for simplicity we set a,b=1 plt . figure ( figsize = ( 10 , 8 )) for k , N in enumerate ( trials ): sx = plt . subplot ( len ( trials ) / 2 , 2 , k + 1 ) posterior = stats . beta . pdf ( x , 1 + heads [ k ], 1 + trials [ k ] - heads [ k ]) plt . plot ( x , posterior , alpha = 0.5 , label = f ' { trials [ k ] } tosses \\n { heads [ k ] } heads' ); plt . fill_between ( x , 0 , posterior , color = \"#348ABD\" , alpha = 0.4 ) plt . legend ( loc = 'upper left' , fontsize = 10 ) plt . legend () plt . autoscale ( tight = True ) plt . suptitle ( \"Posterior probabilities for coin flips\" , fontsize = 15 ); plt . tight_layout () plt . subplots_adjust ( top = 0.88 ) References Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 (https://doi.org/10.7717/peerj-cs.55) Distributions in PyMC3 More Details on Distributions This linear regression example is from the original paper on PyMC3: Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55 Cool Reading How Bayesian Analysis and Lawrence D. Stone found the Wreckage of Air France Flight AF 447 . Search for the gold on the sunken SS Central America .",
        "tags": "Lectures",
        "url": "lectures/lecture10/notebook/"
    }, {
        "title": "Lecture 9: Bayesian statistics (part 2)",
        "text": "",
        "tags": "lectures",
        "url": "lectures/lecture09/"
    }, {
        "title": "Lecture 8: Clustering in Python (Lab)",
        "text": "Exercises Lecture 8: Pre-Class Notebook (Notebook) Lecture 8: Review of statsmodels, numpy, and linear algebra (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture08/"
    }, {
        "title": "Lecture 8 - Clustering in Python (Lab)",
        "text": "Title Pre-Class Notebook Description : This notebook is from CS109A. You can run it on Ed. If you choose to download, make sure you download the Heart-2.csv file as well. This is material you should be familiar with. It is provided here as a review. In [1]: % matplotlib inline import sys import numpy as np import pylab as pl import pandas as pd import sklearn as sk import statsmodels.api as sm import matplotlib.pyplot as plt import matplotlib import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.decomposition import PCA import sklearn.metrics as met pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) PCA Part 0: Reading the data In this notebook, we will be using the same Heart dataset from last lecture. As a reminder the variables we will be using today include: AHD : whether or not the patient presents atherosclerotic heart disease (a heart attack): Yes or No Sex : a binary indicator for whether the patient is male (Sex=1) or female (Sex=0) Age : age of patient, in years MaxHR : the maximum heart rate of patient based on exercise testing RestBP : the resting systolic blood pressure of the patient Chol : the HDL cholesterol level of the patient Oldpeak : ST depression induced by exercise relative to rest (on an ECG) Slope : the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) Ca : number of major vessels (0-3) colored by flourosopy For further information on the dataset, please see the UC Irvine Machine Learning Repository . In [2]: df_heart = pd . read_csv ( 'Heart-2.csv' ) # Force the response into a binary indicator: df_heart [ 'AHD' ] = 1 * ( df_heart [ 'AHD' ] == \"Yes\" ) print ( df_heart . shape ) df_heart . head () (303, 15) Out[2]: Unnamed: 0 Age Sex ChestPain RestBP Chol Fbs RestECG MaxHR ExAng Oldpeak Slope Ca Thal AHD 0 1 63 1 typical 145 233 1 2 150 0 2.3 3 0.0 fixed 0 1 2 67 1 asymptomatic 160 286 0 2 108 1 1.5 2 3.0 normal 1 2 3 67 1 asymptomatic 120 229 0 2 129 1 2.6 2 2.0 reversable 1 3 4 37 1 nonanginal 130 250 0 0 187 0 3.5 3 0.0 normal 0 4 5 41 0 nontypical 130 204 0 2 172 0 1.4 1 0.0 normal 0 Here are some basic summaries and EDA we've seen before: Part 1: Principal Components Analysis (PCA) Q1.1 Just a sidebar (and a curiosity), what happens when two of the identical predictor is used in logistic regression? Is an error created? Should one be? Investigate by predicting AHD from two copies of Age , and compare to the simple logistic regression model with Age alone. In [3]: y = df_heart [ 'AHD' ] logit1 = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" ) . fit ( df_heart [[ 'Age' ]], y ) # investigating what happens when two identical predictors of 'Age' are used logit2 = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" ) . fit ( df_heart [[ 'Age' , 'Age' ]], y ) print ( \"The coef estimate for Age (when in the model once):\" , logit1 . coef_ ) print ( \"The coef estimates for Age (when in the model twice):\" , logit2 . coef_ ) The coef estimate for Age (when in the model once): [[0.05198618]] The coef estimates for Age (when in the model twice): [[0.02599311 0.02599311]] your answer here The single coefficient for Age is distributed equally across the two predictors. This is a very reasonable approach as predictions will still be stable. We will apply PCA to the heart dataset when there are just 4 predictors considered (remember: PCA is used when dimensionality is high (lots of predictors), but this will help us get our heads around what is going on): In [4]: # For pedagogical purposes, let's simplify our lives and use just 7 predictors X = df_heart [[ 'Age' , 'RestBP' , 'Chol' , 'MaxHR' , 'Sex' , 'Oldpeak' , 'Slope' ]] y = df_heart [ 'AHD' ] X . describe () Out[4]: Age RestBP Chol MaxHR Sex Oldpeak Slope count 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 mean 54.438944 131.689769 246.693069 149.607261 0.679868 1.039604 1.600660 std 9.038662 17.599748 51.776918 22.875003 0.467299 1.161075 0.616226 min 29.000000 94.000000 126.000000 71.000000 0.000000 0.000000 1.000000 25% 48.000000 120.000000 211.000000 133.500000 0.000000 0.000000 1.000000 50% 56.000000 130.000000 241.000000 153.000000 1.000000 0.800000 2.000000 75% 61.000000 140.000000 275.000000 166.000000 1.000000 1.600000 2.000000 max 77.000000 200.000000 564.000000 202.000000 1.000000 6.200000 3.000000 In [5]: # Here is the table of correlations between our predictors. This will be useful later on X . corr () Out[5]: Age RestBP Chol MaxHR Sex Oldpeak Slope Age 1.000000 0.284946 0.208950 -0.393806 -0.097542 0.203805 0.161770 RestBP 0.284946 1.000000 0.130120 -0.045351 -0.064456 0.189171 0.117382 Chol 0.208950 0.130120 1.000000 -0.003432 -0.199915 0.046564 -0.004062 MaxHR -0.393806 -0.045351 -0.003432 1.000000 -0.048663 -0.343085 -0.385601 Sex -0.097542 -0.064456 -0.199915 -0.048663 1.000000 0.102173 0.037533 Oldpeak 0.203805 0.189171 0.046564 -0.343085 0.102173 1.000000 0.577537 Slope 0.161770 0.117382 -0.004062 -0.385601 0.037533 0.577537 1.000000 Q1.2 Is there any evidence of multicollinearity in the set of predictors? How do you know? How will PCA handle these correlations? Solution: Yes, there is evidence of collinearity as the estimated $\\beta$ coefficient for Age changes greatly from the simple regression model ($\\hat{\\beta} = 0.0520$) to this multiple regression model ($\\hat{\\beta} = 0.00525$): a 10-fold decrease. Next we apply the PCA transformation in a few steps, and show some of the results below: In [6]: # create/fit the 'full' pca transformation pca = PCA () . fit ( X ) # apply the pca transformation to the full predictor set pcaX = pca . transform ( X ) # convert to a data frame pcaX_df = pd . DataFrame ( pcaX , columns = [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' , 'PCA7' ]]) # here are the weighting (eigen-vectors) of the variables (first 2 at least) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 ,:]) print ( \"Second PCA Component (w2):\" , pca . components_ [ 1 ,:]) # here is the variance explained: print ( \"Variance explained by each component:\" , pca . explained_variance_ratio_ ) First PCA Component (w1): [ 3.84007875e-02 5.04636205e-02 9.97978046e-01 -3.74374033e-03 -1.80930152e-03 1.15452228e-03 -3.60044587e-06] Second PCA Component (w2): [ 1.80599125e-01 1.05022594e-01 -1.59461519e-02 -9.77583674e-01 8.45378533e-04 1.79192401e-02 1.04056605e-02] Variance explained by each component: [7.47915963e-01 1.50222918e-01 8.52586952e-02 1.61372448e-02 3.47699826e-04 6.25312302e-05 5.49481983e-05] Q1.3 Now try the PCA decompositon on the standardized version of X instead. In [7]: ### edTest(test_pcaZ) ### # create/fit the standardized version of X Z = sk . preprocessing . StandardScaler () . fit ( X ) . transform ( X ) # create/fit the 'full' pca transformation on Z pca_standard = PCA () . fit ( Z ) pcaZ = pca_standard . transform ( Z ) # convert to a data frame pcaZ_df = pd . DataFrame ( pcaZ , columns = [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' , 'PCA7' ]]) In [8]: # Let's look at them to see what they are comprised of: pd . DataFrame . from_dict ({ 'Variable' : X . columns , 'PCA1' : pca . components_ [ 0 ], 'PCA2' : pca . components_ [ 1 ], 'PCA-Z1' : pca_standard . components_ [ 0 ], 'PCA-Z2' : pca_standard . components_ [ 1 ]}) Out[8]: Variable PCA1 PCA2 PCA-Z1 PCA-Z2 0 Age 0.038401 0.180599 0.411736 0.353496 1 RestBP 0.050464 0.105023 0.269612 0.344915 2 Chol 0.997978 -0.015946 0.123722 0.570322 3 MaxHR -0.003744 -0.977584 -0.473658 0.121837 4 Sex -0.001809 0.000845 0.010681 -0.546336 5 Oldpeak 0.001155 0.017919 0.515666 -0.221566 6 Slope -0.000004 0.010406 0.502094 -0.261511 Q2.3 Interpret the results above. What doss $w_1$ represent based on the untransformed data? What doss $w_1$ represent based on the standardized data? Which is a better representation of the data? your answer here $w_1$ represents the transformation (change in basis) to convert the columns of $\\mathbf{X}$ to the first PCA vector, $z_1$. They elements after quaring sum up to 1, so the magnitude represents euclidean weighting in the transformation (the larger value means more weight in the transformation). In [9]: np . sum ( pca . components_ [ 0 ,:] ** 2 ) Out[9]: 1.0000000000000007 It is common for a model with high dimensional data (lots of predictors) to be plotted along the first 2 PCA components (with the classification boundaries added). Below is the scatter plot for these data (without a classificaiton boundary, since we do not have a model yet): In [10]: # Plot the response over the first 2 PCA component vectors fig ,( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax1 . scatter ( pcaX_df [[ 'PCA1' ]][ y == 0 ], pcaX_df [[ 'PCA2' ]][ y == 0 ]) ax1 . scatter ( pcaX_df [[ 'PCA1' ]][ y == 1 ], pcaX_df [[ 'PCA2' ]][ y == 1 ]) ax1 . legend ([ \"AHD = No\" , \"AHD = Yes\" ]) ax1 . set_xlabel ( \"First PCA Component Vector\" ) ax1 . set_ylabel ( \"Second PCA Component Vector\" ); ax2 . scatter ( pcaZ_df [[ 'PCA1' ]][ y == 0 ], pcaZ_df [[ 'PCA2' ]][ y == 0 ]) ax2 . scatter ( pcaZ_df [[ 'PCA1' ]][ y == 1 ], pcaZ_df [[ 'PCA2' ]][ y == 1 ]) ax2 . legend ([ \"AHD = No\" , \"AHD = Yes\" ]) ax2 . set_xlabel ( \"First PCA Component Vector (on standardized data)\" ) ax2 . set_ylabel ( \"Second PCA Component Vector (on standardized data)\" ); Out[10]: Text(0, 0.5, 'Second PCA Component Vector (on standardized data)') Q2.4 Does there appear to be good potential here? Which form of the data appear to provide better information as to seperate the classes in the response? What at would a classification boundary look like if a logistic regression model were fit using the first 2 principal components as the predictors? your answer here It would again be linear. Here, most likely the boundary would be a line with negative slope. Part 2: PCA in Regression (PCR) First let's fit the full logistic regression model to predict AHD from the 7 predictors above. Remember: PCA is an approach to handling the predictors (unsupervised), so it does not matter if we are using it for a regression or classification type problem. In [11]: #fit the 'full' model on the 7 predictors. and print out the coefficients logit_full = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" , max_iter = 2000 ) . fit ( X , y ) beta = logit_full . coef_ [ 0 ] print ( beta ) [ 0.01194387 0.01540617 0.00698371 -0.03597127 1.74038737 0.55627766 0.30007484] Below is the result of the PCR-1 (logistic) to predict AHD from the first principal component vector. In [12]: logit_pcr1 = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' ]], y ) print ( \"Intercept from simple PCR-Logistic:\" , logit_pcr1 . intercept_ ) print ( \"'Slope' from simple PCR-Logistic:\" , logit_pcr1 . coef_ ) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 : 1 ,:]) Intercept from simple PCR-Logistic: [-0.16620988] 'Slope' from simple PCR-Logistic: [[0.00351119]] First PCA Component (w1): [[ 3.84007875e-02 5.04636205e-02 9.97978046e-01 -3.74374033e-03 -1.80930152e-03 1.15452228e-03 -3.60044587e-06]] Q2.1 What does this PCR-1 model tell us about how the predictors relate to the response (aka, estimate the coefficient(s) in the original predictor space)? Is it truly a simple logistic regression model in the original predictor space? In [13]: # your code here: do a multiplication of pcr_1's coefficients times the first component vector from PCA ( logit_pcr1 . coef_ * pca . components_ [ 0 : 1 ,:]) Out[13]: array([[ 1.34832630e-04, 1.77187582e-04, 3.50409493e-03, -1.31450001e-05, -6.35280937e-06, 4.05375218e-06, -1.26418654e-08]]) your answer here The estimated slope from PCR1 ( $\\hat{\\beta}= 0.00351092$) is distributed across the 4 actual predictors, so that the formula would be: $$\\hat{y} = 0.00351(Z_1) = 0.00351(w&#94;T_1\\mathbf{X}) = 0.00351(0.0384X_1+0.0505X_2+0.998X_3-0.0037X_4) \\\\ = 0.000135X_1+0.000177X_2+0.00350X_3-0.0000131X_4) $$ This is how to interpret the estimated coefficients from a regression with PCA components as the predictors: some transformation back to the original space is required. Here is the above claculation fora few up tothe 7th PCR logistic regression, and then plotted on a 'pretty' plot: In [14]: # Fit the other 3 PCRs on the rest of the 7 predictors #pcaX_df.iloc[:,np.arange(0,5)].head() logit_pcr2 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' ]], y ) logit_pcr3 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' ]], y ) logit_pcr4 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' ]], y ) logit_pcr5 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' ]], y ) logit_pcr6 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' ]], y ) logit_pcr7 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df , y ) pcr1 = ( logit_pcr1 . coef_ * np . transpose ( pca . components_ [ 0 : 1 ,:])) . sum ( axis = 1 ) pcr2 = ( logit_pcr2 . coef_ * np . transpose ( pca . components_ [ 0 : 2 ,:])) . sum ( axis = 1 ) pcr3 = ( logit_pcr3 . coef_ * np . transpose ( pca . components_ [ 0 : 3 ,:])) . sum ( axis = 1 ) pcr4 = ( logit_pcr4 . coef_ * np . transpose ( pca . components_ [ 0 : 4 ,:])) . sum ( axis = 1 ) pcr5 = ( logit_pcr5 . coef_ * np . transpose ( pca . components_ [ 0 : 5 ,:])) . sum ( axis = 1 ) pcr6 = ( logit_pcr6 . coef_ * np . transpose ( pca . components_ [ 0 : 6 ,:])) . sum ( axis = 1 ) pcr7 = ( logit_pcr7 . coef_ * np . transpose ( pca . components_ [ 0 : 7 ,:])) . sum ( axis = 1 ) results = np . vstack (( pcr1 , pcr2 , pcr3 , pcr4 , pcr5 , pcr6 , pcr7 , beta )) print ( results ) [[ 1.34832630e-04 1.77187582e-04 3.50409493e-03 -1.31450001e-05 -6.35280937e-06 4.05375218e-06 -1.26418654e-08] [ 8.22888139e-03 4.90460029e-03 3.52035681e-03 -4.36772643e-02 3.00832874e-05 8.05215560e-04 4.64728134e-04] [ 9.67851637e-03 1.57232886e-02 2.98524575e-03 -4.25103732e-02 1.66262028e-05 9.08384148e-04 4.91486409e-04] [ 4.87937328e-03 1.65220860e-02 3.12832994e-03 -4.33112238e-02 3.92241495e-05 9.15507194e-04 5.08938426e-04] [ 1.97160112e-03 1.21592916e-02 3.37846293e-03 -3.28220970e-02 3.64476979e-02 6.57141425e-01 2.16558303e-01] [ 1.76384914e-03 1.28415055e-02 3.91486367e-03 -3.62050051e-02 3.73981251e-01 8.31601131e-01 -3.35831180e-01] [ 1.19527616e-02 1.54021528e-02 6.98403516e-03 -3.59641629e-02 1.74063117e+00 5.56305309e-01 3.00152829e-01] [ 1.19438739e-02 1.54061660e-02 6.98371275e-03 -3.59712705e-02 1.74038737e+00 5.56277658e-01 3.00074835e-01]] In [15]: plt . plot ([ 'PCR1' , 'PCR2' , 'PCR3' , 'PCR4' , 'PCR5' , 'PCR6' , 'PCR7' , 'Logistic' ], results ) plt . ylabel ( \"Back-calculated Beta Coefficients\" ); #plt.legend(X.columns); Out[15]: Text(0, 0.5, 'Back-calculated Beta Coefficients') Q2.5 Interpret the plot above. Specifically, compare how each PCA vector \"contributes\" to the original logistic regression model using all 7 original predictors. How Does PCR-4 compare to the original logistic regression model (in estimated coefficients)? your answer here This plot shows that as more PCA vectors are included in the PCA-Regression, the estimated $\\beta$s from the original regression model are recovered: if PCR($p$) is used (where $p$ is the number of predictors we started with), they are mathemtaically equivalent. All of this PCA work should have been done using the standardized versions of the predictors. Below is the code that does exactly that: In [16]: scaler = sk . preprocessing . StandardScaler () scaler . fit ( X ) Z = scaler . transform ( X ) pca = PCA ( n_components = 7 ) . fit ( Z ) pcaZ = pca . transform ( Z ) pcaZ_df = pd . DataFrame ( pcaZ , columns = [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' , 'PCA7' ]]) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 ,:]) print ( \"Second PCA Component (w2):\" , pca . components_ [ 1 ,:]) First PCA Component (w1): [ 0.41173599 0.26961154 0.12372188 -0.47365763 0.01068056 0.51566635 0.50209418] Second PCA Component (w2): [ 0.35349616 0.34491485 0.57032206 0.121837 -0.54633621 -0.22156592 -0.26151097] In [17]: #fit the 'full' model on the 4 predictors. and print out the coefficients logit_full = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( Z , y ) betaZ = logit_full . coef_ [ 0 ] print ( \"Logistic coef. on standardized predictors:\" , betaZ ) Logistic coef. on standardized predictors: [ 0.1079209 0.27060575 0.36100403 -0.82125596 0.81211581 0.64488006 0.1846543 ] In [18]: # Fit the PCR logit_pcr1Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' ]], y ) logit_pcr2Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' , 'PCA2' ]], y ) logit_pcr3Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' , 'PCA2' , 'PCA3' ]], y ) logit_pcr4Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' ]], y ) logit_pcr7Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df , y ) pcr1Z = ( logit_pcr1Z . coef_ * np . transpose ( pca . components_ [ 0 : 1 ,:])) . sum ( axis = 1 ) pcr2Z = ( logit_pcr2Z . coef_ * np . transpose ( pca . components_ [ 0 : 2 ,:])) . sum ( axis = 1 ) pcr3Z = ( logit_pcr3Z . coef_ * np . transpose ( pca . components_ [ 0 : 3 ,:])) . sum ( axis = 1 ) pcr4Z = ( logit_pcr4Z . coef_ * np . transpose ( pca . components_ [ 0 : 4 ,:])) . sum ( axis = 1 ) pcr7Z = ( logit_pcr7Z . coef_ * np . transpose ( pca . components_ )) . sum ( axis = 1 ) resultsZ = np . vstack (( pcr1Z , pcr2Z , pcr3Z , pcr4Z , pcr7Z , betaZ )) print ( resultsZ ) plt . plot ([ 'PCR1-Z' , 'PCR2-Z' , 'PCR3-Z' , 'PCR4-Z' , 'PCR7-Z' , 'Logistic' ], resultsZ ) plt . ylabel ( \"Back-calculated Beta Coefficients\" ); plt . legend ( X . columns ); [[ 0.36018456 0.23585481 0.10823127 -0.4143533 0.0093433 0.45110231 0.43922945] [ 0.23486422 0.11213646 -0.09984974 -0.46425637 0.21008141 0.53808503 0.54073257] [ 0.25062392 0.32032679 -0.14162369 -0.38896297 0.34364814 0.56142451 0.51311087] [ 0.36663298 0.29038468 -0.18487686 -0.48912818 0.40597179 0.4822163 0.43312993] [ 0.1079209 0.27060575 0.36100403 -0.82125596 0.81211581 0.64488006 0.1846543 ] [ 0.1079209 0.27060575 0.36100403 -0.82125596 0.81211581 0.64488006 0.1846543 ]] Out[18]: Q2.6 Compare this plot to the previous one; why does this plot make sense?. What does this illustrate? Solution: This plot shows that the components are now more evenly composed of the predictors, rather than the first component being dominated by the predictor with the most variability. The 4 lines move more similarly here than in th eprevious plot where they essentially moved one predictor for one component. In [19]:",
        "tags": "Lectures",
        "url": "lectures/lecture08/notebook1/"
    }, {
        "title": "Lecture 8 - Clustering in Python (Lab)",
        "text": "Title : Clustering with Python Description : Download and run on your own environment. CS109B Data Science 2: Advanced Topics in Data Science Lecture 8 - Clustering with Python Harvard University Spring 2021 Instructors: Pavlos Protopapas, Mark Glickman, and Chris Tanner Lab Instructor: Eleni Kaxiras Content: Eleni Kaxiras, Chris Tanner, and Will Claybaugh In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn import preprocessing from sklearn.datasets import make_classification from sklearn.datasets import make_blobs from sklearn.datasets import make_gaussian_quantiles % matplotlib inline Learning Objectives Review what PCA is and know the differences between PCA and clustering, Understand the common distance metrics (e.g., Euclidean, Manhattan, Hamming). Understand how different clustering algorithms work (e.g., k-means, Hierarchical, DBSCAN). Quantitatively describe the quality clusters' fit, according to different metrics. Table of Contents PCA Refresher Distance Metrics Clustering Algorithms and Measuring Quality of Clusters Unsupervised Learning and Classification Review : What is unsupervised learning? What does it mean to perform classification? 1. PCA Refresher image source: [1] Review What is PCA? How can it be useful? What are its limitations? image source: [1] Sklearn's sklearn.decomposition.PCA uses the LAPACK library written in Fortran 90 (based on the LINPACK library from the 70s) which provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations. How to use the sklearn PCA package: a. Instantiate a new PCA object: pca_transformer = PCA() b. Fit some data (learns the transformation based on this data): fitted_pca = pca_transformer.fit(data_frame) c. Transform the data to the reduced dimensions: pca_df = fitted_pca.transform(data_frame) Using two distinct steps (i.e., (b) and (c)) to fit and transform our data allows one the flexibility to transform any dataset according to our learned fit() . Alternatively, if you know you only want to transform a single dataset, you can combine (b) and (c) into one step: Fit and transform: pca_df = pca_transformer.fit_transform(pca_df) Note: We fit on the training set and transform both training and test set. Example: Playing with synthetic data Sklearn has some wonderful methods for generating synthetic datasets . They can be quite useful for testing clustering for classification purposes. In [3]: plt . title ( \"my synthetic data\" , fontsize = 'medium' ) X1 , Y1 = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 1 , n_clusters_per_class = 1 , n_classes = 1 ) plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); class_df = pd . DataFrame ( X1 , Y1 ) plt . axis ( 'equal' ); Out[3]: (-2.438222172750026, 0.8376852266041566, -3.986103041621982, 2.773276271189026) In [4]: pca_transformer = PCA ( n_components = 2 ) fitted_pca = pca_transformer . fit ( class_df ) pca_df = pd . DataFrame ( fitted_pca . transform ( class_df )) pca_df . head () Out[4]: 0 1 0 1.432439 -0.312532 1 -0.490130 0.301419 2 0.051971 0.252416 3 0.795097 0.464159 4 -1.195042 -0.285882 In [5]: fitted_pca . explained_variance_ratio_ Out[5]: array([0.77081362, 0.22918638]) In [6]: print ( fitted_pca . components_ ), fitted_pca . mean_ [[ 0.10057958 -0.99492902] [ 0.99492902 0.10057958]] Out[6]: (None, array([-0.95177291, -0.18406218])) Discussion: We didn't scale our data before applying PCA. Should we usually do so? Why or why not? For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 2 - Distance Metrics The Euclidean norm (or length) of a vector $\\textbf{v}=[v_1,v_2,..,v_n]&#94;T$ in $\\mathbb{R}&#94;n$ is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\sqrt{\\textbf{v}\\cdot \\textbf{v}} = \\sqrt{{v_1}&#94;2+{v_2}&#94;2+\\cdots+{v_n}&#94;2} \\end{aligned} The Manhattan norm of the same vector is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\lvert \\textbf{v} \\rvert = \\lvert v_1 \\rvert + \\lvert v_2 \\rvert + \\cdots + \\lvert v_n \\rvert \\end{aligned} The distance between two vectors $\\textbf{v}$ and $\\textbf{u}$ is defined by $d(\\textbf{v}, \\textbf{u}) = \\lVert \\textbf{v} - \\textbf{u} \\rVert$ Let's practice on the diagram below; we are concerned with measuring the distance between two points, $\\textbf{p}=(p_1,p_2)$ and $\\textbf{q}=(q_1,q_2)$. (edited from Wikipedia.org) Euclidean distance: The Euclidean distance measures the shortest path between the two points, navigating through all dimensions: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2}$ For vectors in $\\mathbb{R}&#94;n$: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2+\\cdots +{(p_n-q_n)}&#94;2}$ Manhattan distance: The Manhattan distance measures the cumulative difference between the two points, across all dimensions. $d_1(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert_1 = \\sum_{i=1}&#94;{n} \\mid p_i-q_1 \\mid$ Cosine distance (extra) $\\cos{\\theta} = \\frac{\\textbf{q}\\textbf{q}}{\\lVert \\textbf{p}\\rVert \\lVert\\textbf{q} \\rVert} $ In [7]: from sklearn.feature_extraction.text import CountVectorizer from scipy.spatial import distance count_vect = CountVectorizer () sent0 = \"Biden is here\" sent1 = \"President is coming here\" corpus = [ sent0 , sent1 ] sentences = count_vect . fit_transform ( corpus ) v1 = sentences . toarray ()[ 0 ] v2 = sentences . toarray ()[ 1 ] print ( f 'v1 = { v1 } , \\n v2 = { v2 } ' ) # pretty print df = pd . DataFrame ( sentences . toarray (), \\ columns = count_vect . get_feature_names (), index = [ 'Sentence 0' , 'Sentence 1' ]) print ( f 'distance = { distance . cosine ( v1 , v2 ) } ' ) df v1 = [1 0 1 1 0], v2 = [0 1 1 1 1] distance = 0.42264973081037416 Out[7]: biden coming here is president Sentence 0 1 0 1 1 0 Sentence 1 0 1 1 1 1 Note : Normally cosine value=0 means that the two vectors are orthogonal to each other. scipy implements cosine as 1-cosine , so cosine=0 means no connection and cosine=1 means orthogonal. Cosine metric is used in Collaborative Filtering (Recommender systems for movies). More on cosine distance on the second part of the class. Hamming Distance (extra): If our two elements of comparison can be represented a sequence of discrete items, it can be useful to measure how many of their elements differ. For example: Mahmoud and Mahmood differ by just 1 character and thus have a hamming distance of 1. 10101 and 01101 have a hamming distance of 2. Mary and Barry have a hamming distance of 3 (m->b, y->r, null->y). Note : the last example may seem sub-optimal, as we could transform Mary to Barry by just 2 operations (substituting the M with a B, then adding an 'r'). So, their so-called edit distance is smaller than their Hamming distance. The very related Levenshtein distance here can handle this, and thus tends to be more appropriate for Strings. 3 - Clustering Algorithms Question: Why do we care about clustering? How/why is it useful? We will now walk through three clustering algorithms, first discussing them at a high-level, then showing how to implement them with Python libraries. Let's first load and scale our data, so that particular dimensions don't naturally dominate in their contributions in the distant calculations: In [8]: multishapes = pd . read_csv ( \"data/multishapes.csv\" ) # loads x,y columns of a dataset ms_df = multishapes [[ 'x' , 'y' ]] ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Blue' , \\ title = \"Multishapes data\" , \\ figsize = ( 5.5 , 4.2 )) plt . show () ms_df . head () Out[8]: x y 0 -0.803739 -0.853053 1 0.852851 0.367618 2 0.927180 -0.274902 3 -0.752626 -0.511565 4 0.706846 0.810679 In [9]: # displays our summary statistics of our data ms_df . describe () Out[9]: x y count 1100.000000 1100.000000 mean -0.081222 -0.625431 std 0.644967 1.176170 min -1.489180 -3.353462 25% -0.478839 -1.126752 50% -0.132920 -0.297040 75% 0.366072 0.250817 max 1.492208 1.253874 In [10]: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler = scaler . fit ( ms_df ) print ( scaler . mean_ ) scaled_df = scaler . transform ( ms_df ) ###### if I had a test set I would transform here: # test_scaled_df = scaler.transform(test_df) ################################################## scaled_df = pd . DataFrame ( scaled_df , \\ index = multishapes [ 'shape' ], columns = ms_df . columns ) scaled_df . describe () [-0.08122171 -0.6254313 ] Out[10]: x y count 1.100000e+03 1.100000e+03 mean -2.563606e-17 -2.995584e-16 std 1.000455e+00 1.000455e+00 min -2.183985e+00 -2.320473e+00 25% -6.167723e-01 -4.264248e-01 50% -8.019252e-02 2.793306e-01 75% 6.938298e-01 7.453401e-01 max 2.440659e+00 1.598544e+00 Very important reminder!! If you have a training and a test set, always .fit() your scaler only to the training set, and then .transform() both sets. Let's plot this data with and without scaling In [11]: # plot our data msplot = ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (no scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () In [12]: # plots our data msplot = scaled_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (w/ scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () 3a. k-Means clustering: Code (via sklearn ): In [13]: from sklearn.cluster import KMeans ms_kmeans = KMeans ( n_clusters = 2 , init = 'random' , n_init = 3 , random_state = 109 ) . fit ( scaled_df ) That's it! Just 1 line of code! Now that we've run k-Means, we can look at various attributes of our clusters. Full documenation is here . In [14]: display ( ms_kmeans . cluster_centers_ ) display ( ms_kmeans . labels_ [ 0 : 10 ]) array([[-1.02507148, -0.98833731], [ 0.42239787, 0.40726094]]) array([0, 1, 1, 0, 1, 1, 1, 0, 0, 0], dtype=int32) Plotting Take note of matplotlib's c= argument to color items in the plot, along with our stacking two different plotting functions in the same plot. In [15]: plt . figure ( figsize = ( 10 , 10 )) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = ms_kmeans . labels_ ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 100 ); Out[15]: Question : Is this expected or did something go wrong? Lessons: Initializations matter; run multiple times k-Means can struggle with clusters that are close together; they can get lumped into one There's no notion of 'not part of any cluster' or 'part of two clusters' (see Gaussian Mixture Models for this). Visualization here Breakroom Exercise : With your room, collectively discuss how k-means works. Create a synthetic data set with a few clusters and explore using K-means. Quality of Clusters: Inertia Inertia measures the total squared distance from points to their cluster's centroid. We obviously want this distance to be relatively small. If we increase the number of clusters, it will naturally make the average distance smaller. If every point has its own cluster, then our distance would be 0. That's obviously not an ideal way to cluster. One way to determine a reasonable number of clusters to simply try many different clusterings as we vary k , and each time, measure the overall inertia. In [16]: wss = [] for i in range ( 1 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) wss . append ( fitx . inertia_ ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), wss , 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Inertia' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Look for the place(s) where distance stops decreasing as much (i.e., the 'elbow' of the curve). It seems that 4 would be a good number of clusters, as a higher k yields diminishing returns. Exercise : Run K-means again with 4 clusters this time. Quality of Clusters: Silhouette Let's say we have a data point $i$, and the cluster it belongs to is referred to as $C(i)$. One way to measure the quality of a cluster $C(i)$ is to measure how close its data points are to each other (within-cluster) compared to nearby, other clusters $C(j)$. This is what Silhouette Scores provide for us. The range is [-1,1]; 0 indicates a point on the decision boundary (equal average closeness to points intra-cluster and out-of-cluster), and negative values mean that datum might be better in a different cluster. Specifically, let $a(i)$ denote the average distance data point $i$ is to the other points in the same cluster: Similarly, we can also compute the average distance that data point $i$ is to all other clusters. The cluster that yields the minimum distance is denoted by $b(i)$: Hopefully our data point $i$ is much closer, on average, to points within its own cluster (i.e., $a(i)$ than it is to its closest neighboring cluster $b(i)$). The silhouette score quantifies this as $s(i)$: NOTE: If data point $i$ belongs to its own cluster (no other points), then the silhouette score is set to 0 (otherwise, $a(i)$ would be undefined). The silhouette score plotted below is the overall average across all points in our dataset. The silhouette_score() function is available in sklearn . We can manually loop over values of K (for applying k-Means algorithm), then plot its silhouette score. In [17]: from sklearn.metrics import silhouette_score scores = [ 0 ] for i in range ( 2 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) score = silhouette_score ( scaled_df , fitx . labels_ ) scores . append ( score ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), np . array ( scores ), 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Average Silhouette' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Visualizing all Silhoutte scores for a particular clustering Below, we borrow from an sklearn example. The second plot may be overkill. The second plot is just the scaled data. It is not a PCA plot If you only need the raw silhouette scores, use the silhouette_samples() function In [18]: from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.cm as cm #modified code from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html def silplot ( X , clusterer , pointlabels = None ): cluster_labels = clusterer . labels_ n_clusters = clusterer . n_clusters # Create a subplot with 1 row and 2 columns fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) fig . set_size_inches ( 11 , 8.5 ) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1 . set_xlim ([ - 0.1 , 1 ]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1 . set_ylim ([ 0 , len ( X ) + ( n_clusters + 1 ) * 10 ]) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score ( X , cluster_labels ) print ( \"For n_clusters = \" , n_clusters , \", the average silhouette_score is \" , silhouette_avg , \".\" , sep = \"\" ) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples ( X , cluster_labels ) y_lower = 10 for i in range ( 0 , n_clusters + 1 ): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values [ cluster_labels == i ] ith_cluster_silhouette_values . sort () size_cluster_i = ith_cluster_silhouette_values . shape [ 0 ] y_upper = y_lower + size_cluster_i color = cm . nipy_spectral ( float ( i ) / n_clusters ) ax1 . fill_betweenx ( np . arange ( y_lower , y_upper ), 0 , ith_cluster_silhouette_values , facecolor = color , edgecolor = color , alpha = 0.7 ) # Label the silhouette plots with their cluster numbers at the middle ax1 . text ( - 0.05 , y_lower + 0.5 * size_cluster_i , str ( i )) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1 . set_title ( \"The silhouette plot for the various clusters.\" ) ax1 . set_xlabel ( \"The silhouette coefficient values\" ) ax1 . set_ylabel ( \"Cluster label\" ) # The vertical line for average silhouette score of all the values ax1 . axvline ( x = silhouette_avg , color = \"red\" , linestyle = \"--\" ) ax1 . set_yticks ([]) # Clear the yaxis labels / ticks ax1 . set_xticks ([ - 0.1 , 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1 ]) # 2nd Plot showing the actual clusters formed colors = cm . nipy_spectral ( cluster_labels . astype ( float ) / n_clusters ) ax2 . scatter ( X [:, 0 ], X [:, 1 ], marker = '.' , s = 200 , lw = 0 , alpha = 0.7 , c = colors , edgecolor = 'k' ) xs = X [:, 0 ] ys = X [:, 1 ] if pointlabels is not None : for i in range ( len ( xs )): plt . text ( xs [ i ], ys [ i ], pointlabels [ i ]) # Labeling the clusters centers = clusterer . cluster_centers_ # Draw white circles at cluster centers ax2 . scatter ( centers [:, 0 ], centers [:, 1 ], marker = 'o' , c = \"white\" , alpha = 1 , s = 200 , edgecolor = 'k' ) for i , c in enumerate ( centers ): ax2 . scatter ( c [ 0 ], c [ 1 ], marker = '$ %d $' % int ( i ), alpha = 1 , s = 50 , edgecolor = 'k' ) ax2 . set_title ( \"The visualization of the clustered data.\" ) ax2 . set_xlabel ( \"Feature space for the 1st feature\" ) ax2 . set_ylabel ( \"Feature space for the 2nd feature\" ) plt . suptitle (( \"Silhouette analysis for KMeans clustering on sample data \" \"with n_clusters = %d \" % n_clusters ), fontsize = 14 , fontweight = 'bold' ) In [19]: # run k-means with 3 clusters ms_kmeans = KMeans ( n_clusters = 4 , init = 'random' , n_init = 3 , random_state = 109 ) . fit ( scaled_df ) # plot a fancy silhouette plot silplot ( scaled_df . values , ms_kmeans ) For n_clusters = 4, the average silhouette_score is 0.4588053944508592. Food for thought : Using the silhouette scores' optimal number of clusters (per the elbow plot above): Fit a new k-Means model with that many clusters Plot the clusters like we originally did with k-means Plot the silhouette scores just like the above cells Which seems like a better clustering (i.e., 3 clusters or the number returned by the elbow plot above)? In [20]: Quality of Clusters: Gap Statistic The gap statistic compares within-cluster distances (such as in silhouette), but instead of comparing against the second-best existing cluster for that point, it compares our clustering's overall average to the average we'd see if the data were generated at random (we'd expect randomly generated data to not necessarily have any inherit patterns that can be easily clustered). In essence, the within-cluster distances (in the elbow plot) will go down just becuse we have more clusters. We additionally calculate how much they'd go down on non-clustered data with the same spread as our data and subtract that trend out to produce the plot below. The original paper is : \" Estimating the number of clusters in a data set via the gap statistic \" (Tibshirani et al.). As suggested in the paper, we would choose the value of $\\hat{k}$ (number of clusters) such that $\\hat{k}$ = smallest $k$ such that Gap($k$) $\\geq$ Gap($k+1$) - $s_{k+1}$. We compare the actual Gap value of the k point to the lower bar of the Gap value of the k+1 point. The following graph should make it clearer. The plot is from the original paper (Fig. 2) (dashed lines are mine) We could argue that we should have chosen the largest value (k =3) instead of the first value that satisfies the Gap statistic equation (k=1 in this case). If you're able to compute for a range of k, then you can choose the maximum. For example, in the graph above, since we're computing over k=1,..,10, we could choose k=3. The original paper although it suggests that we look at the whole range, chooses k=1 in the case above; if you see the raw data plotted (Fig. 2 in the paper) you will also notice that there is really not much structure for cluster subdivision but we should always investigate the whole plot. Also, it's very computationally intensive to compute the Gap statistic. Additionally, you can use domain knowledge or whatever information you have about the data to choose k. The gap statistic is implemented by Miles Granger in the gap_statistic Python library. The library also implements the Gap$&#94;*$ statistic described in \" A comparison of Gap statistic definitions with and with-out logarithm function \" (Mohajer, M., Englmeier, K. H., & Schmid, V. J., 2011) which is less conservative but tends to perform suboptimally when clusters overlap. In [21]: from gap_statistic import OptimalK from sklearn.datasets.samples_generator import make_blobs from sklearn.datasets import make_classification import warnings warnings . filterwarnings ( 'ignore' ) /usr/lib/python3.9/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API. warnings.warn(message, FutureWarning) Playing with synthetic data Sklearn has some wonderful methods for generative synthetic datasets . They can be quite useful for testing clustering for classification purposes. In [22]: plt . title ( \"Three blobs\" , fontsize = 'small' ) X1 , Y1 = make_blobs ( n_features = 2 , centers = 4 ) # centers is number of classes plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); blob_df = pd . DataFrame ( X1 , Y1 ) plt . axis ( 'equal' ); blob_df . head () Out[22]: 0 1 0 -3.423606 5.679890 0 -4.791970 6.547547 1 3.849651 -3.347212 2 -4.069686 8.470518 0 -5.766943 4.150653 In [23]: gs_obj = OptimalK ( n_jobs = 1 , n_iter = 10 ) n_clusters = gs_obj ( X1 , n_refs = 50 , cluster_array = np . arange ( 1 , 15 )) print ( 'Optimal number of clusters: ' , n_clusters ) Optimal number of clusters: 5 In [24]: ms_kmeans = KMeans ( n_clusters = n_clusters , init = 'random' , \\ n_init = 3 , random_state = 109 ) . fit ( X1 ) plt . figure ( figsize = ( 5 , 5 )) plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], \\ ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 200 ); plt . axis ( 'equal' ); Out[24]: (-8.710021442970524, 6.936551818355399, -13.185487954180806, 12.952525116735263) In [25]: gs_obj . plot_results () In [26]: # show GAP and GAP* statistics gs_obj . gap_df . iloc [:,: 3 ] Out[26]: n_clusters gap_value gap* 0 1.0 -0.159882 -1053.161146 1 2.0 1.072560 1862.749122 2 3.0 1.181629 1434.066364 3 4.0 0.889080 885.141358 4 5.0 1.918201 955.771545 5 6.0 1.873629 794.569395 6 7.0 1.829375 658.033928 7 8.0 1.678665 556.816137 8 9.0 1.534336 453.338871 9 10.0 1.611888 411.066925 10 11.0 1.378228 350.825496 11 12.0 1.782349 353.877387 12 13.0 1.716997 317.795464 13 14.0 1.600748 284.313771 If we wish to add error bars to help us decide how many clusters to use, the following code displays such: In [27]: def display_gapstat_with_errbars ( gap_df ): gaps = gap_df [ \"gap_value\" ] . values diffs = gap_df [ \"diff\" ] err_bars = np . zeros ( len ( gap_df )) err_bars [ 1 :] = diffs [: - 1 ] - gaps [: - 1 ] + gaps [ 1 :] plt . scatter ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ]) plt . errorbar ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ], yerr = err_bars , capsize = 6 ) plt . xlabel ( \"Number of Clusters\" ) plt . ylabel ( \"Gap Statistic\" ) plt . show () display_gapstat_with_errbars ( gs_obj . gap_df ) For more information about the gap_stat package, please see the full documentation here . 3b. Agglomerative Clustering Code (via scipy ): There are many different cluster-merging criteria, one of which is Ward's criteria. Ward's optimizes having the lowest total within-cluster distances, so it merges the two clusters that will harm this objective least. scipy 's agglomerative clustering function implements Ward's method. In [28]: import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist plt . figure ( figsize = ( 11 , 8.5 )) dist_mat = pdist ( scaled_df , metric = \"euclidean\" ) ward_data = hac . ward ( dist_mat ) hac . dendrogram ( ward_data ); plt . show () Discussion : How do you read a plot like the above? What are valid options for number of clusters, and how can you tell? Are some more valid than others? Does it make sense to compute silhouette scores for an agglomerative clustering? If we wanted to compute silhouette scores, what would we need for this to be possible? Lessons: It's expensive: O(n&#94;3) time complexity and O(n&#94;2) space complexity. Many choices for linkage criteria Every node gets clustered (no child left behind) In [29]: 3c. DBSCAN Clustering DBSCAN uses an intuitive notion of denseness to define clusters, rather than defining clusters by a central point as in k-means. Code (via sklearn ): DBscan is implemented in good 'ol sklearn, but there aren't great automated tools for searching for the optimal epsilon parameter. For full documentation, please visit this page In [30]: from sklearn.cluster import DBSCAN plt . figure ( figsize = ( 11 , 8.5 )) fitted_dbscan = DBSCAN ( eps = 0.2 ) . fit ( scaled_df ) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = fitted_dbscan . labels_ ); Out[30]: Note: the dark purple dots are not clustered with anything else. They are lone singletons. You can validate such by setting epsilon to a very small value, and increase the min_samples to a high value. Under these conditions, nothing would cluster, and yet all dots become dark purple. Exercise : Experiment with the above code by changing its epsilon value and the min_samples (what is the default value for it, since the above code doesn't specify a value?) Instead of just empirically observing how the epsilon value affects the clustering (which would be very costly for large, high-dimensional data), we can also inspect how far each data point is to its $N&#94;{th}$ closest neighbor: In [31]: from sklearn.neighbors import NearestNeighbors # x-axis is each individual data point, numbered by an artificial index # y-axis is the distance to its 2nd closest neighbor def plot_epsilon ( df , min_samples ): fitted_neigbors = NearestNeighbors ( n_neighbors = min_samples ) . fit ( df ) distances , indices = fitted_neigbors . kneighbors ( df ) dist_to_nth_nearest_neighbor = distances [:, - 1 ] plt . plot ( np . sort ( dist_to_nth_nearest_neighbor )) plt . xlabel ( \"Index \\n (sorted by increasing distances)\" ) plt . ylabel ( \" {} -NN Distance (epsilon)\" . format ( min_samples - 1 )) plt . tick_params ( right = True , labelright = True ) In [32]: plot_epsilon ( scaled_df , 3 ) Lessons: Can cluster non-linear relationships very well; potential for more natural, arbritrarily shaped groupings Does not require specifying the # of clusters (i.e., k ); the algorithm determines such Robust to outliers Very sensitive to the parameters (requires strong knowledge of the data) Doesn't guarantee that every (or ANY) item will be clustered Discussion : When should we prefer one type of clustering over another? Should we always just try all of them? Imagine you work at Spotify and you want to create personalized playlists for each person. One could imagine a dataset exists whereby each row is a particular song, and the columns are features (e.g., tempo (BPM), average vocal frequency, amount of bass, sentiment of lyrics, duration in seconds, etc). Let's use clustering to group one's catalog of favorite music, which will serve as disjoint starting points for suggesting future songs. Specifically, imagine that you've 'liked' 500 songs on Spotify so far, and your recommendation algorithm needs to cluster those 500 songs. Would you first experiment with k-Means, Agglomerative, or DBScan? Why? References: [1] A Tutorial on Principal Component Analysis",
        "tags": "Lectures",
        "url": "lectures/lecture08/notebook2/"
    }, {
        "title": "Lecture 7: Bayesian statistics (part 1)",
        "text": "Exercises Lecture 7 : Bayesian Example Notebook (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture07/"
    }, {
        "title": "Lecture 7 - Bayesian statistics (part 1)",
        "text": "Title Bayesian Example Notebook Description : This notebook provides example code based on the lecture material. If you wish to run or edit the notebook, we recommend downloading it and running it either on your local machine or on JupyterHub. In [2]: import pymc3 as pm In [1]: import numpy as np % matplotlib inline import matplotlib.pyplot as plt import pandas as pd import scipy.stats as st import seaborn as sns import matplotlib.pyplot as plt In [3]: n_theta = 10000 # generate 10,000 values from Beta(2,5) theta = np . random . beta ( 2 , 5 , n_theta ) print ( \"First five values of theta: \\n\\t \" , theta [ 0 : 5 ]) print ( \"Sample mean: \\n\\t \" , np . mean ( theta )) print ( \"The 2.5 % a nd 97.5 % o f quantiles: \\n\\t \" , np . percentile ( theta ,[ 2.5 , 97.5 ])) First five values of theta: [0.09380107 0.09315237 0.43342451 0.40271348 0.1825154 ] Sample mean: 0.2846024748126768 The 2.5% and 97.5% of quantiles: [0.04172583 0.63937365] In [4]: plt . hist ( theta , 50 ) plt . xlabel ( \"Value of Theta\" ) plt . ylabel ( \"Count\" ) plt . show () In [5]: # simulate y from posterior predictive distribution y = np . random . binomial ( 1 , theta , n_theta ) # generate a heads/tails value from each of the 10,000 thetas print ( \"First 5 heads/tails values (tails=0, heads=1) \\n\\t \" , y [ 0 : 10 ]) print ( \"Overall frequency of Tails and Heads, accounting for uncertainty about theta itself \\n\\t \" , np . bincount ( y ) / 10000 ) plt . hist ( y , density = True ) plt . xticks ([ . 05 , . 95 ],[ \"Tails\" , \"Heads\" ]) plt . show () First 5 heads/tails values (tails=0, heads=1) [0 0 0 0 0 1 0 0 0 0] Overall frequency of Tails and Heads, accounting for uncertainty about theta itself [0.7163 0.2837] Rejection sampling and Weighted bootstrap Example adapted from https://wiseodd.github.io/techblog/2015/10/21/rejection-sampling/ In [2]: sns . set () def h ( x ): return st . norm . pdf ( x , loc = 30 , scale = 10 ) + st . norm . pdf ( x , loc = 80 , scale = 20 ) def g ( x ): return st . norm . pdf ( x , loc = 50 , scale = 30 ) x = np . arange ( - 50 , 151 ) M = max ( h ( x ) / g ( x )) # for rejection sampling h is a mixture of two normal distributions (unnormalized), and density h is a normal distribution with mean 50 and standard deviation 30. In [17]: plt . plot ( x , h ( x )) plt . show () In [18]: # Superimpose h and g on same plot plt . plot ( x , h ( x )) plt . plot ( x , g ( x )) plt . show () In [19]: # Superimpose h and M*g on same plot - now M*g envelopes h plt . plot ( x , h ( x )) plt . plot ( x , M * g ( x )) plt . show () In [5]: def rejection_sampling ( maxiter = 10000 , sampsize = 1000 ): samples = [] sampcount = 0 # counter for accepted samples maxcount = 0 # counter for proposal simulation # sampcount/maxcount at any point in the iteration is the acceptance rate while ( sampcount < sampsize and maxcount < maxiter ): z = np . random . normal ( 50 , 30 ) u = np . random . uniform ( 0 , 1 ) maxcount += 1 if u <= h ( z ) / ( M * g ( z )): samples . append ( z ) sampcount += 1 print ( 'Rejection rate is' , 100 * ( 1 - sampcount / maxcount )) if maxcount == maxiter : print ( 'Maximum iterations achieved' ) return np . array ( samples ) s = rejection_sampling ( maxiter = 10000 , sampsize = 1000 ) sns . displot ( s ) Rejection rate is 49.54591321897074 Out[5]: In [25]: # weighted bootstrap computation involving h and g import random def weighted_bootstrap ( iter = 1000 , size = 100 ): w = [] y = [] for i in range ( iter ): z = np . random . normal ( 50 , 30 ) y . append ( z ) wz = h ( z ) / g ( z ) w . append ( wz ) v = random . choices ( y , weights = w , k = size ) # do not need to renormalize w return np . array ( v ) wb = weighted_bootstrap ( iter = 10000 , size = 1000 ) sns . displot ( wb ) Out[25]: Beetles In [5]: beetles_x = np . array ([ 1.6907 , 1.7242 , 1.7552 , 1.7842 , 1.8113 , 1.8369 , 1.8610 , 1.8839 ]) beetles_x_mean = beetles_x - np . mean ( beetles_x ) beetles_n = np . array ([ 59 , 60 , 62 , 56 , 63 , 59 , 62 , 60 ]) beetles_y = np . array ([ 6 , 13 , 18 , 28 , 52 , 53 , 61 , 60 ]) beetles_N = np . array ([ 8 ] * 8 ) In [6]: from scipy.special import expit expit ( 2 ) Out[6]: 0.8807970779778823 In [7]: with pm . Model () as beetle_model : # The intercept (log probability of beetles dying when dose=0) # is centered at zero, and wide-ranging (easily anywhere from 0 to 100%) # If we wanted, we could choose something like Normal(-3,2) for a no-dose # death rate roughly between .007 and .25 alpha_star = pm . Normal ( 'alpha*' , mu = 0 , sigma = 100 ) # the effect on the log-odds of each unit of the dose is wide-ranging: # we're saying we've got little idea what the effect will be, and it could # be strongly negative. beta = pm . Normal ( 'beta' , mu = 0 , sigma = 100 ) # given alpha, beta, and the dosage, the probability of death is deterministic: # it's the inverse logit of the intercept+slope*dosage # Because beetles_x has 8 entries, we end up with 8 p_i values p_i = pm . Deterministic ( '$P_i$' , pm . math . invlogit ( alpha_star + beta * beetles_x_mean )) # finally, the number of bettles we see killed is Binomial(n=number of beetles, p=probability of death) deaths = pm . Binomial ( 'obs_deaths' , n = beetles_n , p = p_i , observed = beetles_y ) trace = pm . sample ( 2000 , tune = 2000 , target_accept = 0.9 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta, alpha*] Sampling 2 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:03<00:00, 2500.25draws/s] In [8]: pm . traceplot ( trace , compact = False ); In [9]: def trace_summary ( trace , var_names = None ): if var_names is None : var_names = trace . varnames quants = [ 0.025 , 0.25 , 0.5 , 0.75 , 0.975 ] colnames = [ 'mean' , 'sd' , * [ \" {} %\" . format ( x * 100 ) for x in quants ]] rownames = [] series = [] for cur_var in var_names : var_trace = trace [ cur_var ] if var_trace . ndim == 1 : vals = [ np . mean ( var_trace , axis = 0 ), np . std ( var_trace , axis = 0 ), * np . quantile ( var_trace , quants , axis = 0 )] series . append ( pd . Series ( vals , colnames )) rownames . append ( cur_var ) else : for i in range ( var_trace . shape [ 1 ]): cur_col = var_trace [:, i ] vals = [ np . mean ( cur_col , axis = 0 ), np . std ( cur_col , axis = 0 ), * np . quantile ( cur_col , quants , axis = 0 )] series . append ( pd . Series ( vals , colnames )) rownames . append ( \" {} [ {} ]\" . format ( cur_var , i )) return pd . DataFrame ( series , index = rownames ) trace_summary ( trace ) Out[9]: mean sd 2.5% 25.0% 50.0% 75.0% 97.5% alpha* 0.751390 0.141753 0.478711 0.652996 0.748475 0.846896 1.033710 beta 34.616909 2.931295 28.936764 32.611365 34.572128 36.488580 40.633339 $P_i$[0] 0.059134 0.016350 0.032743 0.047590 0.057312 0.068486 0.096447 $P_i$[1] 0.163829 0.028815 0.112463 0.143730 0.161691 0.182162 0.224477 $P_i$[2] 0.361563 0.035229 0.294835 0.337852 0.360971 0.384664 0.430386 $P_i$[3] 0.605892 0.032435 0.541279 0.583828 0.605457 0.627823 0.668789 $P_i$[4] 0.796114 0.026486 0.742425 0.778172 0.796806 0.814523 0.846053 $P_i$[5] 0.903587 0.018611 0.863879 0.891767 0.904673 0.917122 0.936220 $P_i$[6] 0.955078 0.011669 0.929487 0.947880 0.956265 0.963560 0.974797 $P_i$[7] 0.978768 0.006921 0.962938 0.974709 0.979674 0.983812 0.989860 We can also plot the density each chain explored. Any major deviations between chains are signs of difficulty converging. In [10]: for x in trace . varnames : pm . plot_forest ( trace , var_names = [ x ], combined = True ) In addition to the above summaries of the distribution, pymc3 has statistics intended to summarize the quality of the samples. The most common of these is r_hat, which measures whether the different chains seem to be exploring the same space or if they're stuck in different spaces. R-hat above 1.3 is a strong sign the sample isn't good yet. Values close to 1 are ideal. In [11]: pm . summary ( trace ) Out[11]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat alpha* 0.751 0.142 0.503 1.035 0.003 0.002 2665.0 2665.0 2665.0 2345.0 1.0 beta 34.617 2.932 29.249 40.277 0.058 0.041 2574.0 2550.0 2591.0 2500.0 1.0 $P_i$[0] 0.059 0.016 0.032 0.090 0.000 0.000 3302.0 3211.0 3261.0 2703.0 1.0 $P_i$[1] 0.164 0.029 0.112 0.220 0.000 0.000 3520.0 3502.0 3495.0 2802.0 1.0 $P_i$[2] 0.362 0.035 0.298 0.427 0.001 0.000 3612.0 3612.0 3596.0 2665.0 1.0 $P_i$[3] 0.606 0.032 0.542 0.664 0.001 0.000 2917.0 2917.0 2924.0 2393.0 1.0 $P_i$[4] 0.796 0.026 0.745 0.844 0.001 0.000 2369.0 2369.0 2367.0 2337.0 1.0 $P_i$[5] 0.904 0.019 0.868 0.937 0.000 0.000 2253.0 2253.0 2254.0 2097.0 1.0 $P_i$[6] 0.955 0.012 0.933 0.975 0.000 0.000 2262.0 2262.0 2255.0 2012.0 1.0 $P_i$[7] 0.979 0.007 0.966 0.991 0.000 0.000 2294.0 2294.0 2281.0 2258.0 1.0 Sleep Study In [14]: import pandas as pd sleepstudy = pd . read_csv ( \"sleepstudy.csv\" ) In [15]: sleepstudy Out[15]: Reaction Days Subject 0 249.5600 0 308 1 258.7047 1 308 2 250.8006 2 308 3 321.4398 3 308 4 356.8519 4 308 5 414.6901 5 308 6 382.2038 6 308 7 290.1486 7 308 8 430.5853 8 308 9 466.3535 9 308 10 222.7339 0 309 11 205.2658 1 309 12 202.9778 2 309 13 204.7070 3 309 14 207.7161 4 309 15 215.9618 5 309 16 213.6303 6 309 17 217.7272 7 309 18 224.2957 8 309 19 237.3142 9 309 20 199.0539 0 310 21 194.3322 1 310 22 234.3200 2 310 23 232.8416 3 310 24 229.3074 4 310 25 220.4579 5 310 26 235.4208 6 310 27 255.7511 7 310 28 261.0125 8 310 29 247.5153 9 310 ... ... ... ... 150 225.2640 0 370 151 234.5235 1 370 152 238.9008 2 370 153 240.4730 3 370 154 267.5373 4 370 155 344.1937 5 370 156 281.1481 6 370 157 347.5855 7 370 158 365.1630 8 370 159 372.2288 9 370 160 269.8804 0 371 161 272.4428 1 371 162 277.8989 2 371 163 281.7895 3 371 164 279.1705 4 371 165 284.5120 5 371 166 259.2658 6 371 167 304.6306 7 371 168 350.7807 8 371 169 369.4692 9 371 170 269.4117 0 372 171 273.4740 1 372 172 297.5968 2 372 173 310.6316 3 372 174 287.1726 4 372 175 329.6076 5 372 176 334.4818 6 372 177 343.2199 7 372 178 369.1417 8 372 179 364.1236 9 372 180 rows √ó 3 columns In [16]: # adding a column that numbers the subjects from 0 to n raw_ids = np . unique ( sleepstudy [ 'Subject' ]) raw2newid = { x : np . where ( raw_ids == x )[ 0 ][ 0 ] for x in raw_ids } sleepstudy [ 'SeqSubject' ] = sleepstudy [ 'Subject' ] . map ( raw2newid ) sleepstudy Out[16]: Reaction Days Subject SeqSubject 0 249.5600 0 308 0 1 258.7047 1 308 0 2 250.8006 2 308 0 3 321.4398 3 308 0 4 356.8519 4 308 0 5 414.6901 5 308 0 6 382.2038 6 308 0 7 290.1486 7 308 0 8 430.5853 8 308 0 9 466.3535 9 308 0 10 222.7339 0 309 1 11 205.2658 1 309 1 12 202.9778 2 309 1 13 204.7070 3 309 1 14 207.7161 4 309 1 15 215.9618 5 309 1 16 213.6303 6 309 1 17 217.7272 7 309 1 18 224.2957 8 309 1 19 237.3142 9 309 1 20 199.0539 0 310 2 21 194.3322 1 310 2 22 234.3200 2 310 2 23 232.8416 3 310 2 24 229.3074 4 310 2 25 220.4579 5 310 2 26 235.4208 6 310 2 27 255.7511 7 310 2 28 261.0125 8 310 2 29 247.5153 9 310 2 ... ... ... ... ... 150 225.2640 0 370 15 151 234.5235 1 370 15 152 238.9008 2 370 15 153 240.4730 3 370 15 154 267.5373 4 370 15 155 344.1937 5 370 15 156 281.1481 6 370 15 157 347.5855 7 370 15 158 365.1630 8 370 15 159 372.2288 9 370 15 160 269.8804 0 371 16 161 272.4428 1 371 16 162 277.8989 2 371 16 163 281.7895 3 371 16 164 279.1705 4 371 16 165 284.5120 5 371 16 166 259.2658 6 371 16 167 304.6306 7 371 16 168 350.7807 8 371 16 169 369.4692 9 371 16 170 269.4117 0 372 17 171 273.4740 1 372 17 172 297.5968 2 372 17 173 310.6316 3 372 17 174 287.1726 4 372 17 175 329.6076 5 372 17 176 334.4818 6 372 17 177 343.2199 7 372 17 178 369.1417 8 372 17 179 364.1236 9 372 17 180 rows √ó 4 columns In [17]: with pm . Model () as sleep_model : # In this model, we're going to say the alphas (individuals' intercepts; their starting reaction time) # and betas (individuals' slopes; how much worse they get with lack of sleep) are normally distributed. # We'll specify that we're certain about the mean of those distribution [more on that later], but admit # we're uncertain about how much spread there is (i.e. uncertain about the SD). Tau_alpha and Tau_beta # will be the respective SD. # # Of course, the SDs must be positive (negative SD isn't mathematically possible), so we draw them from # a Gamma, which cannot ever output negative numbers. Here, we use alpha and beta values that spread the # distribution: \"the SD could be anything!\". If we had more intuition (e.g. \"the starting reaction times can't # have SD above 3,000\") we would plot Gamma(a,b) and tune the parameters so that there was little mass # above 3,000, then use those values below) tau_alpha = pm . Gamma ( 'tau_alpha' , alpha =. 001 , beta =. 001 ) tau_beta = pm . Gamma ( 'tau_beta' , alpha =. 001 , beta =. 001 ) # Across the population of people, we suppose that # the slopes are normally distributed, as are the intercepts, # and the two are drawn independently # # (Here, we hard-code assumed means, but we don't have to. # In general, these should be set from our pre-data intuition, # rather than from plots/exploration of the data) alpha = pm . Normal ( 'alpha' , mu = 300 , tau = tau_alpha , shape = len ( raw_ids )) beta = pm . Normal ( 'beta' , mu = 10 , tau = tau_beta , shape = len ( raw_ids )) # Remember: there's only one alpha/beta per person, but # we have lots of observations per person. The below # builds a vector with one entry per observation, recording # the alpha/beta we want to use with that observation. # # That is, the length is 180, but it only has 17 unique values, # matching the 17 unique patients' personal slopes or intercepts intercepts = alpha [ sleepstudy [ 'SeqSubject' ]] slopes = beta [ sleepstudy [ 'SeqSubject' ]] # now we have the true/predicted response time for each observation (each row of original data) # (Here we use pm.Deterministic to signal that this is something we'll care about) mu_i = pm . Deterministic ( 'mu_i' , intercepts + slopes * sleepstudy [ 'Days' ]) # The _observed_ values are noisy versions of the hidden true values, however! # Specifically, we model them as a normal at the true value and single unknown variance # (one explanation: we're saying the measurement equipment adds normally-distributed noise tau_obs # so noise doesn't vary from observation to observation or person to person: there's just one universal # noise level) tau_obs = pm . Gamma ( 'tau_obs' , 0.001 , 0.001 ) obs = pm . Normal ( 'observed' , mu = mu_i , tau = tau_obs , observed = sleepstudy [ 'Reaction' ]) trace = pm . sample ( 2000 , tune = 2000 , target_accept = 0.9 ) /usr/local/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. rval = inputs[0].__getitem__(inputs[1:]) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... /usr/local/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. rval = inputs[0].__getitem__(inputs[1:]) Multiprocess sampling (2 chains in 2 jobs) NUTS: [tau_obs, beta, alpha, tau_beta, tau_alpha] /usr/local/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. rval = inputs[0].__getitem__(inputs[1:]) Sampling 2 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:31<00:00, 253.43draws/s] /usr/local/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. rval = inputs[0].__getitem__(inputs[1:]) /usr/local/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. rval = inputs[0].__getitem__(inputs[1:]) In [18]: # this command can take a few minutes to finish... or never :-/ #pm.traceplot(trace); In [19]: trace_summary ( trace , var_names = [ 'tau_alpha' , 'tau_beta' , 'alpha' , 'beta' , 'tau_obs' ]) Out[19]: mean sd 2.5% 25.0% 50.0% 75.0% 97.5% tau_alpha 0.000351 0.000126 0.000157 0.000258 0.000335 0.000424 0.000636 tau_beta 0.032879 0.014897 0.012892 0.022287 0.030065 0.039815 0.070586 alpha[0] 257.610159 14.276656 230.115045 247.875540 257.918984 267.516111 284.538994 alpha[1] 204.767474 14.186468 176.932727 194.945433 204.590902 214.482879 233.084446 alpha[2] 206.308667 13.936264 179.203795 196.948602 206.214474 215.736714 233.825074 alpha[3] 284.217173 13.812208 257.704908 275.015790 284.269294 293.375280 312.064149 alpha[4] 282.553242 13.794822 255.756413 272.742095 282.844921 292.141532 309.047249 alpha[5] 266.389315 13.301735 240.061457 257.526522 266.321948 275.317963 292.045105 alpha[6] 275.756079 13.898037 249.559137 266.256139 275.445158 285.073414 302.814478 alpha[7] 245.947350 13.812559 220.028833 236.301384 245.823140 255.208295 273.564540 alpha[8] 254.546107 14.107783 226.722294 244.874362 254.873410 264.339746 282.056730 alpha[9] 298.360781 13.877619 270.250737 289.090439 298.492528 307.424551 326.031047 alpha[10] 223.806270 13.688663 197.522834 214.846560 223.526668 233.104416 251.253458 alpha[11] 238.477493 13.881598 211.236122 229.199742 238.468575 247.921632 265.016599 alpha[12] 260.528639 13.862791 233.029493 250.978244 260.487759 270.120198 287.657004 alpha[13] 280.915228 13.621030 254.200772 271.707665 280.972223 290.439985 307.043620 alpha[14] 258.965037 13.841049 231.507900 249.678285 259.082006 268.228741 286.604578 alpha[15] 223.087924 13.632430 196.058643 214.076329 222.919884 232.662976 249.742013 alpha[16] 255.924069 13.591729 229.473078 246.796792 256.075080 265.065273 283.011312 alpha[17] 270.164065 13.323427 244.310881 261.067455 270.192393 279.027785 295.950650 beta[0] 18.997266 2.667786 14.014857 17.184207 18.913597 20.853035 24.286721 beta[1] 2.823013 2.581428 -2.371087 1.095152 2.853109 4.531844 7.863045 beta[2] 5.992810 2.524769 1.110905 4.286558 5.978541 7.658774 11.105639 beta[3] 4.326279 2.577575 -0.882860 2.640365 4.382466 6.056187 9.304027 beta[4] 6.016240 2.516203 1.092221 4.289339 6.051726 7.736804 10.856854 beta[5] 9.272482 2.431098 4.494618 7.593650 9.286661 10.909383 14.127629 beta[6] 9.060972 2.527984 4.093397 7.408408 9.117599 10.778553 13.914114 beta[7] 11.260951 2.483213 6.417567 9.595526 11.263871 12.920773 16.121563 beta[8] -0.747060 2.643854 -5.964781 -2.518308 -0.764329 1.066817 4.372805 beta[9] 17.169261 2.543533 12.175148 15.435345 17.175887 18.827482 22.325273 beta[10] 11.977814 2.477412 7.032425 10.387415 11.944083 13.620032 16.861610 beta[11] 16.996413 2.565449 11.919296 15.317595 16.977097 18.726924 21.886162 beta[12] 6.762851 2.528101 1.847453 5.017565 6.796922 8.480314 11.720424 beta[13] 12.658657 2.507393 7.745680 10.970936 12.652850 14.359067 17.652091 beta[14] 10.677100 2.512257 5.819120 8.993925 10.666675 12.367837 15.621083 beta[15] 15.618545 2.486056 10.715424 13.966950 15.604372 17.301670 20.496553 beta[16] 8.895759 2.499516 4.066615 7.166113 8.935806 10.606355 13.760942 beta[17] 10.718514 2.472771 6.009184 9.051166 10.653531 12.396039 15.576719 tau_obs 0.001516 0.000182 0.001182 0.001389 0.001510 0.001633 0.001894 In [20]: pm . summary ( trace , var_names = [ 'tau_alpha' , 'tau_beta' , 'alpha' , 'beta' , 'tau_obs' ]) Out[20]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat tau_alpha 0.000 0.000 0.000 0.001 0.000 0.000 4962.0 4841.0 4570.0 2616.0 1.0 tau_beta 0.033 0.015 0.010 0.060 0.000 0.000 3944.0 3156.0 4351.0 3222.0 1.0 alpha[0] 257.610 14.278 230.512 282.679 0.191 0.135 5581.0 5581.0 5596.0 3088.0 1.0 alpha[1] 204.767 14.188 179.835 233.325 0.195 0.138 5302.0 5261.0 5327.0 3152.0 1.0 alpha[2] 206.309 13.938 179.206 231.732 0.187 0.132 5577.0 5571.0 5577.0 2851.0 1.0 alpha[3] 284.217 13.814 257.316 308.996 0.182 0.129 5783.0 5720.0 5783.0 2953.0 1.0 alpha[4] 282.553 13.797 257.089 307.915 0.175 0.124 6195.0 6195.0 6239.0 3133.0 1.0 alpha[5] 266.389 13.303 240.627 290.772 0.173 0.124 5898.0 5779.0 5944.0 2723.0 1.0 alpha[6] 275.756 13.900 249.832 300.624 0.176 0.126 6220.0 6114.0 6217.0 2664.0 1.0 alpha[7] 245.947 13.814 220.274 271.449 0.178 0.127 6015.0 5932.0 6058.0 2878.0 1.0 alpha[8] 254.546 14.110 227.740 280.716 0.191 0.135 5450.0 5450.0 5466.0 3309.0 1.0 alpha[9] 298.361 13.879 271.794 324.844 0.185 0.131 5641.0 5625.0 5631.0 3109.0 1.0 alpha[10] 223.806 13.690 197.589 248.781 0.175 0.125 6094.0 6020.0 6064.0 2923.0 1.0 alpha[11] 238.477 13.883 211.667 263.119 0.196 0.138 5040.0 5040.0 5055.0 3124.0 1.0 alpha[12] 260.529 13.865 235.832 287.952 0.179 0.126 6022.0 6022.0 6068.0 3259.0 1.0 alpha[13] 280.915 13.623 256.622 307.060 0.183 0.130 5513.0 5513.0 5480.0 2825.0 1.0 alpha[14] 258.965 13.843 232.875 285.499 0.185 0.131 5625.0 5625.0 5623.0 2764.0 1.0 alpha[15] 223.088 13.634 197.461 248.683 0.181 0.129 5659.0 5598.0 5637.0 3231.0 1.0 alpha[16] 255.924 13.593 230.817 281.683 0.176 0.125 5976.0 5891.0 5980.0 2759.0 1.0 alpha[17] 270.164 13.325 244.756 294.399 0.178 0.126 5633.0 5590.0 5645.0 2749.0 1.0 beta[0] 18.997 2.668 13.982 23.852 0.036 0.026 5595.0 5175.0 5684.0 3344.0 1.0 beta[1] 2.823 2.582 -2.183 7.532 0.033 0.030 6111.0 3708.0 6140.0 3367.0 1.0 beta[2] 5.993 2.525 1.093 10.622 0.034 0.027 5590.0 4462.0 5608.0 2575.0 1.0 beta[3] 4.326 2.578 -0.702 8.992 0.033 0.026 6240.0 4928.0 6247.0 3475.0 1.0 beta[4] 6.016 2.517 1.073 10.390 0.032 0.025 6178.0 5272.0 6200.0 3556.0 1.0 beta[5] 9.272 2.431 4.532 13.651 0.031 0.023 6113.0 5811.0 6079.0 2772.0 1.0 beta[6] 9.061 2.528 4.399 13.781 0.034 0.024 5466.0 5381.0 5492.0 2641.0 1.0 beta[7] 11.261 2.484 6.418 15.734 0.031 0.023 6249.0 5859.0 6261.0 2674.0 1.0 beta[8] -0.747 2.644 -5.870 4.064 0.035 0.039 5739.0 2324.0 5742.0 3485.0 1.0 beta[9] 17.169 2.544 12.072 21.622 0.033 0.024 5777.0 5747.0 5756.0 3323.0 1.0 beta[10] 11.978 2.478 7.613 16.990 0.032 0.024 5958.0 5431.0 5926.0 3021.0 1.0 beta[11] 16.996 2.566 11.788 21.388 0.035 0.025 5231.0 5069.0 5225.0 3268.0 1.0 beta[12] 6.763 2.528 1.527 11.016 0.035 0.026 5249.0 4626.0 5262.0 3325.0 1.0 beta[13] 12.659 2.508 7.613 17.073 0.033 0.024 5949.0 5483.0 5930.0 3211.0 1.0 beta[14] 10.677 2.513 5.861 15.251 0.035 0.026 5177.0 4781.0 5174.0 2931.0 1.0 beta[15] 15.619 2.486 10.922 20.275 0.035 0.025 5053.0 4965.0 5061.0 3350.0 1.0 beta[16] 8.896 2.500 4.289 13.568 0.034 0.024 5396.0 5396.0 5383.0 2782.0 1.0 beta[17] 10.719 2.473 6.254 15.402 0.033 0.024 5777.0 5384.0 5805.0 3029.0 1.0 tau_obs 0.002 0.000 0.001 0.002 0.000 0.000 3644.0 3601.0 3640.0 3035.0 1.0 In [21]: import statsmodels.formula.api as sm import seaborn as sns from matplotlib import gridspec ymin , ymax = np . min ( sleepstudy [ \"Reaction\" ]), np . max ( sleepstudy [ \"Reaction\" ]) plt . figure ( figsize = ( 11 , 8.5 )) gs = gridspec . GridSpec ( 3 , 6 ) gs . update ( wspace = 0.5 , hspace = 0.5 ) for i , subj in enumerate ( np . unique ( sleepstudy [ 'Subject' ])): ss_extract = sleepstudy . loc [ sleepstudy [ 'Subject' ] == subj ] ss_extract_ols = sm . ols ( formula = \"Reaction~Days\" , data = ss_extract ) . fit () #new subplot subplt = plt . subplot ( gs [ i ]) #plot without confidence intervals sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , data = ss_extract ) . set_title ( 'Subject ' + str ( subj )) if i not in [ 0 , 6 , 12 ]: plt . ylabel ( \"\" ) i += 1 subplt . set_ylim ( ymin , ymax ) _ = plt . figlegend ([ 'Estimated from each subject alone' ], loc = 'lower center' , ncol = 6 ) _ = plt . show () In [22]: plt . figure ( figsize = ( 11 , 8.5 )) for i , subj in enumerate ( np . unique ( sleepstudy [ 'Subject' ])): ss_extract = sleepstudy . loc [ sleepstudy [ 'Subject' ] == subj ] #new subplot subplt = plt . subplot ( gs [ i ]) #plot without confidence intervals sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , data = ss_extract ) . set_title ( 'Subject ' + str ( subj )) sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , scatter = False , data = sleepstudy ) if i not in [ 0 , 6 , 12 ]: plt . ylabel ( \"\" ) i += 1 subplt . set_ylim ( ymin , ymax ) _ = plt . figlegend ([ 'Estimated from each subject alone' , 'Pooling all subjects' ], loc = 'lower center' , ncol = 6 ) _ = plt . show () In [23]: plt . figure ( figsize = ( 11 , 8.5 )) subj_arr = np . unique ( sleepstudy [ 'Subject' ]) for i , subj in enumerate ( subj_arr ): ss_extract = sleepstudy . loc [ sleepstudy [ 'Subject' ] == subj ] #new subplot subplt = plt . subplot ( gs [ i ]) #plot without confidence intervals sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , data = ss_extract ) . set_title ( 'Subject ' + str ( subj )) sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , scatter = False , data = sleepstudy ) subj_num = int ( np . where ( subj_arr == subj )[ 0 ]) subjects_avg_intercept = np . mean ( trace [ 'alpha' ][:, i ]) subjects_avg_slope = np . mean ( trace [ 'beta' ][:, i ]) hmodel_fit = [ subjects_avg_intercept + subjects_avg_slope * x for x in range ( - 1 , 11 )] sns . lineplot ( x = range ( - 1 , 11 ), y = hmodel_fit ) if i not in [ 0 , 6 , 12 ]: plt . ylabel ( \"\" ) i += 1 subplt . set_ylim ( ymin , ymax ) _ = plt . figlegend ([ 'Estimated from each subject alone' , 'Pooling all subjects' , 'Hierarchical (partial pooling)' ], loc = 'lower center' , ncol = 6 ) _ = plt . show () In [24]: model_predictions = trace [ 'mu_i' ] . mean ( axis = 0 ) obs_reactions = sleepstudy [ 'Reaction' ] plt . figure ( figsize = ( 11 , 8.5 )) plt . scatter ( sleepstudy [ 'Reaction' ], model_predictions ) plt . plot ( plt . xlim (), plt . ylim (), c = 'black' ) plt . xlabel ( \"Observed Reaction Time (ms)\" ) plt . ylabel ( \"Predicted Reaction Time [Mean of Posterior] (ms)\" ) plt . title ( \"Observed and Fitted Reaction Times from . Bayesian Hierarchical Model\" ) plt . show ()",
        "tags": "Lectures",
        "url": "lectures/lecture07/notebook1/"
    }, {
        "title": "Lecture 7 - Bayesian statistics (part 1)",
        "text": "In [1]: import pymc3 as pm import arviz as az In [3]: import numpy as np % matplotlib inline import matplotlib.pyplot as plt import pandas as pd import scipy.stats as st import seaborn as sns import matplotlib.pyplot as plt In [28]: # Ignore a common pymc3 warning that comes from library functions, not our code. # Pymc3 may throw additional warnings, but other warnings should be manageable # by following the instructions included within the warning messages. import warnings messages = [ \"Using `from_pymc3` without the model will be deprecated in a future release\" , ] for m in messages : warnings . filterwarnings ( \"ignore\" , message = m ) In [4]: n_theta = 10000 # generate 10,000 values from Beta(2,5) theta = np . random . beta ( 2 , 5 , n_theta ) print ( \"First five values of theta: \\n\\t \" , theta [ 0 : 5 ]) print ( \"Sample mean: \\n\\t \" , np . mean ( theta )) print ( \"The 2.5 % a nd 97.5 % o f quantiles: \\n\\t \" , np . percentile ( theta ,[ 2.5 , 97.5 ])) First five values of theta: [0.47610973 0.17998453 0.11623631 0.4425017 0.1875366 ] Sample mean: 0.28306686085461086 The 2.5% and 97.5% of quantiles: [0.04394589 0.63105426] In [5]: plt . hist ( theta , 50 ) plt . xlabel ( \"Value of Theta\" ) plt . ylabel ( \"Count\" ) plt . show () In [6]: # simulate y from posterior predictive distribution y = np . random . binomial ( 1 , theta , n_theta ) # generate a heads/tails value from each of the 10,000 thetas print ( \"First 5 heads/tails values (tails=0, heads=1) \\n\\t \" , y [ 0 : 10 ]) print ( \"Overall frequency of Tails and Heads, accounting for uncertainty about theta itself \\n\\t \" , np . bincount ( y ) / 10000 ) plt . hist ( y , density = True ) plt . xticks ([ . 05 , . 95 ],[ \"Tails\" , \"Heads\" ]) plt . show () First 5 heads/tails values (tails=0, heads=1) [1 1 0 0 0 0 1 0 0 0] Overall frequency of Tails and Heads, accounting for uncertainty about theta itself [0.7098 0.2902] Rejection sampling and Weighted bootstrap Example adapted from https://wiseodd.github.io/techblog/2015/10/21/rejection-sampling/ In [2]: sns . set () def h ( x ): return st . norm . pdf ( x , loc = 30 , scale = 10 ) + st . norm . pdf ( x , loc = 80 , scale = 20 ) def g ( x ): return st . norm . pdf ( x , loc = 50 , scale = 30 ) x = np . arange ( - 50 , 151 ) M = max ( h ( x ) / g ( x )) # for rejection sampling h is a mixture of two normal distributions (unnormalized), and density h is a normal distribution with mean 50 and standard deviation 30. In [17]: plt . plot ( x , h ( x )) plt . show () In [18]: # Superimpose h and g on same plot plt . plot ( x , h ( x )) plt . plot ( x , g ( x )) plt . show () In [19]: # Superimpose h and M*g on same plot - now M*g envelopes h plt . plot ( x , h ( x )) plt . plot ( x , M * g ( x )) plt . show () In [5]: def rejection_sampling ( maxiter = 10000 , sampsize = 1000 ): samples = [] sampcount = 0 # counter for accepted samples maxcount = 0 # counter for proposal simulation # sampcount/maxcount at any point in the iteration is the acceptance rate while ( sampcount < sampsize and maxcount < maxiter ): z = np . random . normal ( 50 , 30 ) u = np . random . uniform ( 0 , 1 ) maxcount += 1 if u <= h ( z ) / ( M * g ( z )): samples . append ( z ) sampcount += 1 print ( 'Rejection rate is' , 100 * ( 1 - sampcount / maxcount )) if maxcount == maxiter : print ( 'Maximum iterations achieved' ) return np . array ( samples ) s = rejection_sampling ( maxiter = 10000 , sampsize = 1000 ) sns . displot ( s ) Rejection rate is 49.54591321897074 Out[5]: In [25]: # weighted bootstrap computation involving h and g import random def weighted_bootstrap ( iter = 1000 , size = 100 ): w = [] y = [] for i in range ( iter ): z = np . random . normal ( 50 , 30 ) y . append ( z ) wz = h ( z ) / g ( z ) w . append ( wz ) v = random . choices ( y , weights = w , k = size ) # do not need to renormalize w return np . array ( v ) wb = weighted_bootstrap ( iter = 10000 , size = 1000 ) sns . displot ( wb ) Out[25]: Beetles In [30]: beetles_x = np . array ([ 1.6907 , 1.7242 , 1.7552 , 1.7842 , 1.8113 , 1.8369 , 1.8610 , 1.8839 ]) beetles_x_mean = beetles_x - np . mean ( beetles_x ) beetles_n = np . array ([ 59 , 60 , 62 , 56 , 63 , 59 , 62 , 60 ]) beetles_y = np . array ([ 6 , 13 , 18 , 28 , 52 , 53 , 61 , 60 ]) beetles_N = np . array ([ 8 ] * 8 ) In [31]: from scipy.special import expit expit ( 2 ) Out[31]: 0.8807970779778823 In [32]: with pm . Model () as beetle_model : # The intercept (log probability of beetles dying when dose=0) # is centered at zero, and wide-ranging (easily anywhere from 0 to 100%) # If we wanted, we could choose something like Normal(-3,2) for a no-dose # death rate roughly between .007 and .25 alpha_star = pm . Normal ( 'alpha*' , mu = 0 , sigma = 100 ) # the effect on the log-odds of each unit of the dose is wide-ranging: # we're saying we've got little idea what the effect will be, and it could # be strongly negative. beta = pm . Normal ( 'beta' , mu = 0 , sigma = 100 ) # given alpha, beta, and the dosage, the probability of death is deterministic: # it's the inverse logit of the intercept+slope*dosage # Because beetles_x has 8 entries, we end up with 8 p_i values p_i = pm . Deterministic ( '$P_i$' , pm . math . invlogit ( alpha_star + beta * beetles_x_mean )) # finally, the number of bettles we see killed is Binomial(n=number of beetles, p=probability of death) deaths = pm . Binomial ( 'obs_deaths' , n = beetles_n , p = p_i , observed = beetles_y ) trace = pm . sample ( 2000 , tune = 2000 , target_accept = 0.9 ) /home/glickm/.local/lib/python3.9/site-packages/pymc3/sampling.py:465: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning. warnings.warn( Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta, alpha*] 100.00% [8000/8000 00:02<00:00 Sampling 2 chains, 0 divergences] Sampling 2 chains for 2_000 tune and 2_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds. In [33]: az . plot_trace ( trace , compact = False ); In [34]: def trace_summary ( trace , var_names = None ): if var_names is None : var_names = trace . varnames quants = [ 0.025 , 0.25 , 0.5 , 0.75 , 0.975 ] colnames = [ 'mean' , 'sd' , * [ \" {} %\" . format ( x * 100 ) for x in quants ]] rownames = [] series = [] for cur_var in var_names : var_trace = trace [ cur_var ] if var_trace . ndim == 1 : vals = [ np . mean ( var_trace , axis = 0 ), np . std ( var_trace , axis = 0 ), * np . quantile ( var_trace , quants , axis = 0 )] series . append ( pd . Series ( vals , colnames )) rownames . append ( cur_var ) else : for i in range ( var_trace . shape [ 1 ]): cur_col = var_trace [:, i ] vals = [ np . mean ( cur_col , axis = 0 ), np . std ( cur_col , axis = 0 ), * np . quantile ( cur_col , quants , axis = 0 )] series . append ( pd . Series ( vals , colnames )) rownames . append ( \" {} [ {} ]\" . format ( cur_var , i )) return pd . DataFrame ( series , index = rownames ) trace_summary ( trace ) Out[34]: mean sd 2.5% 25.0% 50.0% 75.0% 97.5% alpha* 0.748408 0.142126 0.484625 0.647875 0.746483 0.844844 1.035891 beta 34.598944 2.952582 29.160611 32.533871 34.531622 36.484915 40.660437 $P_i$[0] 0.059033 0.016087 0.032537 0.047387 0.057374 0.068892 0.094452 $P_i$[1] 0.163540 0.028339 0.111428 0.143686 0.162045 0.182056 0.220862 $P_i$[2] 0.361014 0.034651 0.294708 0.337156 0.360277 0.385165 0.429006 $P_i$[3] 0.605225 0.032339 0.543134 0.582635 0.605705 0.627732 0.668252 $P_i$[4] 0.795550 0.026790 0.744184 0.777235 0.796137 0.814087 0.846153 $P_i$[5] 0.903212 0.018882 0.864207 0.891043 0.904234 0.916307 0.937264 $P_i$[6] 0.954857 0.011829 0.929360 0.947438 0.955950 0.963315 0.974979 $P_i$[7] 0.978645 0.007007 0.962990 0.974396 0.979538 0.983683 0.989899 We can also plot the density each chain explored. Any major deviations between chains are signs of difficulty converging. In [35]: for x in trace . varnames : az . plot_forest ( trace , var_names = [ x ], combined = True ) In addition to the above summaries of the distribution, pymc3 has statistics intended to summarize the quality of the samples. The most common of these is r_hat, which measures whether the different chains seem to be exploring the same space or if they're stuck in different spaces. R-hat above 1.3 is a strong sign the sample isn't good yet. Values close to 1 are ideal. In [16]: az . summary ( trace ) /home/glickm/.local/lib/python3.9/site-packages/arviz/data/io_pymc3.py:87: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. warnings.warn( Out[16]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat alpha* 0.750 0.139 0.499 1.020 0.003 0.002 2840.0 2696.0 2867.0 2296.0 1.0 beta 34.626 2.963 28.628 39.807 0.055 0.039 2905.0 2841.0 2945.0 2388.0 1.0 $P_i$[0] 0.059 0.016 0.030 0.088 0.000 0.000 3520.0 3433.0 3458.0 2536.0 1.0 $P_i$[1] 0.163 0.028 0.111 0.214 0.000 0.000 3695.0 3675.0 3666.0 2439.0 1.0 $P_i$[2] 0.361 0.034 0.295 0.423 0.001 0.000 3701.0 3678.0 3714.0 3039.0 1.0 $P_i$[3] 0.606 0.031 0.550 0.666 0.001 0.000 3081.0 3035.0 3101.0 2536.0 1.0 $P_i$[4] 0.796 0.026 0.747 0.844 0.001 0.000 2712.0 2693.0 2677.0 2360.0 1.0 $P_i$[5] 0.903 0.019 0.868 0.938 0.000 0.000 2687.0 2680.0 2632.0 2055.0 1.0 $P_i$[6] 0.955 0.012 0.933 0.977 0.000 0.000 2722.0 2720.0 2653.0 2112.0 1.0 $P_i$[7] 0.979 0.007 0.966 0.991 0.000 0.000 2760.0 2759.0 2680.0 2122.0 1.0 Sleep Study In [17]: import pandas as pd sleepstudy = pd . read_csv ( \"sleepstudy.csv\" ) In [18]: sleepstudy Out[18]: Reaction Days Subject 0 249.5600 0 308 1 258.7047 1 308 2 250.8006 2 308 3 321.4398 3 308 4 356.8519 4 308 ... ... ... ... 175 329.6076 5 372 176 334.4818 6 372 177 343.2199 7 372 178 369.1417 8 372 179 364.1236 9 372 180 rows √ó 3 columns In [19]: # adding a column that numbers the subjects from 0 to n raw_ids = np . unique ( sleepstudy [ 'Subject' ]) raw2newid = { x : np . where ( raw_ids == x )[ 0 ][ 0 ] for x in raw_ids } sleepstudy [ 'SeqSubject' ] = sleepstudy [ 'Subject' ] . map ( raw2newid ) sleepstudy Out[19]: Reaction Days Subject SeqSubject 0 249.5600 0 308 0 1 258.7047 1 308 0 2 250.8006 2 308 0 3 321.4398 3 308 0 4 356.8519 4 308 0 ... ... ... ... ... 175 329.6076 5 372 17 176 334.4818 6 372 17 177 343.2199 7 372 17 178 369.1417 8 372 17 179 364.1236 9 372 17 180 rows √ó 4 columns In [20]: with pm . Model () as sleep_model : # In this model, we're going to say the alphas (individuals' intercepts; their starting reaction time) # and betas (individuals' slopes; how much worse they get with lack of sleep) are normally distributed. # We'll specify that we're certain about the mean of those distribution [more on that later], but admit # we're uncertain about how much spread there is (i.e. uncertain about the SD). Tau_alpha and Tau_beta # will be the respective SD. # # Of course, the SDs must be positive (negative SD isn't mathematically possible), so we draw them from # a Gamma, which cannot ever output negative numbers. Here, we use alpha and beta values that spread the # distribution: \"the SD could be anything!\". If we had more intuition (e.g. \"the starting reaction times can't # have SD above 3,000\") we would plot Gamma(a,b) and tune the parameters so that there was little mass # above 3,000, then use those values below) tau_alpha = pm . Gamma ( 'tau_alpha' , alpha =. 001 , beta =. 001 ) tau_beta = pm . Gamma ( 'tau_beta' , alpha =. 001 , beta =. 001 ) # Across the population of people, we suppose that # the slopes are normally distributed, as are the intercepts, # and the two are drawn independently # # (Here, we hard-code assumed means, but we don't have to. # In general, these should be set from our pre-data intuition, # rather than from plots/exploration of the data) alpha = pm . Normal ( 'alpha' , mu = 300 , tau = tau_alpha , shape = len ( raw_ids )) beta = pm . Normal ( 'beta' , mu = 10 , tau = tau_beta , shape = len ( raw_ids )) # Remember: there's only one alpha/beta per person, but # we have lots of observations per person. The below # builds a vector with one entry per observation, recording # the alpha/beta we want to use with that observation. # # That is, the length is 180, but it only has 17 unique values, # matching the 17 unique patients' personal slopes or intercepts intercepts = alpha [ sleepstudy [ 'SeqSubject' ]] slopes = beta [ sleepstudy [ 'SeqSubject' ]] # now we have the true/predicted response time for each observation (each row of original data) # (Here we use pm.Deterministic to signal that this is something we'll care about) mu_i = pm . Deterministic ( 'mu_i' , intercepts + slopes * sleepstudy [ 'Days' ]) # The _observed_ values are noisy versions of the hidden true values, however! # Specifically, we model them as a normal at the true value and single unknown variance # (one explanation: we're saying the measurement equipment adds normally-distributed noise tau_obs # so noise doesn't vary from observation to observation or person to person: there's just one universal # noise level) tau_obs = pm . Gamma ( 'tau_obs' , 0.001 , 0.001 ) obs = pm . Normal ( 'observed' , mu = mu_i , tau = tau_obs , observed = sleepstudy [ 'Reaction' ]) trace = pm . sample ( 2000 , tune = 2000 , target_accept = 0.9 ) /home/glickm/.local/lib/python3.9/site-packages/pymc3/sampling.py:465: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning. warnings.warn( Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [tau_obs, beta, alpha, tau_beta, tau_alpha] 100.00% [8000/8000 00:16<00:00 Sampling 2 chains, 0 divergences] Sampling 2 chains for 2_000 tune and 2_000 draw iterations (4_000 + 4_000 draws total) took 17 seconds. In [21]: # this command can take a few minutes to finish... or never :-/ #az.plot_trace(trace); In [22]: trace_summary ( trace , var_names = [ 'tau_alpha' , 'tau_beta' , 'alpha' , 'beta' , 'tau_obs' ]) Out[22]: mean sd 2.5% 25.0% 50.0% 75.0% 97.5% tau_alpha 0.000346 0.000126 0.000148 0.000256 0.000330 0.000415 0.000632 tau_beta 0.033659 0.015637 0.013079 0.023094 0.030559 0.040748 0.072732 alpha[0] 258.152009 14.085221 229.587865 248.875997 258.031798 267.548487 285.905580 alpha[1] 203.987112 13.798313 177.167592 194.562082 204.073271 213.510437 231.477768 alpha[2] 206.126645 13.890339 179.265453 196.916117 205.844053 215.249876 233.399891 alpha[3] 283.997308 13.540079 257.583370 274.934328 283.951485 293.016335 310.396503 alpha[4] 282.410892 13.518552 255.264925 273.224767 282.552675 291.504971 308.997292 alpha[5] 265.806415 13.531943 239.467840 256.855144 265.607351 274.862315 292.612168 alpha[6] 275.757596 13.320304 249.045762 267.021728 275.787887 284.376585 302.710662 alpha[7] 245.740413 13.714924 219.631293 236.229034 245.464725 255.177507 272.645132 alpha[8] 253.870482 14.255955 225.796467 244.251182 253.993258 263.769349 281.444838 alpha[9] 298.553345 13.755800 271.078080 289.243294 298.823119 307.431320 325.569036 alpha[10] 223.610381 13.990920 196.093509 214.008280 223.561530 233.307163 251.138863 alpha[11] 238.871549 13.761936 211.447031 229.581532 239.062261 247.913414 265.707026 alpha[12] 260.368873 13.413522 233.823679 251.571895 260.526191 269.315076 285.961425 alpha[13] 280.742010 13.829812 253.175439 271.743084 280.698178 289.996805 307.893626 alpha[14] 258.800724 13.492456 231.671886 249.901913 258.941095 267.936373 285.521491 alpha[15] 223.528867 14.165204 196.547061 213.777409 223.291463 233.232906 251.393429 alpha[16] 255.784792 13.460106 229.445291 246.648575 255.697083 265.101533 282.197640 alpha[17] 270.456634 13.558380 243.777977 261.344061 270.401545 279.587780 297.676718 beta[0] 18.880788 2.636258 13.763675 17.116352 18.826027 20.619523 24.186513 beta[1] 2.971997 2.571664 -2.007634 1.211468 2.980907 4.680537 8.058643 beta[2] 6.005203 2.555142 0.961511 4.326752 6.030397 7.745092 10.972043 beta[3] 4.308504 2.497274 -0.653887 2.619826 4.312722 5.983469 9.216847 beta[4] 6.111156 2.504019 1.258975 4.388857 6.113937 7.779817 10.971868 beta[5] 9.295519 2.463106 4.511339 7.626141 9.290686 10.936594 14.084005 beta[6] 9.101971 2.422311 4.292805 7.465481 9.102780 10.698237 13.893795 beta[7] 11.260015 2.500966 6.308763 9.555129 11.297954 12.939190 16.112974 beta[8] -0.646928 2.663938 -5.951725 -2.427896 -0.650508 1.184857 4.604859 beta[9] 17.165573 2.559193 12.221724 15.413945 17.140421 18.823903 22.378692 beta[10] 11.949868 2.529520 6.957955 10.273520 11.929817 13.635788 16.907556 beta[11] 16.909605 2.535119 11.934900 15.232328 16.884626 18.602441 21.954967 beta[12] 6.782651 2.460363 1.774711 5.181803 6.800000 8.406616 11.625047 beta[13] 12.685451 2.551037 7.689587 10.984800 12.725454 14.378128 17.749365 beta[14] 10.666371 2.469020 5.793505 9.070583 10.656603 12.269673 15.562084 beta[15] 15.553713 2.563791 10.643113 13.771380 15.569299 17.274128 20.549376 beta[16] 8.910774 2.469050 4.100091 7.209558 8.957572 10.572388 13.671468 beta[17] 10.708832 2.500048 5.846379 8.982458 10.748232 12.420740 15.558593 tau_obs 0.001517 0.000178 0.001191 0.001391 0.001511 0.001637 0.001882 In [23]: az . summary ( trace , var_names = [ 'tau_alpha' , 'tau_beta' , 'alpha' , 'beta' , 'tau_obs' ]) /home/glickm/.local/lib/python3.9/site-packages/arviz/data/io_pymc3.py:87: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. warnings.warn( Out[23]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat tau_alpha 0.000 0.000 0.000 0.001 0.000 0.000 5872.0 5400.0 5408.0 2903.0 1.0 tau_beta 0.034 0.016 0.011 0.061 0.000 0.000 2799.0 2560.0 3067.0 3254.0 1.0 alpha[0] 258.152 14.087 229.981 283.320 0.208 0.147 4598.0 4577.0 4615.0 2671.0 1.0 alpha[1] 203.987 13.800 178.345 230.169 0.237 0.167 3402.0 3402.0 3405.0 3394.0 1.0 alpha[2] 206.127 13.892 180.632 232.780 0.215 0.152 4179.0 4179.0 4146.0 3045.0 1.0 alpha[3] 283.997 13.542 259.688 310.220 0.199 0.141 4626.0 4626.0 4629.0 3100.0 1.0 alpha[4] 282.411 13.520 256.634 307.660 0.214 0.152 3982.0 3982.0 3961.0 3202.0 1.0 alpha[5] 265.806 13.534 242.135 293.297 0.208 0.147 4222.0 4216.0 4222.0 3289.0 1.0 alpha[6] 275.758 13.322 249.853 300.722 0.212 0.150 3949.0 3949.0 3924.0 3194.0 1.0 alpha[7] 245.740 13.717 220.770 271.740 0.215 0.152 4079.0 4079.0 4080.0 3263.0 1.0 alpha[8] 253.870 14.258 227.625 281.013 0.243 0.172 3438.0 3438.0 3437.0 3205.0 1.0 alpha[9] 298.553 13.758 272.172 324.239 0.243 0.172 3214.0 3211.0 3210.0 2818.0 1.0 alpha[10] 223.610 13.993 198.706 251.420 0.212 0.150 4359.0 4359.0 4353.0 2827.0 1.0 alpha[11] 238.872 13.764 211.660 263.609 0.228 0.161 3643.0 3642.0 3649.0 2780.0 1.0 alpha[12] 260.369 13.415 235.299 285.691 0.216 0.153 3868.0 3859.0 3872.0 3168.0 1.0 alpha[13] 280.742 13.832 254.178 305.953 0.223 0.157 3858.0 3858.0 3863.0 3249.0 1.0 alpha[14] 258.801 13.494 233.524 284.470 0.210 0.149 4111.0 4111.0 4126.0 2873.0 1.0 alpha[15] 223.529 14.167 196.548 249.253 0.228 0.162 3845.0 3831.0 3850.0 3142.0 1.0 alpha[16] 255.785 13.462 230.441 280.705 0.217 0.154 3846.0 3844.0 3846.0 2647.0 1.0 alpha[17] 270.457 13.560 243.626 294.911 0.212 0.150 4085.0 4085.0 4076.0 3205.0 1.0 beta[0] 18.881 2.637 13.858 23.717 0.042 0.030 3984.0 3973.0 3982.0 3262.0 1.0 beta[1] 2.972 2.572 -1.896 7.753 0.044 0.033 3481.0 3019.0 3469.0 3303.0 1.0 beta[2] 6.005 2.555 1.354 10.922 0.038 0.029 4423.0 4020.0 4425.0 3095.0 1.0 beta[3] 4.309 2.498 -0.366 9.042 0.038 0.030 4425.0 3469.0 4426.0 2886.0 1.0 beta[4] 6.111 2.504 1.495 10.792 0.041 0.030 3721.0 3538.0 3718.0 3168.0 1.0 beta[5] 9.296 2.463 4.766 14.010 0.038 0.028 4142.0 3784.0 4135.0 2781.0 1.0 beta[6] 9.102 2.423 4.659 13.710 0.037 0.028 4262.0 3842.0 4255.0 3053.0 1.0 beta[7] 11.260 2.501 6.322 15.730 0.039 0.028 4029.0 3908.0 4023.0 3078.0 1.0 beta[8] -0.647 2.664 -5.742 4.330 0.045 0.038 3478.0 2423.0 3472.0 3124.0 1.0 beta[9] 17.166 2.560 12.338 22.019 0.044 0.031 3436.0 3436.0 3432.0 3106.0 1.0 beta[10] 11.950 2.530 7.478 17.014 0.038 0.027 4385.0 4295.0 4385.0 2637.0 1.0 beta[11] 16.910 2.535 12.220 21.790 0.043 0.031 3493.0 3446.0 3502.0 2766.0 1.0 beta[12] 6.783 2.461 2.333 11.682 0.040 0.028 3878.0 3846.0 3875.0 3166.0 1.0 beta[13] 12.685 2.551 7.697 17.322 0.041 0.029 3883.0 3753.0 3885.0 3076.0 1.0 beta[14] 10.666 2.469 5.744 15.045 0.038 0.027 4124.0 4114.0 4121.0 3187.0 1.0 beta[15] 15.554 2.564 10.636 20.174 0.042 0.030 3726.0 3709.0 3714.0 3312.0 1.0 beta[16] 8.911 2.469 4.372 13.518 0.040 0.028 3833.0 3767.0 3865.0 2918.0 1.0 beta[17] 10.709 2.500 6.022 15.286 0.038 0.027 4260.0 4192.0 4266.0 3048.0 1.0 tau_obs 0.002 0.000 0.001 0.002 0.000 0.000 4092.0 4028.0 4113.0 3219.0 1.0 In [24]: import statsmodels.formula.api as sm import seaborn as sns from matplotlib import gridspec ymin , ymax = np . min ( sleepstudy [ \"Reaction\" ]), np . max ( sleepstudy [ \"Reaction\" ]) plt . figure ( figsize = ( 11 , 8.5 )) gs = gridspec . GridSpec ( 3 , 6 ) gs . update ( wspace = 0.5 , hspace = 0.5 ) for i , subj in enumerate ( np . unique ( sleepstudy [ 'Subject' ])): ss_extract = sleepstudy . loc [ sleepstudy [ 'Subject' ] == subj ] ss_extract_ols = sm . ols ( formula = \"Reaction~Days\" , data = ss_extract ) . fit () #new subplot subplt = plt . subplot ( gs [ i ]) #plot without confidence intervals sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , data = ss_extract ) . set_title ( 'Subject ' + str ( subj )) if i not in [ 0 , 6 , 12 ]: plt . ylabel ( \"\" ) i += 1 subplt . set_ylim ( ymin , ymax ) _ = plt . figlegend ([ 'Estimated from each subject alone' ], loc = 'lower center' , ncol = 6 ) _ = plt . show () In [25]: plt . figure ( figsize = ( 11 , 8.5 )) for i , subj in enumerate ( np . unique ( sleepstudy [ 'Subject' ])): ss_extract = sleepstudy . loc [ sleepstudy [ 'Subject' ] == subj ] #new subplot subplt = plt . subplot ( gs [ i ]) #plot without confidence intervals sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , data = ss_extract ) . set_title ( 'Subject ' + str ( subj )) sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , scatter = False , data = sleepstudy ) if i not in [ 0 , 6 , 12 ]: plt . ylabel ( \"\" ) i += 1 subplt . set_ylim ( ymin , ymax ) _ = plt . figlegend ([ 'Estimated from each subject alone' , 'Pooling all subjects' ], loc = 'lower center' , ncol = 6 ) _ = plt . show () In [26]: plt . figure ( figsize = ( 11 , 8.5 )) subj_arr = np . unique ( sleepstudy [ 'Subject' ]) for i , subj in enumerate ( subj_arr ): ss_extract = sleepstudy . loc [ sleepstudy [ 'Subject' ] == subj ] #new subplot subplt = plt . subplot ( gs [ i ]) #plot without confidence intervals sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , data = ss_extract ) . set_title ( 'Subject ' + str ( subj )) sns . regplot ( x = 'Days' , y = 'Reaction' , ci = None , scatter = False , data = sleepstudy ) subj_num = int ( np . where ( subj_arr == subj )[ 0 ]) subjects_avg_intercept = np . mean ( trace [ 'alpha' ][:, i ]) subjects_avg_slope = np . mean ( trace [ 'beta' ][:, i ]) hmodel_fit = [ subjects_avg_intercept + subjects_avg_slope * x for x in range ( - 1 , 11 )] sns . lineplot ( x = range ( - 1 , 11 ), y = hmodel_fit ) if i not in [ 0 , 6 , 12 ]: plt . ylabel ( \"\" ) i += 1 subplt . set_ylim ( ymin , ymax ) _ = plt . figlegend ([ 'Estimated from each subject alone' , 'Pooling all subjects' , 'Hierarchical (partial pooling)' ], loc = 'lower center' , ncol = 6 ) _ = plt . show () In [27]: model_predictions = trace [ 'mu_i' ] . mean ( axis = 0 ) obs_reactions = sleepstudy [ 'Reaction' ] plt . figure ( figsize = ( 11 , 8.5 )) plt . scatter ( sleepstudy [ 'Reaction' ], model_predictions ) plt . plot ( plt . xlim (), plt . ylim (), c = 'black' ) plt . xlabel ( \"Observed Reaction Time (ms)\" ) plt . ylabel ( \"Predicted Reaction Time [Mean of Posterior] (ms)\" ) plt . title ( \"Observed and Fitted Reaction Times from . Bayesian Hierarchical Model\" ) plt . show () In [ ]:",
        "tags": "Lectures",
        "url": "lectures/lecture07/notebook2/"
    }, {
        "title": "Lecture 6: Unsupervised learning cluster analysis (part 2)",
        "text": "Exercises Lecture 6 : Principal Components Analysis (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture06/"
    }, {
        "title": "Lecture 6 - Unsupervised learning, cluster analysis (part 2)",
        "text": "Title Principal Components Analysis Description : This exercise demonstrates the effect of scaling for Principal Components Analysis: After this exercise you should see following two plots: Hints: Principal Components Analysis Standard scaler Refer to lecture notebook. Do not change any other code except the blanks. In [1]: import pandas as pd import numpy as np from matplotlib import pyplot as plt from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler % matplotlib inline In [2]: df = pd . read_csv ( 'data2.csv' ) display ( df . describe ()) df . head () a1 a2 a3 a4 count 200.000000 200.000000 200.000000 200.000000 mean -0.857044 4.212110 0.031775 -0.021885 std 421.494274 420.739376 4.074328 4.183966 min -605.324722 -646.589819 -6.809775 -6.249051 25% -422.060527 -399.810929 -3.883741 -4.010144 50% 46.593553 40.746538 -0.333865 -0.071820 75% 405.738027 403.029485 3.981151 4.128475 max 618.733299 629.307897 6.430227 6.691714 Out[2]: a1 a2 a3 a4 0 343.952435 619.881035 3.926444 5.074012 1 376.982251 531.241298 2.831349 3.972653 2 555.870831 373.485494 3.365252 3.966670 3 407.050839 454.319406 3.971158 2.483932 4 412.928774 358.566005 4.670696 4.790385 In [0]: ### edTest(test_pca_noscaling) ### #Fit and Plot the first 2 principal components (no scaling) fitted_pca = PCA () . fit ( ____ ) pca_result = fitted_pca . transform ( ____ ) plt . scatter ( pca_result [:, 0 ], pca_result [:, 1 ]) plt . xlabel ( \"Principal Component 1\" ) plt . ylabel ( \"Principal Component 2\" ) plt . title ( \"PCA - No scaling\" ); In [0]: ### edTest(test_pca_scaled) ### #scale the data and plot first 2 principal components scaled_df = StandardScaler () . ____ fitted_pca = PCA () . fit ( ____ ) pca_result = fitted_pca . transform ( ____ ) plt . scatter ( pca_result [:, 0 ], pca_result [:, 1 ]) plt . xlabel ( \"Principal Component 1\" ) plt . ylabel ( \"Principal Component 2\" ) plt . title ( \"PCA - with scaled data\" ); In [0]:",
        "tags": "Lectures",
        "url": "lectures/lecture06/notebook/"
    }, {
        "title": "Lecture 5: Unsupervised learning cluster analysis (part 1)",
        "text": "Exercises Lecture 5: Clustering Example Notebook (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture05/"
    }, {
        "title": "Lecture 5.5 - pyGAM",
        "text": "Title Notebook A: pyGAM Description : You will work on the notebook outside of Ed. Download the notebook (using the >> in the Challenge) and image files (if any) to your local enviromnent. DO NOT RUN IN Ed. Run by choosing on of the options presented inside the notebook. CS109B Data Science 2: Advanced Topics in Data Science Lecture 5.5 - Smoothers and Generalized Additive Models - Model Fitting JUST A NOTEBOOK Harvard University Spring 2021 Instructors: Mark Glickman, Pavlos Protopapas, and Chris Tanner Lab Instructor: Eleni Kaxiras Content: Eleni Kaxiras and Will Claybaugh In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import numpy as np from scipy.interpolate import interp1d import matplotlib.pyplot as plt import pandas as pd % matplotlib inline Table of Contents 1 - Overview - A Top View of LMs, GLMs, and GAMs to set the stage 2 - A review of Linear Regression with statsmodels . Formulas. 3 - Splines 4 - Generative Additive Models with pyGAM 5 - Smooting Splines using csaps Overview image source: Dani Serv√©n Mar√≠n (one of the developers of pyGAM) A - Linear Models First we have the Linear Models which you know from 109a. These models are linear in the coefficients. Very interpretable but suffer from high bias because let's face it, few relationships in life are linear. Simple Linear Regression (defined as a model with one predictor) as well as Multiple Linear Regression (more than one predictors) are examples of LMs. Polynomial Regression extends the linear model by adding terms that are still linear for the coefficients but non-linear when it somes to the predictiors which are now raised in a power or multiplied between them. $$ \\begin{aligned} y = \\beta{_0} + \\beta{_1}{x_1} & \\quad \\mbox{(simple linear regression)}\\\\ y = \\beta{_0} + \\beta{_1}{x_1} + \\beta{_2}{x_2} + \\beta{_3}{x_3} & \\quad \\mbox{(multiple linear regression)}\\\\ y = \\beta{_0} + \\beta{_1}{x_1} + \\beta{_2}{x_1&#94;2} + \\beta{_3}{x_3&#94;3} & \\quad \\mbox{(polynomial multiple regression)}\\\\ \\end{aligned} $$ Questions to think about What does it mean for a model to be interpretable ? Are linear regression models interpretable? Are random forests? What about Neural Networks such as Feed Forward? Do we always want interpretability? Describe cases where we do and cases where we do not care. B - Generalized Linear Models (GLMs) Generalized Linear Models is a term coined in the early 1970s by Nelder and Wedderburn for a class of models that includes both Linear Regression and Logistic Regression. A GLM fits one coefficient per feature (predictor). C - Generalized Additive Models (GAMs) Hastie and Tidshirani coined the term Generalized Additive Models in 1986 for a class of non-linear extensions to Generalized Linear Models. $$ \\begin{aligned} y = \\beta{_0} + f_1\\left(x_1\\right) + f_2\\left(x_2\\right) + f_3\\left(x_3\\right) \\\\ y = \\beta{_0} + f_1\\left(x_1\\right) + f_2\\left(x_2, x_3\\right) + f_3\\left(x_3\\right) & \\mbox{(with interaction terms)} \\end{aligned} $$ In practice we add splines and regularization via smoothing penalties to our GLMs. image source: Dani Serv√©n Mar√≠n D - Basis Functions In our models we can use various types of functions as \"basis\". Monomials such as $x&#94;2$, $x&#94;4$ ( Polynomial Regression ) Sigmoid functions (neural networks) Fourier functions Wavelets Regression splines 1 - Piecewise Polynomials a.k.a. Splines Splines are a type of piecewise polynomial interpolant. A spline of degree k is a piecewise polynomial that is continuously differentiable k ‚àí 1 times. Splines are the basis of CAD software and vector graphics including a lot of the fonts used in your computer. The name \"spline\" comes from a tool used by ship designers to draw smooth curves. Here is the letter $epsilon$ written with splines: font idea inspired by Chris Rycroft (AM205) If the degree is 1 then we have a Linear Spline. If it is 3 then we have a Cubic spline. It turns out that cubic splines because they have a continous 2nd derivative (curvature) at the knots are very smooth to the eye. We do not need higher order than that. The Cubic Splines are usually Natural Cubic Splines which means they have the added constrain of the end points' second derivative = 0. We will use the CubicSpline and the B-Spline as well as the Linear Spline. scipy.interpolate See all the different splines that scipy.interpolate has to offer: https://docs.scipy.org/doc/scipy/reference/interpolate.html Let's use the simplest form which is interpolate on a set of points and then find the points between them. In [3]: from scipy.interpolate import splrep , splev from scipy.interpolate import BSpline , CubicSpline from scipy.interpolate import interp1d # define the range of the function a = - 1 b = 1 # define the number of knots num_knots = 11 knots = np . linspace ( a , b , num_knots ) # define the function we want to approximate y = 1 / ( 1 + 25 * ( knots ** 2 )) # make a linear spline linspline = interp1d ( knots , y ) # sample at these points to plot xx = np . linspace ( a , b , 1000 ) yy = 1 / ( 1 + 25 * ( xx ** 2 )) plt . plot ( knots , y , '*' ) plt . plot ( xx , yy , label = 'true function' ) plt . plot ( xx , linspline ( xx ), label = 'linear spline' ); plt . legend (); Out[3]: Exercise The Linear interpolation does not look very good. Fit a Cubic Spline and plot along the Linear to compare. Feel free to solve and then look at the solution. In [4]: # your answer here In [5]: # solution # define the range of the function a = - 1 b = 1 # define the knots num_knots = 10 x = np . linspace ( a , b , num_knots ) # define the function we want to approximate y = 1 / ( 1 + 25 * ( x ** 2 )) # make the Cubic spline cubspline = CubicSpline ( x , y ) print ( f 'Num knots in cubic spline: { num_knots } ' ) # OR make a linear spline linspline = interp1d ( x , y ) # plot xx = np . linspace ( a , b , 1000 ) yy = 1 / ( 1 + 25 * ( xx ** 2 )) plt . plot ( xx , yy , label = 'true function' ) plt . plot ( x , y , '*' , label = 'knots' ) plt . plot ( xx , linspline ( xx ), label = 'linear' ); plt . plot ( xx , cubspline ( xx ), label = 'cubic' ); plt . legend (); Num knots in cubic spline: 10 Out[5]: Questions to think about Change the number of knots to 100 and see what happens. What would happen if we run a polynomial model of degree equal to the number of knots (a global one as in polynomial regression, not a spline)? What makes a spline 'Natural'? In [6]: # Optional and Outside of the scope of this class: create the `epsilon` in the figure above x = np . array ([ 1. , 0. , - 1.5 , 0. , - 1.5 , 0. ]) y = np . array ([ 1.5 , 1. , 2.5 , 3 , 4 , 5 ]) t = np . linspace ( 0 , 5 , 6 ) f = interp1d ( t , x , kind = 'cubic' ) g = interp1d ( t , y , kind = 'cubic' ) tplot = np . linspace ( 0 , 5 , 200 ) plt . plot ( x , y , '*' , f ( tplot ), g ( tplot )); Out[6]: [ , ] B-Splines (de Boor, 1978) One way to construct a curve given a set of points is to interpolate the points , that is, to force the curve to pass through the points. A B-splines (Basis Splines) is defined by a set of control points and a set of basis functions that fit the function between these points. By choosing to have no smoothing factor we force the final B-spline to pass though all the points. If, on the other hand, we set a smothing factor, our function is more of an approximation with the control points as \"guidance\". The latter produced a smoother curve which is prefferable for drawing software. For more on Splines see: https://en.wikipedia.org/wiki/B-spline ) We will use scipy.splrep to calulate the coefficients for the B-Spline and draw it. B-Spline with no smooting In [7]: from scipy.interpolate import splev , splrep x = np . linspace ( 0 , 10 , 10 ) y = np . sin ( x ) # (t,c,k) is a tuple containing the vector of knots, coefficients, degree of the spline t , c , k = splrep ( x , y ) x2 = np . linspace ( 0 , 10 , 200 ) y2 = BSpline ( t , c , k ) plt . plot ( x , y , 'o' , x2 , y2 ( x2 )) plt . show () In [8]: from scipy.interpolate import splrep x = np . linspace ( 0 , 10 , 10 ) y = np . sin ( x ) t , c , k = splrep ( x , y , k = 3 ) # (tck) is a tuple containing the vector of knots, coefficients, degree of the spline # define the points to plot on (x2) print ( f 'Knots ( { len ( t ) } of them): { t } \\n ' ) print ( f 'B-Spline coefficients ( { len ( c ) } of them): { c } \\n ' ) print ( f 'B-Spline degree { k } ' ) x2 = np . linspace ( 0 , 10 , 100 ) y2 = BSpline ( t , c , k ) plt . figure ( figsize = ( 10 , 5 )) plt . plot ( x , y , 'o' , label = 'true points' ) plt . plot ( x2 , y2 ( x2 ), label = 'B-Spline' ) tt = np . zeros ( len ( t )) plt . plot ( t , tt , 'g*' , label = 'knots eval by the function' ) plt . legend () plt . show () Knots (14 of them): [ 0. 0. 0. 0. 2.22222222 3.33333333 4.44444444 5.55555556 6.66666667 7.77777778 10. 10. 10. 10. ] B-Spline coefficients (14 of them): [-4.94881722e-18 8.96543619e-01 1.39407154e+00 -2.36640266e-01 -1.18324030e+00 -8.16301228e-01 4.57836125e-01 1.48720677e+00 1.64338775e-01 -5.44021111e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00] B-Spline degree 3 What do the tuple values returned by scipy.splrep mean? The t variable is the array that contains the knots' position in the x axis. The length of this array is, of course, the number of knots. The c variable is the array that holds the coefficients for the B-Spline. Its length should be the same as t . We have number_of_knots - 1 B-spline basis elements to the spline constructed via this method, and they are defined as follows: $$ \\begin{aligned} B_{i, 0}(x) = 1, \\textrm{if $t i \\le x < t {i+1}$, otherwise $0$,} \\ \\ B_{i, k}(x) = \\frac{x - t i}{t {i+k} - t i} B {i, k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1}(x) \\end{aligned} $$ t $\\in [t_1, t_2, ..., t_]$ is the knot vector c : are the spline coefficients k : is the spline degree B-Spline with smooting factor s In [9]: from scipy.interpolate import splev , splrep x = np . linspace ( 0 , 10 , 5 ) y = np . sin ( x ) s = 0.5 # add smoothing factor task = 0 # task needs to be set to 0, which represents: # we are specifying a smoothing factor and thus only want # splrep() to find the optimal t and c t , c , k = splrep ( x , y , task = task , s = s ) # draw the line segments linspline = interp1d ( x , y ) # define the points to plot on (x2) x2 = np . linspace ( 0 , 10 , 200 ) y2 = BSpline ( t , c , k ) plt . plot ( x , y , 'o' , x2 , y2 ( x2 )) plt . plot ( x2 , linspline ( x2 )) plt . show () B-Spline with given knots In [10]: x = np . linspace ( 0 , 10 , 100 ) y = np . sin ( x ) knots = np . quantile ( x , [ 0.25 , 0.5 , 0.75 ]) print ( knots ) [2.5 5. 7.5] In [11]: # calculate the B-Spline t , c , k = splrep ( x , y , t = knots ) In [12]: curve = BSpline ( t , c , k ) curve Out[12]: In [13]: plt . scatter ( x = x , y = y , c = 'grey' , alpha = 0.4 ) yknots = np . sin ( knots ) plt . scatter ( knots , yknots , c = 'r' ) plt . plot ( x , curve ( x )) plt . show () 2 - GAMs https://readthedocs.org/projects/pygam/downloads/pdf/latest/ Classification in pyGAM Let's get our (multivariate!) data, the kyphosis dataset, and the LogisticGAM model from pyGAM to do binary classification. kyphosis - wherther a particular deformation was present post-operation age - patient's age in months number - the number of vertebrae involved in the operation start - the number of the topmost vertebrae operated on In [14]: kyphosis = pd . read_csv ( \"data/kyphosis.csv\" ) display ( kyphosis . head ()) display ( kyphosis . describe ( include = 'all' )) display ( kyphosis . dtypes ) Kyphosis Age Number Start 0 absent 71 3 5 1 absent 158 3 14 2 present 128 4 5 3 absent 2 5 1 4 absent 1 4 15 Kyphosis Age Number Start count 81 81.000000 81.000000 81.000000 unique 2 NaN NaN NaN top absent NaN NaN NaN freq 64 NaN NaN NaN mean NaN 83.654321 4.049383 11.493827 std NaN 58.104251 1.619423 4.883962 min NaN 1.000000 2.000000 1.000000 25% NaN 26.000000 3.000000 9.000000 50% NaN 87.000000 4.000000 13.000000 75% NaN 130.000000 5.000000 16.000000 max NaN 206.000000 10.000000 18.000000 Kyphosis object Age int64 Number int64 Start int64 dtype: object In [15]: # convert the outcome in a binary form, 1 or 0 kyphosis = pd . read_csv ( \"data/kyphosis.csv\" ) kyphosis [ \"outcome\" ] = 1 * ( kyphosis [ \"Kyphosis\" ] == \"present\" ) kyphosis . describe () Out[15]: Age Number Start outcome count 81.000000 81.000000 81.000000 81.000000 mean 83.654321 4.049383 11.493827 0.209877 std 58.104251 1.619423 4.883962 0.409758 min 1.000000 2.000000 1.000000 0.000000 25% 26.000000 3.000000 9.000000 0.000000 50% 87.000000 4.000000 13.000000 0.000000 75% 130.000000 5.000000 16.000000 0.000000 max 206.000000 10.000000 18.000000 1.000000 In [0]: from pygam import LogisticGAM , s , f , l X = kyphosis [[ \"Age\" , \"Number\" , \"Start\" ]] y = kyphosis [ \"outcome\" ] kyph_gam = LogisticGAM () . fit ( X , y ) Outcome dependence on features To help us see how the outcome depends on each feature, pyGAM has the partial_dependence() function. pdep, confi = kyph_gam.partial_dependence(term=i, X=XX, width=0.95) For more on this see the : https://pygam.readthedocs.io/en/latest/api/logisticgam.html In [21]: res = kyph_gam . deviance_residuals ( X , y ) for i , term in enumerate ( kyph_gam . terms ): if term . isintercept : continue XX = kyph_gam . generate_X_grid ( term = i ) pdep , confi = kyph_gam . partial_dependence ( term = i , X = XX , width = 0.95 ) pdep2 , _ = kyph_gam . partial_dependence ( term = i , X = X , width = 0.95 ) plt . figure () plt . scatter ( X . iloc [:, term . feature ], pdep2 + res ) plt . plot ( XX [:, term . feature ], pdep ) plt . plot ( XX [:, term . feature ], confi , c = 'r' , ls = '--' ) plt . title ( X . columns . values [ term . feature ]) plt . show () Notice that we did not specify the basis functions in the .fit(). pyGAM figures them out for us by using $s()$ (splines) for numerical variables and $f()$ for categorical features. If this is not what we want we can manually specify the basis functions, as follows: In [22]: kyph_gam = LogisticGAM ( s ( 0 ) + s ( 1 ) + s ( 2 )) . fit ( X , y ) In [23]: res = kyph_gam . deviance_residuals ( X , y ) for i , term in enumerate ( kyph_gam . terms ): if term . isintercept : continue XX = kyph_gam . generate_X_grid ( term = i ) pdep , confi = kyph_gam . partial_dependence ( term = i , X = XX , width = 0.95 ) pdep2 , _ = kyph_gam . partial_dependence ( term = i , X = X , width = 0.95 ) plt . figure () plt . scatter ( X . iloc [:, term . feature ], pdep2 + res ) plt . plot ( XX [:, term . feature ], pdep ) plt . plot ( XX [:, term . feature ], confi , c = 'r' , ls = '--' ) plt . title ( X . columns . values [ term . feature ]) plt . show () Regression in pyGAM For regression problems, we can use a linearGAM model. For this part we will use the wages dataset. https://pygam.readthedocs.io/en/latest/api/lineargam.html The wages dataset Let's inspect another dataset that is included in pyGAM that notes the wages of people based on their age, year of employment and education. In [24]: # from the pyGAM documentation from pygam import LinearGAM , s , f from pygam.datasets import wage X , y = wage ( return_X_y = True ) ## model gam = LinearGAM ( s ( 0 ) + s ( 1 ) + f ( 2 )) gam . gridsearch ( X , y ) ## plotting plt . figure (); fig , axs = plt . subplots ( 1 , 3 ); titles = [ 'year' , 'age' , 'education' ] for i , ax in enumerate ( axs ): XX = gam . generate_X_grid ( term = i ) ax . plot ( XX [:, i ], gam . partial_dependence ( term = i , X = XX )) ax . plot ( XX [:, i ], gam . partial_dependence ( term = i , X = XX , width =. 95 )[ 1 ], c = 'r' , ls = '--' ) if i == 0 : ax . set_ylim ( - 30 , 30 ) ax . set_title ( titles [ i ]); 100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00 3 - Smoothing Splines using csaps Note : this is the spline model that minimizes $MSE - \\lambda\\cdot\\text{wiggle penalty}$ $=$ $\\sum_{i=1}&#94;N \\left(y_i - f(x_i)\\right)&#94;2 - \\lambda \\int \\left(f''(t)\\right)&#94;2 dt$, across all possible functions $f$. For csaps : a) input data should be strictly increasing, so no duplicate values (see https://csaps.readthedocs.io/en/latest/api.html ). You need to sort the values in an ascending order with no duplicates. b) the smoothing parameter is entered as $\\lambda$ (or p) but it seems to be $1-\\lambda$ (1-p) in the formula. For p=0 we get a straight line, for p=1 there is no smoothing (overfit). In [26]: from csaps import csaps np . random . seed ( 1234 ) x = np . linspace ( 0 , 10 , 300000 ) y = np . sin ( x * 2 * np . pi ) * x + np . random . randn ( len ( x )) xs = np . linspace ( x [ 0 ], x [ - 1 ], 1000 ) ys = csaps ( x , y , xs , smooth = 0.99 ) print ( ys . shape ) #plt.plot(x, y, 'o', xs, ys, '-') plt . plot ( x , y , 'o' , xs , ys , '-' ) plt . show () (1000,) 4 - Data fitting using pyGAM and Penalized B-Splines When we use a spline in pyGAM we are effectively using a penalized B-Spline with a regularization parameter $\\lambda$. E.g. LogisticGAM(s(0)+s(1, lam=0.5)+s(2)).fit(X,y) Let's see how this smoothing works in pyGAM . We start by creating some arbitrary data and fitting them with a GAM. In [27]: X = np . linspace ( 0 , 10 , 500 ) y = np . sin ( X * 2 * np . pi ) * X + np . random . randn ( len ( X )) plt . scatter ( X , y ); In [28]: # let's try a large lambda first and lots of splines gam = LinearGAM ( lam = 1e6 , n_splines = 50 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX )); We see that the large $\\lambda$ forces a straight line, no flexibility. Let's see now what happens if we make it smaller. In [29]: # let's try a smaller lambda gam = LinearGAM ( lam = 1e2 , n_splines = 50 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX )); There is some curvature there but still not a good fit. Let's try no penalty. That should have the line fit exactly. In [30]: # no penalty, let's try a 0 lambda gam = LinearGAM ( lam = 0 , n_splines = 50 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ) plt . plot ( XX , gam . predict ( XX )) Out[30]: [ ] Yes, that is good. Now let's see what happens if we lessen the number of splines. The fit should not be as good. In [31]: # no penalty, let's try a 0 lambda gam = LinearGAM ( lam = 0 , n_splines = 10 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX ));",
        "tags": "Lectures",
        "url": "lectures/lecture05.5/notebook1/"
    }, {
        "title": "Lecture 5.5 - pyGAM with some additions",
        "text": "Title pyGAM with some additions Description : Download and run in own environment. This notebook is for the most part exactly the same as Notebook A , with a a few additions. Those additions are clearly marked in the notebook. CS109B Data Science 2: Advanced Topics in Data Science Lecture 5.5 - Smoothers and Generalized Additive Models - Model Fitting JUST A NOTEBOOK + SOME ADDITIONS (see New Material) Harvard University Spring 2021 Instructors: Mark Glickman, Pavlos Protopapas, and Chris Tanner Lab Instructor: Eleni Kaxiras Content: Eleni Kaxiras and Will Claybaugh New material here: References info on B-spline basis functions Cleaner formula for csaps Also cleaner Table of Contents In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import numpy as np from scipy.interpolate import interp1d import matplotlib.pyplot as plt import pandas as pd % matplotlib inline Table of Contents 1 - Overview - A Top View of LMs, GLMs, and GAMs to set the stage 2 - Splines 3 - Generative Additive Models with pyGAM 4 - Smooting Splines using csaps 5 - Penalized B-splines 1 - Overview image source: Dani Serv√©n Mar√≠n (one of the developers of pyGAM) A - Linear Models First we have the Linear Models which you know from 109a. These models are linear in the coefficients. Very interpretable but suffer from high bias because let's face it, few relationships in life are linear. Simple Linear Regression (defined as a model with one predictor) as well as Multiple Linear Regression (more than one predictors) are examples of LMs. Polynomial Regression extends the linear model by adding terms that are still linear for the coefficients but non-linear when it somes to the predictiors which are now raised in a power or multiplied between them. $$ \\begin{aligned} y = \\beta{_0} + \\beta{_1}{x_1} & \\quad \\mbox{(simple linear regression)}\\\\ y = \\beta{_0} + \\beta{_1}{x_1} + \\beta{_2}{x_2} + \\beta{_3}{x_3} & \\quad \\mbox{(multiple linear regression)}\\\\ y = \\beta{_0} + \\beta{_1}{x_1} + \\beta{_2}{x_1&#94;2} + \\beta{_3}{x_3&#94;3} & \\quad \\mbox{(polynomial multiple regression)}\\\\ \\end{aligned} $$ Questions to think about What does it mean for a model to be interpretable ? Are linear regression models interpretable? Are random forests? What about Neural Networks such as Feed Forward? Do we always want interpretability? Describe cases where we do and cases where we do not care. B - Generalized Linear Models (GLMs) Generalized Linear Models is a term coined in the early 1970s by Nelder and Wedderburn for a class of models that includes both Linear Regression and Logistic Regression. A GLM fits one coefficient per feature (predictor). C - Generalized Additive Models (GAMs) Hastie and Tidshirani coined the term Generalized Additive Models in 1986 for a class of non-linear extensions to Generalized Linear Models. $$ \\begin{aligned} y = \\beta{_0} + f_1\\left(x_1\\right) + f_2\\left(x_2\\right) + f_3\\left(x_3\\right) \\\\ y = \\beta{_0} + f_1\\left(x_1\\right) + f_2\\left(x_2, x_3\\right) + f_3\\left(x_3\\right) & \\mbox{(with interaction terms)} \\end{aligned} $$ In practice we add splines and regularization via smoothing penalties to our GLMs. image source: Dani Serv√©n Mar√≠n D - Basis Functions In our models we can use various types of functions as \"basis\". Monomials such as $x&#94;2$, $x&#94;4$ ( Polynomial Regression ) Sigmoid functions (neural networks) Fourier functions Wavelets Regression splines Smoothing splines 2 - Piecewise Polynomials a.k.a. Splines Splines are a type of piecewise polynomial interpolant. A spline of degree k is a piecewise polynomial that is continuously differentiable k ‚àí 1 times. Splines are the basis of CAD software and vector graphics including a lot of the fonts used in your computer. The name \"spline\" comes from a tool used by ship designers to draw smooth curves. Here is the letter $epsilon$ written with splines: font idea inspired by Chris Rycroft (AM205) If the degree is 1 then we have a Linear Spline. If it is 3 then we have a Cubic spline. It turns out that cubic splines because they have a continous 2nd derivative (curvature) at the knots are very smooth to the eye. We do not need higher order than that. The Cubic Splines are usually Natural Cubic Splines which means they have the added constrain of the end points' second derivative = 0. We will use the CubicSpline and the B-Spline as well as the Linear Spline. scipy.interpolate See all the different splines that scipy.interpolate has to offer: https://docs.scipy.org/doc/scipy/reference/interpolate.html . These routines are based on the FORTRAN library FITPACK written in the '70s. Let's use the simplest form which is interpolate on a set of points and then find the points between them. In [3]: from scipy.interpolate import splrep , splev from scipy.interpolate import BSpline , CubicSpline from scipy.interpolate import interp1d # define the range of the function a = - 1 b = 1 # define the number of knots num_knots = 11 # define the knots as equally spaced points knots = np . linspace ( a , b , num_knots ) # define the function we want to approximate y = 1 / ( 1 + 25 * ( knots ** 2 )) # make a linear spline linspline = interp1d ( knots , y ) # sample at these points to plot xx = np . linspace ( a , b , 1000 ) yy = 1 / ( 1 + 25 * ( xx ** 2 )) plt . plot ( knots , y , '*' ) plt . plot ( xx , yy , label = 'true function' ) plt . plot ( xx , linspline ( xx ), label = 'linear spline' ); plt . legend (); Exercise The Linear interpolation does not look very good. Fit a Cubic Spline and plot along the Linear to compare. Feel free to solve and then look at the solution. In [4]: # your answer here In [5]: # solution # define the range of the function a = - 1 b = 1 # define the knots num_knots = 10 x = np . linspace ( a , b , num_knots ) # define the function we want to approximate y = 1 / ( 1 + 25 * ( x ** 2 )) # make the Cubic spline cubspline = CubicSpline ( x , y ) print ( f 'Num knots in cubic spline: { num_knots } ' ) # OR make a linear spline linspline = interp1d ( x , y ) # plot xx = np . linspace ( a , b , 10000 ) yy = 1 / ( 1 + 25 * ( xx ** 2 )) plt . plot ( xx , yy , label = 'true function' ) plt . plot ( x , y , '*' , label = 'knots' ) plt . plot ( xx , linspline ( xx ), label = 'linear' ); plt . plot ( xx , cubspline ( xx ), label = 'cubic' ); plt . legend (); Num knots in cubic spline: 10 Questions to think about Change the number of knots to 100 and see what happens. What would happen if we run a polynomial model of degree equal to the number of knots (a global one as in polynomial regression, not a spline)? What makes a spline 'Natural'? In [6]: # Optional and Outside of the scope of this class: create the `epsilon` in the figure above x = np . array ([ 1. , 0. , - 1.5 , 0. , - 1.5 , 0. ]) y = np . array ([ 1.5 , 1. , 2.5 , 3 , 4 , 5 ]) t = np . linspace ( 0 , 5 , 6 ) f = interp1d ( t , x , kind = 'cubic' ) g = interp1d ( t , y , kind = 'cubic' ) tplot = np . linspace ( 0 , 5 , 200 ) plt . plot ( x , y , '*' , f ( tplot ), g ( tplot )); B-Splines (Curry and Schoenberg(1966) , further study by de Boor, 1978) One way to construct a curve given a set of points is to interpolate the points , that is, to force the curve to pass through the points. A B-splines (Basis Splines) is defined by a set of control points and a set of basis functions that fit the function between these points. By choosing to have no smoothing factor we force the final B-spline to pass though all the points. If, on the other hand, we set a smothing factor, our function is more of an approximation with the control points as \"guidance\". The latter produced a smoother curve which is prefferable for drawing software. For more on Splines see: https://en.wikipedia.org/wiki/B-spline ) or for a deeper look: De Boor, C. (1978). A practical guide to splines (Applied mathematical sciences (Springer-Verlag New York Inc.) ; v. 27). New York: Springer-Verlag . We will use scipy.splrep to calulate the coefficients for the B-Spline and draw it. B-Spline with no smooting In [7]: 0.1e+01 Out[7]: 1.0 In [8]: from scipy.interpolate import splev , splrep x = np . linspace ( 0 , 10 , 10 ) y = np . sin ( x ) # (t,c,k) is a tuple containing the vector of knots, coefficients, degree of the spline t , c , k = splrep ( x , y ) x2 = np . linspace ( 0 , 10 , 200 ) y2 = BSpline ( t , c , k ) plt . plot ( x , y , 'o' , x2 , y2 ( x2 )) plt . plot ( t , np . zeros ( len ( t )), '*' , label = 'knots' ) plt . legend () plt . show () In [9]: len ( x ), t , len ( t ), len ( c ), k Out[9]: (10, array([ 0. , 0. , 0. , 0. , 2.22222222, 3.33333333, 4.44444444, 5.55555556, 6.66666667, 7.77777778, 10. , 10. , 10. , 10. ]), 14, 14, 3) In [10]: from scipy.interpolate import splrep x = np . linspace ( 0 , 10 , 10 ) y = np . sin ( x ) # (tck) is a tuple containing the vector of knots, # coefficients, degree of the spline t , c , k = splrep ( x , y , k = 3 ) # define the points to plot on (x2) print ( f 'Knots ( { len ( t ) } of them): { t } \\n ' ) print ( f 'B-Spline coefficients ( { len ( c ) } of them): { c } \\n ' ) print ( f 'B-Spline degree { k } ' ) x2 = np . linspace ( 0 , 10 , 100 ) y2 = BSpline ( t , c , k ) plt . figure ( figsize = ( 10 , 5 )) plt . plot ( x , y , 'o' , label = 'true points' ) plt . plot ( x2 , y2 ( x2 ), label = 'B-Spline' ) tt = np . zeros ( len ( t )) plt . plot ( t , tt , 'g*' , label = 'knots evaluated by the function' ) plt . legend () plt . show () Knots (14 of them): [ 0. 0. 0. 0. 2.22222222 3.33333333 4.44444444 5.55555556 6.66666667 7.77777778 10. 10. 10. 10. ] B-Spline coefficients (14 of them): [-4.94881722e-18 8.96543619e-01 1.39407154e+00 -2.36640266e-01 -1.18324030e+00 -8.16301228e-01 4.57836125e-01 1.48720677e+00 1.64338775e-01 -5.44021111e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00] B-Spline degree 3 What do the tuple values returned by scipy.splrep mean? The t variable is the array that contains the knots' position in the x axis. The length of this array is, of course, the number of knots. The c variable is the array that holds the coefficients for the B-Spline. Its length should be the same as t . We have number_of_knots - 1 B-spline basis elements to the spline constructed via this method, and they are defined as follows: $$ \\begin{aligned} B_{i, 0}(x) = 1, \\textrm{if $t i \\le x < t {i+1}$, otherwise $0$,} \\ \\ B_{i, k}(x) = \\frac{x - t i}{t {i+k} - t i} B {i, k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1}(x) \\end{aligned} $$ t $\\in [t_1, t_2, ..., t_n]$ is the knot vector c : are the spline coefficients k : is the spline degree References: All you wanted to know about B-Splines and were afraid to ask, or how scipy.splrep calculates B-spline representations. For more on the basis of B-splines . scipy.interpolate.BSpline.basis_element . Inside the documentation you will find references to books. In [11]: from scipy.interpolate import splev , splrep from scipy.interpolate import BSpline Constructing the B-spline basis elements The degree of the B-spline, k , is inferred from the length of t as len(t)-2 . The knot vector is constructed by appending and prepending k+1 elements to internal knots t . a) The first element is a B-spline of degree $d=0$ with $1$ knot (min number of knots = $d+2$) $(d=0), B_{i, 0}(x) = 1, \\textrm{if $t i \\le x < t {i+1}$, otherwise $0$, (0 interior knots, 2 total)}$ b) The second element is a B-spline of degree $d=1$ with $2$ knots $(d=1), B_{i, 0}(x) = 1, \\textrm{if $t i \\le x < t {i+1}$, otherwise $0$, (1 interior knot, 3 total)}$ and so on ... In [12]: b0 = BSpline . basis_element ([ 0 , 1 ]) # two knots are the boundary nodes with 0 internal knots b1 = BSpline . basis_element ([ 0 , 1 , 2 ]) b2 = BSpline . basis_element ([ 0 , 1 , 2 , 3 ]) b3 = BSpline . basis_element ([ 0 , 1 , 2 , 3 , 4 ]) print ( f 'Knots for b3 = { b3 . t [ 3 : - 3 ] } ' ) # B-splines add knots before and after the boundaries for better construction. print ( f 'Augmented knots for b3 = { b3 . t } ' ) fig , ax = plt . subplots () x0 = np . linspace ( 0 , 1 , 50 ) x1 = np . linspace ( 0 , 2 , 50 ) x2 = np . linspace ( 0 , 3 , 50 ) x3 = np . linspace ( 0 , 4 , 50 ) ax . set_xlim ( 0 , 5 ); ax . set_title ( f 'degree of basis : { b0 . k } ' ) ax . plot ( x0 , b0 ( x0 ), 'g' , label = f 'B-element d= { b0 . k } ' , lw = 3 ); ax . plot ( x1 , b1 ( x1 ), 'brown' , label = f 'B-element d= { b1 . k } ' , lw = 3 ); ax . plot ( x2 , b2 ( x2 ), 'black' , label = f 'B-element d= { b2 . k } ' , lw = 3 ); ax . plot ( x3 , b3 ( x3 ), 'blue' , label = f 'B-element d= { b3 . k } ' , lw = 3 ); ax . legend (); Knots for b3 = [0. 1. 2. 3. 4.] Augmented knots for b3 = [-1. -1. -1. 0. 1. 2. 3. 4. 5. 5. 5.] B-Spline with smooting factor s In [13]: x = np . linspace ( 0 , 10 , 5 ) y = np . sin ( x ) s = 0.1 # add smoothing factor # task needs to be set to 0, which represents: # we are specifying a smoothing factor and thus only want # splrep() to find the optimal t and c task = 0 t , c , k = splrep ( x , y , task = task , s = s ) print ( f 'Knots ( { len ( t ) } ): { t } , { k } ' ) Knots (9): [ 0. 0. 0. 0. 5. 10. 10. 10. 10.], 3 In [14]: # draw the line segments linspline = interp1d ( x , y ) # define the points to plot on (x2) x2 = np . linspace ( 0 , 10 , 200 ) y2 = BSpline ( t , c , k ) plt . plot ( x , y , 'o' , x2 , y2 ( x2 ), label = 'data' ) plt . plot ( x2 , linspline ( x2 ), label = 'linear interpolation' ) #+0.2 is pertubating the line for visibility plt . plot ( x2 , y2 ( x2 ) + 0.2 , 'blue' , label = 'B-spline(moved)' ) plt . legend () plt . show () B-Spline with given knots In [15]: x = np . linspace ( 0 , 10 , 100 ) y = np . sin ( x ) # ‚Äòquantile' knot sequence: the interior knots are the quantiles from the empirical # distribution of the underlying variable. Quantile knots guarantee that each # interval contains an equal number of sample observations knots = np . quantile ( x , [ 0.25 , 0.5 , 0.75 ]) print ( knots ) [2.5 5. 7.5] In [16]: # calculate the B-Spline t , c , k = splrep ( x , y , t = knots ) In [17]: t Out[17]: array([ 0. , 0. , 0. , 0. , 2.5, 5. , 7.5, 10. , 10. , 10. , 10. ]) In [18]: curve = BSpline ( t , c , k ) curve Out[18]: In [19]: plt . scatter ( x = x , y = y , c = 'grey' , alpha = 0.4 ) yknots = np . sin ( knots ) #plt.scatter(knots, yknots, c='r') plt . scatter ( knots , np . zeros ( len ( yknots )), c = 'r' ) plt . plot ( x , curve ( x )) plt . show () 3 - GAMs https://readthedocs.org/projects/pygam/downloads/pdf/latest/ Classification in pyGAM Let's get our (multivariate!) data, the kyphosis dataset, and the LogisticGAM model from pyGAM to do binary classification. kyphosis - wherther a particular deformation was present post-operation age - patient's age in months number - the number of vertebrae involved in the operation start - the number of the topmost vertebrae operated on In [20]: kyphosis = pd . read_csv ( \"../data/kyphosis.csv\" ) display ( kyphosis . head ()) display ( kyphosis . describe ( include = 'all' )) display ( kyphosis . dtypes ) Kyphosis Age Number Start 0 absent 71 3 5 1 absent 158 3 14 2 present 128 4 5 3 absent 2 5 1 4 absent 1 4 15 Kyphosis Age Number Start count 81 81.000000 81.000000 81.000000 unique 2 NaN NaN NaN top absent NaN NaN NaN freq 64 NaN NaN NaN mean NaN 83.654321 4.049383 11.493827 std NaN 58.104251 1.619423 4.883962 min NaN 1.000000 2.000000 1.000000 25% NaN 26.000000 3.000000 9.000000 50% NaN 87.000000 4.000000 13.000000 75% NaN 130.000000 5.000000 16.000000 max NaN 206.000000 10.000000 18.000000 Kyphosis object Age int64 Number int64 Start int64 dtype: object In [21]: # convert the outcome in a binary form, 1 or 0 kyphosis = pd . read_csv ( \"../data/kyphosis.csv\" ) kyphosis [ \"outcome\" ] = 1 * ( kyphosis [ \"Kyphosis\" ] == \"present\" ) kyphosis . describe () Out[21]: Age Number Start outcome count 81.000000 81.000000 81.000000 81.000000 mean 83.654321 4.049383 11.493827 0.209877 std 58.104251 1.619423 4.883962 0.409758 min 1.000000 2.000000 1.000000 0.000000 25% 26.000000 3.000000 9.000000 0.000000 50% 87.000000 4.000000 13.000000 0.000000 75% 130.000000 5.000000 16.000000 0.000000 max 206.000000 10.000000 18.000000 1.000000 In [22]: kyphosis Out[22]: Kyphosis Age Number Start outcome 0 absent 71 3 5 0 1 absent 158 3 14 0 2 present 128 4 5 1 3 absent 2 5 1 0 4 absent 1 4 15 0 ... ... ... ... ... ... 76 present 157 3 13 1 77 absent 26 7 13 0 78 absent 120 2 13 0 79 present 42 7 6 1 80 absent 36 4 13 0 81 rows √ó 5 columns In [23]: from pygam import LogisticGAM , s , f , l X = kyphosis [[ \"Age\" , \"Number\" , \"Start\" ]] y = kyphosis [ \"outcome\" ] kyph_gam = LogisticGAM () . fit ( X , y ) Outcome dependence on features To help us see how the outcome depends on each feature, pyGAM has the partial_dependence() function. pdep, confi = kyph_gam.partial_dependence(term=i, X=XX, width=0.95) For more on this see the : https://pygam.readthedocs.io/en/latest/api/logisticgam.html In [24]: res = kyph_gam . deviance_residuals ( X , y ) for i , term in enumerate ( kyph_gam . terms ): if term . isintercept : continue XX = kyph_gam . generate_X_grid ( term = i ) pdep , confi = kyph_gam . partial_dependence ( term = i , X = XX , width = 0.95 ) pdep2 , _ = kyph_gam . partial_dependence ( term = i , X = X , width = 0.95 ) plt . figure () plt . scatter ( X . iloc [:, term . feature ], pdep2 + res ) plt . plot ( XX [:, term . feature ], pdep ) plt . plot ( XX [:, term . feature ], confi , c = 'r' , ls = '--' ) plt . title ( X . columns . values [ term . feature ]) plt . show () Notice that we did not specify the basis functions in the .fit(). pyGAM figures them out for us by using $s()$ (splines) for numerical variables and $f()$ for categorical features. If this is not what we want we can manually specify the basis functions, as follows: In [25]: kyph_gam = LogisticGAM ( s ( 0 ) + s ( 1 ) + s ( 2 )) . fit ( X , y ) In [26]: res = kyph_gam . deviance_residuals ( X , y ) for i , term in enumerate ( kyph_gam . terms ): if term . isintercept : continue XX = kyph_gam . generate_X_grid ( term = i ) pdep , confi = kyph_gam . partial_dependence ( term = i , X = XX , width = 0.95 ) pdep2 , _ = kyph_gam . partial_dependence ( term = i , X = X , width = 0.95 ) plt . figure () plt . scatter ( X . iloc [:, term . feature ], pdep2 + res ) plt . plot ( XX [:, term . feature ], pdep ) plt . plot ( XX [:, term . feature ], confi , c = 'r' , ls = '--' ) plt . title ( X . columns . values [ term . feature ]) plt . show () Regression in pyGAM For regression problems, we can use a linearGAM model. For this part we will use the wages dataset. https://pygam.readthedocs.io/en/latest/api/lineargam.html The wages dataset Let's inspect another dataset that is included in pyGAM that notes the wages of people based on their age, year of employment and education. In [27]: # from the pyGAM documentation from pygam import LinearGAM , s , f from pygam.datasets import wage X , y = wage ( return_X_y = True ) ## model gam = LinearGAM ( s ( 0 ) + s ( 1 ) + f ( 2 )) gam . gridsearch ( X , y ) ## plotting plt . figure (); fig , axs = plt . subplots ( 1 , 3 ); titles = [ 'year' , 'age' , 'education' ] for i , ax in enumerate ( axs ): XX = gam . generate_X_grid ( term = i ) ax . plot ( XX [:, i ], gam . partial_dependence ( term = i , X = XX )) ax . plot ( XX [:, i ], gam . partial_dependence ( term = i , X = XX , width =. 95 )[ 1 ], c = 'r' , ls = '--' ) if i == 0 : ax . set_ylim ( - 30 , 30 ) ax . set_title ( titles [ i ]); 100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00 4 - Smoothing Splines using csaps A smoothing spline is a solution of the problem of minimizing the goodness-of-fit and the smoothing (wiggliness). $MSE - \\lambda\\cdot\\text{wiggle penalty}$ $=$ $p\\sum_{i=1}&#94;N \\left(y_i - f(x_i)\\right)&#94;2 + (1-p) \\int \\left(f''(t)\\right)&#94;2 dt$, across all possible functions $f$. The p smoothing parameter is in the range of [0,1] with 0 giving you: the smoothing spline which is the least-squares straight line fit to the data, and 1: the natural cubic spline interpolant. In [28]: from csaps import csaps np . random . seed ( 1234 ) x = np . linspace ( 0 , 10 , 300000 ) y = np . sin ( x * 2 * np . pi ) * x + np . random . randn ( len ( x )) xs = np . linspace ( x [ 0 ], x [ - 1 ], 1000 ) ys = csaps ( x , y , xs , smooth = 0.99 ) print ( ys . shape ) plt . plot ( x , y , 'o' , xs , ys , '-' ) plt . show () (1000,) 5 - Data fitting using pyGAM and Penalized B-Splines When we use a spline in pyGAM we are effectively using a penalized B-Spline with a regularization parameter $\\lambda$. E.g. LogisticGAM(s(0)+s(1, lam=0.5)+s(2)).fit(X,y) Let's see how this smoothing works in pyGAM . We start by creating some arbitrary data and fitting them with a GAM. The lambda parameter (lam) goes from 0 to infinity. The default for the LinearGAM, is 0.6. In [29]: X = np . linspace ( 0 , 10 , 500 ) y = np . sin ( X * 2 * np . pi ) * X + np . random . randn ( len ( X )) plt . scatter ( X , y ); In [30]: # let's try a large lambda first and lots of splines gam = LinearGAM ( lam = 1e6 , n_splines = 50 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX )); We see that the large $\\lambda$ forces a straight line, no flexibility. Let's see now what happens if we make it smaller. In [31]: # let's try a smaller lambda gam = LinearGAM ( lam = 1e2 , n_splines = 50 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX )); There is some curvature there but still not a good fit. Let's try no penalty. That should have the line fit exactly. In [32]: # no penalty, let's try a 0 lambda gam = LinearGAM ( lam = 0 , n_splines = 50 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ) plt . plot ( XX , gam . predict ( XX )); Yes, that is good. Now let's see what happens if we lessen the number of splines. The fit should not be as good. In [33]: # no penalty, let's try a 0 lambda gam = LinearGAM ( lam = 0 , n_splines = 10 ) . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX )); In [34]: # no penalty, let's try a 0 lambda gam = LinearGAM () . fit ( X , y ) XX = gam . generate_X_grid ( term = 0 ) plt . scatter ( X , y , alpha = 0.3 ); plt . plot ( XX , gam . predict ( XX ));",
        "tags": "Lectures",
        "url": "lectures/lecture05.5/notebook2/"
    }, {
        "title": "Lecture 5 - Unsupervised learning, cluster analysis (part 1)",
        "text": "Title Clustering Example Notebook Description : This notebook provides example code based on the lecture material. If you wish to run or edit the notebook, we recommend downloading it and running it either on your local machine or on JupyterHub. In [1]: import pandas as pd from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler import numpy as np % matplotlib inline import matplotlib.pyplot as plt Old Faithful and Clustering In [2]: faithful = pd . read_csv ( \"data/faithful.csv\" ) display ( faithful . head ()) display ( faithful . describe ()) eruptions waiting 0 3.600 79 1 1.800 54 2 3.333 74 3 2.283 62 4 4.533 85 eruptions waiting count 272.000000 272.000000 mean 3.487783 70.897059 std 1.141371 13.594974 min 1.600000 43.000000 25% 2.162750 58.000000 50% 4.000000 76.000000 75% 4.454250 82.000000 max 5.100000 96.000000 In [3]: import seaborn as sns plt . figure ( figsize = ( 10 , 5 )) plt . scatter ( faithful [ \"eruptions\" ], faithful [ \"waiting\" ]) plt . xlabel ( \"eruptions\" ) plt . ylabel ( \"waiting\" ) plt . xlim ( 0 , 6 ) plt . ylim ( 30 , 100 ) plt . figure ( figsize = ( 10 , 5 )) sns . kdeplot ( faithful [ \"eruptions\" ], faithful [ \"waiting\" ]) plt . scatter ( faithful [ \"eruptions\" ], faithful [ \"waiting\" ]) plt . xlim ( 0 , 6 ) plt . show ( 30 , 100 ) /home/glickm/.local/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( There are two distinct modes to the data: one with eruption values (voulmes?) of 1 to 3 and low waiting times, and a second cluster with larger eruptions and longer waiting times. Notably, there are very few eruptions in the middle. Review: PCA First, we import data on different types of crime in each US state In [4]: USArrests = pd . read_csv ( \"data/USArrests.csv\" ) USArrests [ 'StateAbbrv' ] = [ \"AL\" , \"AK\" , \"AZ\" , \"AR\" , \"CA\" , \"CO\" , \"CT\" , \"DE\" , \"FL\" , \"GA\" , \"HI\" , \"ID\" , \"IL\" , \"IN\" , \"IA\" , \"KS\" , \"KY\" , \"LA\" , \"ME\" , \"MD\" , \"MA\" , \"MI\" , \"MN\" , \"MS\" , \"MO\" , \"MT\" , \"NE\" , \"NV\" , \"NH\" , \"NJ\" , \"NM\" , \"NY\" , \"NC\" , \"ND\" , \"OH\" , \"OK\" , \"OR\" , \"PA\" , \"RI\" , \"SC\" , \"SD\" , \"TN\" , \"TX\" , \"UT\" , \"VT\" , \"VA\" , \"WA\" , \"WV\" , \"WI\" , \"WY\" ] display ( USArrests . head ()) display ( USArrests . describe ()) State Murder Assault UrbanPop Rape StateAbbrv 0 Alabama 13.2 236 58 21.2 AL 1 Alaska 10.0 263 48 44.5 AK 2 Arizona 8.1 294 80 31.0 AZ 3 Arkansas 8.8 190 50 19.5 AR 4 California 9.0 276 91 40.6 CA Murder Assault UrbanPop Rape count 50.00000 50.000000 50.000000 50.000000 mean 7.78800 170.760000 65.540000 21.232000 std 4.35551 83.337661 14.474763 9.366385 min 0.80000 45.000000 32.000000 7.300000 25% 4.07500 109.000000 54.500000 15.075000 50% 7.25000 159.000000 66.000000 20.100000 75% 11.25000 249.000000 77.750000 26.175000 max 17.40000 337.000000 91.000000 46.000000 The data has more dimensinons than we can easily visualize, so we use PCA to condense it. As usual, we scale the data before applying PCA. (Note that we scale everything, rather than fitting on train and carrying that scaling to future data-- we won't be using a test set here, so it's correct to use all the data to scale). In [5]: from sklearn import preprocessing df = USArrests [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]] scaled_df = pd . DataFrame ( preprocessing . scale ( df ), index = USArrests [ 'State' ], columns = df . columns ) fitted_pca = PCA () . fit ( scaled_df ) USArrests_pca = fitted_pca . transform ( scaled_df ) The biplot function plots the first two PCA components, and provides some helpful annotations In [6]: def biplot ( scaled_data , fitted_pca , original_dim_labels , point_labels ): pca_results = fitted_pca . transform ( scaled_data ) pca1_scores = pca_results [:, 0 ] pca2_scores = pca_results [:, 1 ] # plot each point in 2D post-PCA space plt . scatter ( pca1_scores , pca2_scores ) # label each point for i in range ( len ( pca1_scores )): plt . text ( pca1_scores [ i ], pca2_scores [ i ], point_labels [ i ]) #for each original dimension, plot what an increase of 1 in that dimension means in this space for i in range ( fitted_pca . components_ . shape [ 1 ]): raw_dims_delta_on_pca1 = fitted_pca . components_ [ 0 , i ] raw_dims_delta_on_pca2 = fitted_pca . components_ [ 1 , i ] plt . arrow ( 0 , 0 , raw_dims_delta_on_pca1 , raw_dims_delta_on_pca2 , color = 'r' , alpha = 1 ) plt . text ( raw_dims_delta_on_pca1 * 1.1 , raw_dims_delta_on_pca2 * 1.1 , original_dim_labels [ i ], color = 'g' , ha = 'center' , va = 'center' ) plt . figure ( figsize = ( 8.5 , 8.5 )) plt . xlim ( - 3.5 , 3.5 ) plt . ylim ( - 3.5 , 3.5 ) plt . xlabel ( \"PC {} \" . format ( 1 )) plt . ylabel ( \"PC {} \" . format ( 2 )) plt . grid () biplot ( scaled_df , fitted_pca , original_dim_labels = scaled_df . columns , point_labels = USArrests [ 'StateAbbrv' ]) The red arrows and green text give us a sense of direction. If any state had 'murder' increase by one (scaled) unit, it would move in the direction of the 'murder' line by that amount. An increase by one (scaled) unit of both 'murder' and 'Urban Pop' would apply both moves. We can also make inferrences about what combination of crimes and population puts California at its observed point. Extra: Variance Captured As usual, we want to know how what proportion of the variance each PC captures In [7]: plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 5 ), fitted_pca . explained_variance_ratio_ , \"-o\" ) plt . xlabel ( \"Principal Component\" ) plt . ylabel ( \"Proportion of Variance Explained\" ) plt . ylim ( 0 , 1 ) plt . show () print ( \"Proportion of variance explained by each PC:\" ) print ( fitted_pca . explained_variance_ratio_ ) Proportion of variance explained by each PC: [0.62006039 0.24744129 0.0891408 0.04335752] Even more usefully, we can plot how much of the total variation we'd capture by using N PCs. The PCA-2 plot above has 86.7% of the total variance. In [8]: plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 5 ), np . cumsum ( fitted_pca . explained_variance_ratio_ ), \"-o\" ) plt . xlabel ( \"Principal Component\" ) plt . ylabel ( \"Cumulative Proportion of Variance Explained\" ) plt . ylim ( 0 , 1.1 ) plt . show () print ( \"Total variance capturted when using N PCA components:\" ) print ( np . cumsum ( fitted_pca . explained_variance_ratio_ )) Total variance capturted when using N PCA components: [0.62006039 0.86750168 0.95664248 1. ] Scaling and Distances Returning to the arrest/crime data, we again inspect the data and its PCA plot In [9]: np . random . seed ( 123 ) arrests_sample = USArrests . sample ( 6 ) arrests_sample Out[9]: State Murder Assault UrbanPop Rape StateAbbrv 10 Hawaii 5.3 46 83 20.2 HI 13 Indiana 7.2 113 65 21.0 IN 30 New Mexico 11.4 285 70 32.1 NM 46 Washington 4.0 145 73 26.2 WA 18 Maine 2.1 83 51 7.8 ME 0 Alabama 13.2 236 58 21.2 AL In [10]: np . random . seed ( 123 ) np . round ( scaled_df . sample ( 6 ), 2 ) Out[10]: Murder Assault UrbanPop Rape State Hawaii -0.58 -1.51 1.22 -0.11 Indiana -0.14 -0.70 -0.04 -0.03 New Mexico 0.84 1.38 0.31 1.17 Washington -0.88 -0.31 0.52 0.54 Maine -1.32 -1.06 -1.01 -1.45 Alabama 1.26 0.79 -0.53 -0.00 Distances One of the key ideas in clustering is the distance or disimilarity between points. Euclidean distance is common, though one is free to define domain-specific measures of how similar/distant two observations are. In [11]: from scipy.spatial.distance import pdist from scipy.spatial.distance import squareform The pdist function computes the distances between all pairs of data points (which can be quite expensive for large data). squareform turns the result into a numpy array (the raw format avoids storing redundant values) The distances between a handful of states are shown below. Hawaii and Indiana are relatively similar on these variables, while Maine and New Mexico are relatively different. In [12]: dist_eucl = pdist ( scaled_df , metric = \"euclidean\" ) distances = pd . DataFrame ( squareform ( dist_eucl ), index = USArrests [ \"State\" ] . values , columns = USArrests [ \"State\" ] . values ) sample_distances = distances . loc [ arrests_sample [ \"State\" ], arrests_sample [ \"State\" ]] sample_distances Out[12]: Hawaii Indiana New Mexico Washington Maine Alabama Hawaii 0.000000 1.561769 3.586656 1.560979 2.743631 3.422932 Indiana 1.561769 0.000000 2.617305 1.152154 2.124266 2.097219 New Mexico 3.586656 2.617305 0.000000 2.504780 4.390177 1.615635 Washington 1.560979 1.152154 2.504780 0.000000 2.655948 2.675068 Maine 2.743631 2.124266 4.390177 2.655948 0.000000 3.520494 Alabama 3.422932 2.097219 1.615635 2.675068 3.520494 0.000000 For visualization, we can make a heatmap of the sample state's distances In [13]: plt . figure ( figsize = ( 11 , 8.5 )) sns . heatmap ( sample_distances , cmap = \"mako\" ) plt . show () We can likewise heatmap all the states. In [14]: import seaborn as sns plt . figure ( figsize = ( 11 , 8.5 )) sns . heatmap ( distances ) plt . show () Kmeans Kmeans is a classical, workhorse clustering algorithm, and a common place to start. It assumes there are K centers and, starting from random guesses, algorithmically improves its guess about where the centers must be. In [15]: from sklearn.cluster import KMeans #random_state parameter sets seed for random number generation arrests_km = KMeans ( n_clusters = 3 , n_init = 25 , random_state = 123 ) . fit ( scaled_df ) arrests_km . cluster_centers_ Out[15]: array([[-0.49440658, -0.3864845 , 0.58167593, -0.26431024], [ 1.01513667, 1.02412028, 0.19959126, 0.85556386], [-0.88515915, -1.0213324 , -0.94990286, -0.92016524]]) We can read off where the 3 cluster centers are. (The value 3 is chosen arbitratially- soon we'll see how to tell what number of clusters seems to work best) In [16]: pd . DataFrame ( arrests_km . cluster_centers_ , columns = [ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]) Out[16]: Murder Assault UrbanPop Rape 0 -0.494407 -0.386484 0.581676 -0.264310 1 1.015137 1.024120 0.199591 0.855564 2 -0.885159 -1.021332 -0.949903 -0.920165 The .lables_ tell us which cluster each point was assigned to In [17]: scaled_df_cluster = scaled_df . copy () scaled_df_cluster [ 'Cluster' ] = arrests_km . labels_ scaled_df_cluster . head () Out[17]: Murder Assault UrbanPop Rape Cluster State Alabama 1.255179 0.790787 -0.526195 -0.003451 1 Alaska 0.513019 1.118060 -1.224067 2.509424 1 Arizona 0.072361 1.493817 1.009122 1.053466 1 Arkansas 0.234708 0.233212 -1.084492 -0.186794 2 California 0.281093 1.275635 1.776781 2.088814 1 The mean of the points in each cluster is the cluster center found by K-means In [18]: scaled_df_cluster . groupby ( 'Cluster' ) . mean () Out[18]: Murder Assault UrbanPop Rape Cluster 0 -0.494407 -0.386484 0.581676 -0.264310 1 1.015137 1.024120 0.199591 0.855564 2 -0.885159 -1.021332 -0.949903 -0.920165 Silhouette Plots Silhouette plots give rich information on the quality of a clustering In [19]: from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.cm as cm #modified code from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html def silplot ( X , cluster_labels , clusterer , pointlabels = None ): n_clusters = clusterer . n_clusters # Create a subplot with 1 row and 2 columns fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) fig . set_size_inches ( 11 , 8.5 ) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1 . set_xlim ([ - 0.1 , 1 ]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1 . set_ylim ([ 0 , len ( X ) + ( n_clusters + 1 ) * 10 ]) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score ( X , cluster_labels ) print ( \"For n_clusters = \" , n_clusters , \", the average silhouette_score is \" , silhouette_avg , \".\" , sep = \"\" ) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples ( X , cluster_labels ) y_lower = 10 for i in range ( 0 , n_clusters + 1 ): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values [ cluster_labels == i ] ith_cluster_silhouette_values . sort () size_cluster_i = ith_cluster_silhouette_values . shape [ 0 ] y_upper = y_lower + size_cluster_i color = cm . nipy_spectral ( float ( i ) / n_clusters ) ax1 . fill_betweenx ( np . arange ( y_lower , y_upper ), 0 , ith_cluster_silhouette_values , facecolor = color , edgecolor = color , alpha = 0.7 ) # Label the silhouette plots with their cluster numbers at the middle ax1 . text ( - 0.05 , y_lower + 0.5 * size_cluster_i , str ( i )) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1 . set_title ( \"The silhouette plot for the various clusters.\" ) ax1 . set_xlabel ( \"The silhouette coefficient values\" ) ax1 . set_ylabel ( \"Cluster label\" ) # The vertical line for average silhouette score of all the values ax1 . axvline ( x = silhouette_avg , color = \"red\" , linestyle = \"--\" ) ax1 . set_yticks ([]) # Clear the yaxis labels / ticks ax1 . set_xticks ([ - 0.1 , 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1 ]) # 2nd Plot showing the actual clusters formed colors = cm . nipy_spectral ( cluster_labels . astype ( float ) / n_clusters ) # axes will be first 2 PCA components pca = PCA ( n_components = 2 ) . fit ( X ) X_pca = pca . transform ( X ) ax2 . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], marker = '.' , s = 200 , lw = 0 , alpha = 0.7 , c = colors , edgecolor = 'k' ) xs = X_pca [:, 0 ] ys = X_pca [:, 1 ] if pointlabels is not None : for i in range ( len ( xs )): plt . text ( xs [ i ], ys [ i ], pointlabels [ i ]) # Labeling the clusters (transform to PCA space for plotting) centers = pca . transform ( clusterer . cluster_centers_ ) # Draw white circles at cluster centers ax2 . scatter ( centers [:, 0 ], centers [:, 1 ], marker = 'o' , c = \"white\" , alpha = 1 , s = 200 , edgecolor = 'k' ) for i , c in enumerate ( centers ): ax2 . scatter ( c [ 0 ], c [ 1 ], marker = '$ %d $' % int ( i ), alpha = 1 , s = 50 , edgecolor = 'k' ) ax2 . set_title ( \"The visualization of the clustered data.\" ) ax2 . set_xlabel ( \"PC1\" ) ax2 . set_ylabel ( \"PC2\" ) plt . suptitle (( \"Silhouette analysis for KMeans clustering on sample data \" \"with n_clusters = %d \" % n_clusters ), fontsize = 14 , fontweight = 'bold' ) In [20]: fitted_km = KMeans ( n_clusters = 4 , n_init = 25 , random_state = 123 ) . fit ( scaled_df ) cluster_labels = fitted_km . labels_ silplot ( scaled_df . values , cluster_labels , fitted_km ) For n_clusters = 4, the average silhouette_score is 0.33968891433344395. In [21]: # Objects with negative silhouette sil = silhouette_samples ( scaled_df , fitted_km . labels_ ) USArrests . loc [ sil <= 0 ,:] Out[21]: State Murder Assault UrbanPop Rape StateAbbrv 24 Missouri 9.0 178 70 28.2 MO Elbow plots In [22]: wss = [] for i in range ( 1 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) wss . append ( fitx . inertia_ ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), wss , 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Inertia' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Silhouette Score In [23]: from sklearn.metrics import silhouette_score scores = [ 0 ] # silhouette score for 1 cluster for i in range ( 2 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) score = silhouette_score ( scaled_df , fitx . labels_ ) scores . append ( score ) print ( \"Optimized at\" , max ( range ( len ( scores )), key = scores . __getitem__ ) + 1 , \"clusters\" ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), np . array ( scores ), 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Average Silhouette' ) plt . title ( 'The Silhouette Method showing the optimal $k$' ) plt . show () Optimized at 2 clusters Gap Statistic In [39]: # need to install library 'gap-stat' from gap_statistic import OptimalK gs_obj = OptimalK () n_clusters = gs_obj ( scaled_df . values , n_refs = 50 , cluster_array = np . arange ( 1 , 15 )) print ( 'Optimal clusters: ' , n_clusters ) Optimal clusters: 4 In [40]: gs_obj . gap_df . head () Out[40]: n_clusters gap_value gap* ref_dispersion_std sk sk* diff diff* 0 1.0 0.222159 50.359063 17.148526 0.070837 53.728106 -0.295650 52.722809 1 2.0 0.601961 87.335301 16.146239 0.084152 89.699046 0.076047 89.206030 2 3.0 0.610179 70.534332 12.829924 0.084266 72.405061 -0.072176 71.992951 3 4.0 0.765419 68.017105 10.291019 0.083064 69.475724 0.380796 69.630492 4 5.0 0.478021 41.794611 10.025900 0.093398 43.407999 -0.180014 42.850792 In [41]: gs_obj . plot_results () Hierarchical Clustering K-means is a very 'hard' clustering: points belong to exactly one cluster, no matter what. A hierarchical clustering creates a nesting of clusters as existing clusters are merged or split. Dendograms (literally: branch graphs) can show the pattern of splits/merges. In [27]: import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist plt . figure ( figsize = ( 11 , 8.5 )) dist_mat = pdist ( scaled_df , metric = \"euclidean\" ) ward_data = hac . ward ( dist_mat ) hac . dendrogram ( ward_data , labels = USArrests [ \"State\" ] . values ); plt . show () DBSCAN DBSCAN is a more modern clustering approach that allows points to not be part of any cluster, and determines the number of clusters by itself. First, let's look at out data In [28]: multishapes = pd . read_csv ( \"data/multishapes.csv\" ) ms = multishapes [[ 'x' , 'y' ]] In [29]: msplot = ms . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , title = \"Multishapes data\" , figsize = ( 11 , 8.5 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () To the eye, there's a pretty clear structure to the data However, K-means struggles to find a good clustering In [30]: shape_km = KMeans ( n_clusters = 5 , n_init = 25 , random_state = 123 ) . fit ( ms ) In [31]: plt . figure ( figsize = ( 10 , 10 )) plt . scatter ( ms [ 'x' ], ms [ 'y' ], c = shape_km . labels_ ); plt . scatter ( shape_km . cluster_centers_ [:, 0 ], shape_km . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 100 ); #todo: labels? different markers? DB Scan uses a handful of parameters, including the number of neighbors a point must have to be considered 'core' ( min_samples ) and the distance within which neighbors must fall ( epsilon ). Most reasonable values of min_samples yeild the same results, but tuning epsilon is important. The function below implement's the authors suggestion for setting epsilon: look at the nearest-neighbor distances and find a level where they begin to grow rapidly. In [32]: from sklearn.neighbors import NearestNeighbors def plot_epsilon ( df , min_samples ): fitted_neigbors = NearestNeighbors ( n_neighbors = min_samples ) . fit ( df ) distances , indices = fitted_neigbors . kneighbors ( df ) dist_to_nth_nearest_neighbor = distances [:, - 1 ] plt . plot ( np . sort ( dist_to_nth_nearest_neighbor )) plt . xlabel ( \"Index \\n (sorted by increasing distances)\" ) plt . ylabel ( \" {} -NN Distance (epsilon)\" . format ( min_samples - 1 )) plt . tick_params ( right = True , labelright = True ) In [33]: plot_epsilon ( ms , 3 ) The major slope occurs around eps=0.15 when min_samples set to 3. In [34]: from sklearn.cluster import DBSCAN fitted_dbscan = DBSCAN ( eps = 0.15 ) . fit ( ms ) plt . figure ( figsize = ( 10 , 10 )) plt . scatter ( ms [ 'x' ], ms [ 'y' ], c = fitted_dbscan . labels_ ); We see good results with the suggested epsilon. A lower epsilon (0.12) won't quite merge all the clustersWe DBSCAN on crime data Returning to the crime data, let's tune epsilon and see what clusters are returned In [35]: plot_epsilon ( scaled_df , 5 ) The optimal value is either around 1.4 to 1.6. Choosing one versus the other does not result in different clusterings. In [36]: fitted_dbscan = DBSCAN ( eps = 1.4 , min_samples = 5 ) . fit ( scaled_df ) fitted_dbscan . labels_ Out[36]: array([ 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) At this epsilon and min_samples , all but one state are included in cluster 0. The remaining point (Alaska) is not part of any cluster. DBSCAN is not particularly effective using this data set.",
        "tags": "Lectures",
        "url": "lectures/lecture05/notebook/"
    }, {
        "title": "Lecture 5_5: Smoothers pyGAM csaps",
        "text": "Exercises Lecture 5.5: pyGAM (Notebook) Lecture 5.5: pyGAM with some additions (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture05_5/"
    }, {
        "title": "Lecture 4: Splines Smoothers and GAMs (part 3)",
        "text": "Exercises Lecture 4: B-Splines (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture04/"
    }, {
        "title": "Lecture 4 - Splines, Smoothers, and GAMs (part 3)",
        "text": "Title : B-Splines (Cubic) Regression Description : After this exercise, you should see a plot that looks like : Hints: scipy.interpolate.splrep scipy.interpolate.BSpline Refer to lecture notebook. Do not change any other code except the blanks. In [1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.interpolate import splrep from scipy.interpolate import BSpline % matplotlib inline In [2]: df = pd . read_csv ( 'data1.csv' ) df = df . sort_values ( 'x' ) df . head () Out[2]: x y 73 0.006248 17.329551 142 0.104671 15.268703 34 0.246137 15.394678 17 0.420595 12.743181 5 0.455565 15.480800 In [3]: plt . scatter ( df . x , df . y ); plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . show () A (cubic) B-spline regression of y on x, with knots chosen at the quartiles. In [4]: #Get quartiles (knots) quarts = df [ 'x' ] . quantile ([ 0.25 , 0.5 , 0.75 ]) . values . reshape ( - 1 ) print ( quarts ) [2.74220877 4.82472501 7.26165491] In [0]: ### edTest(test_splrep) ### #Find the BSpline representation using splrep() def test_splrep (): t , c , k = splrep ( ________ ) return t , c , k t , c , k = test_splrep () #A tuple (t,c,k) containing the vector of knots, the B-spline coefficients, and the degree of the spline. In [0]: ### edTest(test_bspline_model) ### def test_bspline_model (): return BSpline ( _______ ) b_spline_model = test_bspline_model () In [0]: ax = df . plot . scatter ( x = 'x' , y = 'y' , c = 'grey' ) ax . plot ( df [ 'x' ], b_spline_model ( df [ 'x' ]), label = \"B-spline, knots at quartiles\" ) plt . legend () plt . show ()",
        "tags": "Lectures",
        "url": "lectures/lecture04/notebook/"
    }, {
        "title": "Lecture 3: Setup and Review of statsmodels",
        "text": "Exercises Lecture 3: Introduction and Setup (Notebook) Lecture 3: Review of statsmodels, numpy, and linear algebra (Notebook) YML File cs109b.yml (yml)",
        "tags": "lectures",
        "url": "lectures/lecture03/"
    }, {
        "title": "Lecture 3: Setup and Review of statsmodels",
        "text": "CS109B Data Science 2: Advanced Topics in Data Science Lecture 3 - Coding Environment Setup and review of statsmodels Notebook B Harvard University Spring 2021 Instructors: Mark Glickman, Pavlos Protopapas, and Chris Tanner Additional Instructor: Eleni Kaxiras Content: Eleni Kaxiras and Will Claybaugh In [5]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) In [9]: import numpy as np from scipy.interpolate import interp1d import matplotlib.pyplot as plt import pandas as pd % matplotlib inline Learning Goals By the end of this lab, you should be able to: use np.linalg.vander use the weird R-style formulas in statsmodels practice least-squares regression in statsmodels Basis Functions In our models we can use various types of functions as basis functions. Strictly speaking, in linear algebra where a basis for a subspace S of $\\mathbb{R}&#94;n$ is a set of vectors that spans S and is linearly independent. As a reminder, a set of vectors $\\textbf{v}_1, \\textbf{v}_2, ..., \\textbf{v}_k$ are considered linearly independent if they cannot be written as a linear combination of each other, such that, if: $c_1\\textbf{v}_1+c_2\\textbf{v}_2+ ...+ c_k\\textbf{v}_k = \\textbf{0}$ then $c_1,c_2,...,c_k$ are all zero. In data science where we have lots of imperfect data (with errors), as well as imperfect computers (with round-off errors), when we substitute their values into the matrices we almost always get column degeneracy, meaning, some of our columns become linear combinations of each other. Especially so if we use the monomial basis and go beyond ~5,6 degree of the polynomial. Examples are: Monomials such as $x,x&#94;2,x&#94;4,x&#94;5$ Sigmoid/ReLU functions (neural networks) Fourier functions Wavelets Splines The matrix produced when we substitute the values of our data into the basis functions is called the design matrix . Linear/Polynomial Regression We will use the diabetes dataset. Variables are: subject: subject ID number age: age diagnosed with diabetes acidity: a measure of acidity called base deficit Response: y: natural log of serum C-peptide concentration Original source is Sockett et al. (1987) mentioned in Hastie and Tibshirani's book \"Generalized Additive Models\". Reading the data in Pandas: In [10]: diab = pd . read_csv ( \"data/diabetes.csv\" ) diab . head () Create the design matrix for a fictitious dataset Let's keep just the age feature and create some columns of our own. Let's see how good this matrix is before we create the design matrix. In [11]: diab_age = diab [[ 'age' ]] . copy () diab_age [ 'age2' ] = diab_age . apply ( lambda row : row [ 'age' ] ** 2 , axis = 1 ) diab_age [ 'random' ] = np . random . normal ( 0 , 1 , len ( diab_age )) diab_age [ 'same' ] = diab_age [ 'age' ] diab_age . head () In [12]: A = diab_age . to_numpy ( copy = True ) A [: 5 ] Let's check if the columns of A are linearly independent by using some linear algebra methods from numpy.linalg and sympy . In [161]: from numpy.linalg import matrix_rank matrix_rank ( A ) In [162]: # check out which rows are linearly independent import sympy _ , inds = sympy . Matrix ( A ) . T . rref () inds In [163]: np . linalg . cond ( A ) Create the design matrix for age using a polynomial basis Let's keep just the age feature again and create the design matrix using a polynomial of degree n . First we will use the basic numpy formula vander() . In [185]: vand = np . vander ( diab_age . age , 2 , increasing = True ) vand [: 3 ], vand . shape In [186]: ## To our point why the Vandermonde matrix is usually ill-conditioned, ## find the condition number of this matrix np . linalg . cond ( vand ), matrix_rank ( vand ) Exercise : Vandermonde matrix Change the degree of the polynomial and comment on what happens to the condition and rank of the matrix. In [187]: vand = np . vander ( diab_age . age , 8 , increasing = True ) vand [: 3 ], vand . shape In [188]: ## To our point why the Vandermonde matrix is usually ill-conditioned, ## find the condition number of this matrix np . linalg . cond ( vand ), matrix_rank ( vand ) Linear/Polynomial regression with statsmodels. As you remember from 109a, we have two tools for Linear Regression: statsmodels https://www.statsmodels.org/stable/regression.html , and sklearn https://scikit-learn.org/stable/index.html Previously, in this notebook, we worked from a vector of target values and a design matrix we built ourself. In 109a we used e.g. sklearn 's PolynomialFeatures to build the matrix. Now we will look at statsmodels which allows users to fit statistical models using R-style formulas . They build the target value and design matrix for you. Note: Categorical features (e.g. let's say we had a categorical feature called Region, are designated by C(Region) ), polynomial features (e.g. age) are entered as np.power(age, n) where n is the degree of the polynomial OR np.vander(age, n, increasing=True) . # Example: if our target variable is 'Lottery', while 'Region' is a categorical predictor and all the others are numerical: df = dta.data[['Lottery', 'Literacy', 'Wealth', 'Region']] formula='Lottery ~ Literacy + Wealth + C(Region) + Literacy * Wealth' For more on these formulas see: https://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html https://patsy.readthedocs.io/en/latest/overview.html In [191]: import statsmodels.formula.api as smf model1 = smf . ols ( 'y ~ age' , data = diab ) fit1_lm = model1 . fit () Let's build a dataframe to predict values on (sometimes this is just the test or validation set). Very useful for making pretty plots of the model predictions - predict for TONS of values, not just whatever's in the training set. In [194]: x_pred = np . linspace ( 0.5 , 20 , 100 ) predict_df = pd . DataFrame ( data = { \"age\" : x_pred }) predict_df . head () Use get_prediction( ).summary_frame() to get the model's prediction (and error bars!) In [195]: prediction_output = fit1_lm . get_prediction ( predict_df ) . summary_frame () prediction_output . head () Plot the data, the fitted model, the confidence intervals , and the prediction intervals . For more on how statsmodels calculates these intervals see: https://www.statsmodels.org/stable/_modules/statsmodels/regression/_prediction.html In [196]: ax1 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'brown' , title = \"Diabetes data with least-squares linear fit\" ) ax1 . set_xlabel ( \"Age at Diagnosis\" ) ax1 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean' ], color = \"green\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ) ax1 . plot ( predict_df . age , prediction_output [ 'obs_ci_lower' ], color = \"green\" , linestyle = \"dashdot\" ) ax1 . plot ( predict_df . age , prediction_output [ 'obs_ci_upper' ], color = \"green\" , linestyle = \"dashdot\" ); Breakout Room Exercise Fit a 3rd degree polynomial model to predict y using only age and Plot the model and its confidence intervals. Change the degree of your polynomial and see what happens to the fitted curve. Does our model have an intercept? Note : we can discover the existence or not of an intercept in our model by running: model_name.params In [67]: # your answer here In [201]: # solution Vandermonde matrix in formulas It's easier to build higher order polynomials using np.vandrer() . In [204]: formula = \"y ~ np.vander(age, 6, increasing=True) -1\" fit3_lm = smf . ols ( formula = formula , data = diab ) . fit () In [205]: fit3_lm . params In [207]: ## To our point why the Vandermonde matrix is usually ill-conditioned, # find the condition number of this matrix np . linalg . cond ( np . vander ( predict_df . age , 6 , increasing = True )) In [208]: # solution poly_predictions = fit3_lm . get_prediction ( predict_df ) . summary_frame () poly_predictions . head () In [209]: # solution x_pred = np . linspace ( 0.5 , 15 , 100 ) predict_df = pd . DataFrame ( data = { \"age\" : x_pred }) ax2 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares cubic fit\" ) ax2 . set_xlabel ( \"Age at Diagnosis\" ) ax2 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean' ], color = \"green\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" , label = 'confidence interval' ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); ax2 . legend (); Discussion QR decomposition ( Beyond the scope of this class ) As you know, to find the parameters of our model, we may try to solve the so-called normal equations , which, written in matrix form, are: \\begin{equation} (\\boldsymbol{A&#94;T}\\cdot \\boldsymbol{A}) \\cdot \\boldsymbol{b} = \\boldsymbol{A} \\cdot \\boldsymbol{y} \\end{equation} The direct solution is $\\hat{\\boldsymbol{b}}=(\\boldsymbol{A}&#94;T\\cdot \\boldsymbol{A})&#94;{-1}\\cdot \\boldsymbol{A}&#94;T \\cdot \\boldsymbol{y}$ Solving the least-squares problem directly via the normal equations is susceptible to roundoff error when the condition of the matrix $\\boldsymbol{A}$ is large. An alternative technique involves QR decomposition (details in any good linear algebra book). statsmodels lets you use this technique via a parameter in the .fit : .fit(method='qr') Let's try with QR now In [210]: formula = \"y ~ np.vander(age, 6, increasing=True) -1\" fit3_lm = smf . ols ( formula = formula , data = diab ) . fit ( method = 'qr' ) In [211]: fit3_lm . params In [212]: # solution poly_predictions = fit3_lm . get_prediction ( predict_df ) . summary_frame () poly_predictions . head () In [213]: # solution ax2 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares cubic fit\" ) ax2 . set_xlabel ( \"Age at Diagnosis\" ) ax2 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean' ], color = \"green\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" , label = 'confidence interval' ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); ax2 . legend ();",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook1/"
    }, {
        "title": "Lecture 3: Setup and Review of statsmodels",
        "text": "CS109B Data Science 2: Advanced Topics in Data Science Lecture 3 - Coding Environment Setup and review of statsmodels Notebook A Harvard University Spring 2021 Instructors: Pavlos Protopapas, Mark Glickman, and Chris Tanner Additional Instructor: Eleni Kaxiras Content: Eleni Kaxiras and Will Claybaugh In [14]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[14]: In [15]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Learning Goals By the end of this lecture, you should be able to: Know how to set up your coding/working environment. Understand how to implement various functions in statsmodels , e.g. how to use R-like formulas for variable representation. 1. Getting class material Option 1A: Download directly from Ed Use the >> to download. Option 1B: Cloning the class repo and then copying the contents in a different directory so you can make changes. You may access the code used in class by cloning the class repo: https://github.com/Harvard-IACS/2021-CS109B Open the Terminal in your computer and go to the Directory where you want to clone the repo. Then run git clone https://github.com/Harvard-IACS/2021-CS109B.git If you have already cloned the repo, OR if new material is added (happens every day), go inside the '/2021-CS109B/' directory and run git pull Caution: If you change the notebooks and then run git pull your changes will be overwritten. So create a playground folder and copy the folder with the notebook with which you want to work. Option 1C: (use at own risk) Forking the class repo In order not to lose any changes you have made when updating the content (pulling) from the main repo, a good practice is to fork the repo locally. For more on this see Maddy Nakada's notes (make sure you substitute 2019 for 2021!): How to Fork a Repo . NOTE: While Fork is a proper way to handle local changes, it doesn't magically solve everything -- if you edit a file that originated from our course repo (e.g., a HW notebook), and later pull from our 'upstream' repo again, any changes you make will require resolving merge conflict(s) . Thus, if you want to safetly and easily preserve any of your changes, we recommend renaming your files and/or copying them into an independent directory within your repo. 2. Running code: Option 2A: Using your local environment Use Virtual Environments: we cannot stress this enough! Isolating your projects inside specific environments helps you manage dependencies and therefore keep your sanity. You can recover from mess-ups by simply deleting an environment. Sometimes certain installation of libraries conflict with one another. The two most popular tools for setting up environments are: conda (a package and environment manager) pip (a Python package manager) with virtualenv (a tool for creating environments) We recommend using conda package installation and environments. conda installs packages from the Anaconda Repository and Anaconda Cloud, whereas pip installs packages from PyPI. Even if you are using conda as your primary package installer and are inside a conda environment, you can still use pip install for those rare packages that are not included in the conda ecosystem. See here for more details on how to manage Conda Environments . Use the cs109b.yml file to create an environment: $ conda env create -f cs109b.yml $ conda activate cs109b We have included the packages that you will need in the cs109b.yml file. Option 2B: Using Cloud Resources Using FAS OnDemand (supported by CS109b) FAS provides a platform, accessible via the FAS OnDemand menu link in Canvas . Most of the libraries such as keras, tensorflow, pandas, etc., are pre-installed. If a library is missing you may install it via the Terminal. NOTE : The AWS platform is funded by FAS for the purposes of the class. You are not allowed to use it for purposes not related to this course. Make sure you stop your instance as soon as you do not need it. Information on how to use the platform is displayed when you click the link. For more see Fas OnDemand Guide . Using Google Colab (on your own) Google's Colab platform https://colab.research.google.com/ offers a GPU enviromnent to test your ideas, it's fast, free, with the only caveat that your files persist only for 12 hours (last time we checked). The solution is to keep your files in a repository and just clone it each time you use Colab. Using AWS in the Cloud (on your own) For those of you who want to have your own machines in the Cloud to run whatever you want, Amazon Web Services is a (paid) solution. For more see: https://docs.aws.amazon.com/polly/latest/dg/setting-up.html Remember, AWS is a paid service, so if you let your machine run for days you will get charged! 3. Ensuring everything is installed correctly Some of the packages we will need for this class Smoothing : statsmodels - https://www.statsmodels.org/ scipy pyGAM - https://pygam.readthedocs.io/en/latest/ csaps - [ https://csaps.readthedocs.io ] Clustering : Sklearn - https://scikit-learn.org/stable/ scipy - https://www.scipy.org gap_statistic (by Miles Granger) - https://anaconda.org/milesgranger/gap-statistic/notebook Bayes : pymc3 - https://docs.pymc.io Neural Networks : keras - https://www.tensorflow.org/guide/keras Exercise 1: Run the following cells to make sure these packages load correctly in our environment. In [3]: from sklearn import datasets iris = datasets . load_iris () digits = datasets . load_digits () digits . target # you should see [0, 1, 2, ..., 8, 9, 8] Out[3]: array([0, 1, 2, ..., 8, 9, 8]) In [4]: from scipy import misc import matplotlib.pyplot as plt face = misc . face () plt . imshow ( face ) plt . show () # you should see a racoon In [5]: import statsmodels.api as sm import statsmodels.formula.api as smf # Load data dat = sm . datasets . get_rdataset ( \"Guerry\" , \"HistData\" ) . data dat . head () Out[5]: dept Region Department Crime_pers Crime_prop Literacy Donations Infants Suicides MainCity ... Crime_parents Infanticide Donation_clergy Lottery Desertion Instruction Prostitutes Distance Area Pop1831 0 1 E Ain 28870 15890 37 5098 33120 35039 2:Med ... 71 60 69 41 55 46 13 218.372 5762 346.03 1 2 N Aisne 26226 5521 51 8901 14572 12831 2:Med ... 4 82 36 38 82 24 327 65.945 7369 513.00 2 3 C Allier 26747 7925 13 10973 17044 114121 2:Med ... 46 42 76 66 16 85 34 161.927 7340 298.26 3 4 E Basses-Alpes 12935 7289 46 2733 23018 14238 1:Sm ... 70 12 37 80 32 29 2 351.399 6925 155.90 4 5 E Hautes-Alpes 17488 8174 69 6962 23076 16171 1:Sm ... 22 23 64 79 35 7 1 320.280 5549 129.10 5 rows √ó 23 columns In [6]: from pygam import PoissonGAM , s , te from pygam.datasets import chicago from mpl_toolkits.mplot3d import Axes3D X , y = chicago ( return_X_y = True ) gam = PoissonGAM ( s ( 0 , n_splines = 200 ) + te ( 3 , 1 ) + s ( 2 )) . fit ( X , y ) In [7]: XX = gam . generate_X_grid ( term = 1 , meshgrid = True ) Z = gam . partial_dependence ( term = 1 , X = XX , meshgrid = True ) ax = plt . axes ( projection = '3d' ) ax . plot_surface ( XX [ 0 ], XX [ 1 ], Z , cmap = 'viridis' ) Out[7]: In [8]: import pymc3 as pm print ( 'Running PyMC3 v {} ' . format ( pm . __version__ )) # you should see 'Running on PyMC3 v3.8' Running PyMC3 v3.8 In [19]: # making sure you have gap_statistic from gap_statistic import OptimalK 4. Plotting matplotlib and seaborn matplotlib seaborn: statistical data visualization . seaborn works great with pandas . It can also be customized easily. Here is the basic seaborn tutorial: Seaborn tutorial . Plotting a function of 2 variables using contours In optimization, our objective function will often be a function of two or more variables. While it's hard to visualize a function of more than 3 variables, it's very informative to plot one of 2 variables. To do this we use contours. First we define the $x1$ and $x2$ variables and then construct their pairs using meshgrid . In [9]: import seaborn as sn In [10]: x1 = np . linspace ( - 0.1 , 0.1 , 50 ) x2 = np . linspace ( - 0.1 , 0.1 , 100 ) xx , yy = np . meshgrid ( x1 , x2 ) z = np . sqrt ( xx ** 2 + yy ** 2 ) plt . contour ( x1 , x2 , z ); 5. We will be using keras via tensorflow TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. Keras , is a high-level API, created by Fran√ßois Chollet, and used for fast prototyping, advanced research, and production. tf.keras is now maintained by Tensorflow. Exercise 2: Run the following cells to make sure you have the basic libraries to do deep learning In [21]: from __future__ import absolute_import , division , print_function , unicode_literals # TensorFlow and tf.keras import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.regularizers import l2 tf . keras . backend . clear_session () # For easy reset of notebook state. # You should see a >=2.3.0 here! # If you do not, upgrade your env to tensorflow==2.3.0 print ( tf . __version__ ) print ( tf . keras . __version__ ) 2.3.0 2.4.0 In [22]: # Check if your machine has NVIDIA GPUs. hasGPU = tf . config . list_physical_devices () print ( f 'My computer has the following devices: { hasGPU } ' ) My computer has the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook2/"
    }, {
        "title": "Lecture 2: Splines Smoothers and GAMs (part 2)",
        "text": "Exercises Lecture 2: Polynomial Regression (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture02/"
    }, {
        "title": "Lecture 2 - Splines, Smoothers, and GAMs (part 1)",
        "text": "Title : Ex: Polynomial Regression Goal: Create cubic polynomial least-squares regression Description After fitting the model and getting the predictions, you should see the following plot: Hints: Formulas in statsmodels sm.ols numpy vander Refer to lecture notebook. Do not change any other code except the blanks. In [1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt import statsmodels.formula.api as sm % matplotlib inline In [2]: df = pd . read_csv ( 'data1.csv' ) df = df . sort_values ( 'x' ) df . head () Out[2]: x y 73 0.006248 17.329551 142 0.104671 15.268703 34 0.246137 15.394678 17 0.420595 12.743181 5 0.455565 15.480800 In [3]: plt . scatter ( df . x , df . y ); plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . show () Cubic polynomial least-squares regression of y on x In [0]: ### edTest(test_ols_formula) ### def fit_model ( formula ): return sm . ols ( formula = formula , data = df ) . fit () formula = _____ fit2_lm = fit_model ( formula ) In [0]: ### edTest(test_predictions_summary) ### #Get the predictions and the summary dataframe poly_predictions = fit2_lm . ______ () . ___ () poly_predictions In [0]: ax2 = df . plot . scatter ( x = 'x' , y = 'y' , c = 'Red' , title = \"Data with least-squares cubic fit\" ) ax2 . set_xlabel ( \"x\" ) ax2 . set_ylabel ( \"y\" ) # CI for the predection at each x value, i.e. the curve itself ax2 . plot ( df . x , poly_predictions [ 'mean' ], color = \"green\" ) ax2 . plot ( df . x , poly_predictions [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax2 . plot ( df . x , poly_predictions [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); Condition number In [0]: c = np . vander ( _ , _ , increasing = True ) np . linalg . cond ( c )",
        "tags": "Lectures",
        "url": "lectures/lecture02/notebook/"
    }, {
        "title": "Advanced Sections 05:",
        "text": "Slides",
        "tags": "a-section",
        "url": "a-section/a-section05/"
    }, {
        "title": "Advanced Sections 06:",
        "text": "Slides",
        "tags": "a-section",
        "url": "a-section/a-section06/"
    }, {
        "title": "Lecture 1: Splines Smoothers and GAMs (part 1)",
        "text": "Slides Lecture 1 : Intro Lecture 109b 2021 (PDF) Exercises Lecture 1: Smoothing Example (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture01/"
    }, {
        "title": "Lecture 1 - Splines, Smoothers, and GAMs (part 1)",
        "text": "Description Smoothing Example Notebook This notebook provides example code based on the lecture material. If you wish to run or edit the notebook, we recommend downloading it and running it either on your local machine or on JupyterHub. In [1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt import statsmodels.formula.api as sm % matplotlib inline In [2]: diab = pd . read_csv ( \"data/diabetes.csv\" ) print ( \"\"\" # Variables are # subject: subject ID number # age: age diagnosed with diabetes # acidity: a measure of acidity called base deficit # y: natural log of serum C-peptide concentration # Original source is Sockett et al. (1987) # mentioned in Hastie and Tibshirani's book # \"Generalized Additive Models\". \"\"\" ) diab . head () # Variables are # subject: subject ID number # age: age diagnosed with diabetes # acidity: a measure of acidity called base deficit # y: natural log of serum C-peptide concentration # Original source is Sockett et al. (1987) # mentioned in Hastie and Tibshirani's book # \"Generalized Additive Models\". Out[2]: subject age acidity y 0 1 5.2 -8.1 4.8 1 2 8.8 -16.1 4.1 2 3 10.5 -0.9 5.2 3 4 10.6 -7.8 5.5 4 5 10.4 -29.0 5.0 In [3]: diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) plt . xlabel ( \"Age at Diagnosis\" ) plt . ylabel ( \"Log C-Peptide Concentration\" ) plt . show () Linear Regression After a lot of time with SKLearn, we're moving back to Statsmodels because of its fuller implementation of statistical tools, like confidence intervals. We'll also be using statsmodels' powerful formula interface. It lets one write complex models succinctly and without building complex design matrices by hand. Below, we write 'y~age' to mean \"the y column is approximately $\\beta_1$ times the age column (plus a constant $\\beta_0$\". We could include more columns, or transformations of the age column, e.g. ' y ~ age + age**2 + acidity' . In [4]: fit1_lm = sm . ols ( 'y~age' , data = diab ) . fit () We create a very dense set of values to predict on. Remember: the point of a model is to provide outputs for a wide variety of inputs. No need to only predict on the training or test values-- the model can predict on anything you ask it to! Further, it's important when working with statsmodels that we make a named data frame - if we only use a numpy array statsmodels won't know it's the 'age' variable. In [5]: xpred = pd . DataFrame ({ \"age\" : np . arange ( 0 , 16.1 , 0.1 )}) Plot the prediction line and confidence intervals In [6]: pred1 = fit1_lm . predict ( xpred ) prediction_output = fit1_lm . get_prediction ( xpred ) . summary_frame () prediction_output . head () Out[6]: mean mean_se mean_ci_lower mean_ci_upper obs_ci_lower obs_ci_upper 0 3.996031 0.244590 3.502071 4.489991 2.600828 5.391235 1 4.004340 0.242324 3.514957 4.493723 2.610750 5.397929 2 4.012648 0.240062 3.527834 4.497463 2.620656 5.404640 3 4.020957 0.237804 3.540703 4.501211 2.630547 5.411367 4 4.029266 0.235550 3.553562 4.504969 2.640421 5.418111 Above, we use fitted_model.get_prediction().summary_frame() to get predictions and confidence intervals. In [7]: ax1 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares linear fit\" ) ax1 . set_xlabel ( \"Age at Diagnosis\" ) ax1 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax1 . plot ( xpred . age , prediction_output [ 'mean' ], color = \"green\" ) # CI for the predection at each x value, i.e. the line itself ax1 . plot ( xpred . age , prediction_output [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax1 . plot ( xpred . age , prediction_output [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); # CIs for where future data will fall #ax1.plot(xpred.age, prediction_output['obs_ci_lower'], color=\"skyblue\",linestyle=\"dashed\") #ax1.plot(xpred.age, prediction_output['obs_ci_upper'], color=\"skyblue\",linestyle=\"dashed\"); Polynomial Regression In sklearn we would use polynomial_features. Here we use vander to build the matrix of transformed inputs. In particular, vander is a numpy function and therefore usable directly in the formula- it saves us from writing out y ~ age + age**2 + age**3 + ... and so on. vander is for Vandermonde. It's a matrix where the first column is $x&#94;0$, the second is $x&#94;1$, the third is $x&#94;2$ and so on. np.vander([6,3,5], 4, increasing=True) = array([[ 1, 6, 36, 216], [ 1, 3, 9, 27], [ 1, 5, 25, 125]]) Since we have a constant column in the matrix, we put a -1 in the formula to drop the additional constant term statsmodels would otherwise insert Note that this is not an orthogonal polynomial basis. Our estimated coefficients will be more sensitive to the data than they need to be. In [8]: # *cubic* polynomial (vander's input is one more than the degree) fit2_lm = sm . ols ( formula = \"y ~ np.vander(age, 4, increasing=True) -1\" , data = diab ) . fit () # the same model written out explicitly fit2_lm_long = sm . ols ( formula = \"y ~ age + np.power(age, 2) + np.power(age, 3)\" , data = diab ) . fit () poly_predictions = fit2_lm . get_prediction ( xpred ) . summary_frame () poly_predictions Out[8]: mean mean_se mean_ci_lower mean_ci_upper obs_ci_lower obs_ci_upper 0 2.740481 0.508197 1.712556 3.768406 1.156238 4.324724 1 2.806326 0.486091 1.823115 3.789538 1.250724 4.361929 2 2.870902 0.464764 1.930827 3.810977 1.342198 4.399606 3 2.934219 0.444219 2.035702 3.832737 1.430714 4.437725 4 2.996291 0.424456 2.137747 3.854835 1.516327 4.476255 ... ... ... ... ... ... ... 156 5.020575 0.362481 4.287387 5.753763 3.609632 6.431518 157 5.031705 0.379499 4.264095 5.799315 3.602572 6.460837 158 5.043414 0.397195 4.240011 5.846817 3.594742 6.492087 159 5.055716 0.415572 4.215142 5.896290 3.586104 6.525328 160 5.068621 0.434633 4.189494 5.947749 3.576622 6.560620 161 rows √ó 6 columns In [9]: ax2 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares cubic fit\" ) ax2 . set_xlabel ( \"Age at Diagnosis\" ) ax2 . set_ylabel ( \"Log C-Peptide Concentration\" ) # CI for the predection at each x value, i.e. the curve itself ax2 . plot ( xpred . age , poly_predictions [ 'mean' ], color = \"green\" ) ax2 . plot ( xpred . age , poly_predictions [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax2 . plot ( xpred . age , poly_predictions [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); # CIs for where future data will fall #ax2.plot(xpred.age, poly_predictions['obs_ci_lower'], color=\"skyblue\",linestyle=\"dashed\") #ax2.plot(xpred.age, poly_predictions['obs_ci_upper'], color=\"skyblue\",linestyle=\"dashed\"); Logistic Regression Statsmodels provides logistic regression via the same formula-based interface. For the sake of doing logistic regression, suppose we have the binary outcome \"Was y above 4?\" In [10]: diab [ 'y_bin' ] = 1 * ( diab [ 'y' ] > 4 ) # multiply by 1 because statsmodels wants 1s and 0s instead of true and false In [11]: logit_model = sm . logit ( \"y_bin ~ age\" , data = diab ) . fit () logit_prediction = logit_model . predict ( xpred ) Optimization terminated successfully. Current function value: 0.353988 Iterations 7 There is no built-in get_predictions for logistic regression. The code provided below can be used as a replacement. Because it was custom-written by your loving TFs, it will handle polynomial models that use vander , but little else. In [12]: from scipy.special import expit import re def get_logit_prediction_intervals ( model , new_data_df ): if type ( new_data_df ) != pd . DataFrame : raise TypeError ( 'new_data_df must be a DataFrame' ) # transform the raw data according to the formula new_data_dict = {} for x in model . params . index : # only presently supports Intercept, a named column, and polynmoials created via np.vander # the trick is finding the correct base column in the raw data if x == \"Intercept\" : new_data_dict [ x ] = np . ones ( new_data_df . shape [ 0 ]) elif x . startswith ( \"np.vander(\" ): try : will = re . match ( r \"np.vander\\((.*), ?(.*)\\)\\[(.*)\\]\" , x ) column , power , index = will . groups () except e : raise ValueError ( \"Couldn't parse formula-derived feature {} \" . format ( x )) new_data_dict [ x ] = np . vander ( new_data_df . loc [:, column ], int ( power ))[:, int ( index )] else : new_data_dict [ x ] = new_data_df . loc [:, x ] new_data = pd . DataFrame ( new_data_dict ) variance_mat = model . cov_params () standard_devs = np . sqrt ( np . sum ( new_data . dot ( variance_mat ) * new_data , axis = 1 )) linear_predictions = new_data . dot ( model . params ) output = pd . DataFrame ({ \"lower\" : expit ( linear_predictions - 1.96 * standard_devs ), \"predicted\" : expit ( linear_predictions ), \"upper\" : expit ( linear_predictions + 1.96 * standard_devs ) }) return output logit_prediction_intervals = get_logit_prediction_intervals ( logit_model , xpred ) logit_prediction_intervals Out[12]: lower predicted upper 0 0.046716 0.230382 0.646460 1 0.049354 0.236818 0.649700 2 0.052128 0.243377 0.652945 3 0.055044 0.250058 0.656198 4 0.058108 0.256860 0.659458 ... ... ... ... 156 0.867631 0.987899 0.999018 157 0.869055 0.988322 0.999074 158 0.870462 0.988729 0.999128 159 0.871853 0.989123 0.999178 160 0.873228 0.989503 0.999225 161 rows √ó 3 columns A pure logistic model In [18]: ax = diab . plot . scatter ( x = 'age' , y = 'y_bin' , c = 'Red' , title = \"Diabetes data with logit-linear fit\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log-odds Log C-Peptide Concentration > 4\" ) ax . plot ( xpred . age , logit_prediction_intervals [ \"predicted\" ], color = \"green\" ) ax . plot ( xpred . age , logit_prediction_intervals [ \"lower\" ], color = \"blue\" , linestyle = \"dashed\" ) ax . plot ( xpred . age , logit_prediction_intervals [ \"upper\" ], color = \"blue\" , linestyle = \"dashed\" ); plt . show () A logistic model wherein the probability is a cubic function of Age In [14]: logit_poly_model = sm . logit ( \"y_bin ~ np.vander(age, 4) - 1\" , data = diab ) . fit () logit_poly_prediction = logit_poly_model . predict ( xpred ) Optimization terminated successfully. Current function value: 0.194005 Iterations 10 In [19]: ax = diab . plot . scatter ( x = 'age' , y = 'y_bin' , c = 'Red' , title = \"Diabetes data with logit-cubic fit\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log-odds Log C-Peptide Concentration > 4\" ) logit_poly_prediction_intervals = get_logit_prediction_intervals ( logit_poly_model , xpred ) ax . plot ( xpred . age , logit_poly_prediction_intervals [ \"predicted\" ], color = \"green\" ) ax . plot ( xpred . age , logit_poly_prediction_intervals [ \"lower\" ], color = \"blue\" , linestyle = \"dashed\" ) ax . plot ( xpred . age , logit_poly_prediction_intervals [ \"upper\" ], color = \"blue\" , linestyle = \"dashed\" ); plt . show () Lo(w)ess Lowess is available in Statsmodels. It takes a fraction of the data that should be used in smoothing each point. Please note that you are not responsible for mastering lowess - this is merely for your edification. In [16]: from statsmodels.nonparametric.smoothers_lowess import lowess as lowess lowess_models = {} for cur_frac in [ . 15 , . 25 , . 7 , 1 ]: lowess_models [ cur_frac ] = lowess ( diab [ 'y' ], diab [ 'age' ], frac = cur_frac ) Note Python's lowess implementation does not have any tool to predict on new data; it only returns the fitted function's value at the training points. We're making up for that by drawing a straight line between consecutive fitted values (scipy's interp1d ). (There are other more sophisticated interpolation techniques, but the ideal approach would be to predict on new points using lowess itself. This is a limitation of the Python implementation, not lowess itself. R, for example, has a much fuller Lowess toolkit) In [17]: from scipy.interpolate import interp1d for cur_frac , cur_model in lowess_models . items (): ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Lowess Fit, Fraction = {} \" . format ( cur_frac )) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) lowess_interpolation = interp1d ( cur_model [:, 0 ], cur_model [:, 1 ], bounds_error = False ) ax . plot ( xpred , lowess_interpolation ( xpred ), color = \"Blue\" ) plt . show () As the fraction of data increases, Lowess moves from high-variance to high-bias In [18]: ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Large variance, low bias smoother\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) lowess_interpolation = interp1d ( lowess_models [ . 15 ][:, 0 ], lowess_models [ . 15 ][:, 1 ], bounds_error = False ) ax . plot ( xpred , lowess_interpolation ( xpred ), color = \"lightgreen\" ) plt . show () ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Low variance, large bias smoother\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) lowess_interpolation = interp1d ( lowess_models [ 1 ][:, 0 ], lowess_models [ 1 ][:, 1 ], bounds_error = False ) ax . plot ( xpred , lowess_interpolation ( xpred ), color = \"lightgreen\" ) plt . show () Splines (via knots) Here, we flash back to OLS and logistic regression. So far, we've made \"design\" matrices by taking powers of the raw data's columns and asking the solver \"how much of each transformed column is there\"? That's fine if there's an a-priori theoretical reason to think the relationship truly is polynomial. But in most cases we can use much richer and better transformations of the raw data The function below is one such better transformation of the raw data. It (depending on the parameters) applies a RELU or truncated cubic to the input data. Let's see what that looks like In [19]: def h ( x , knot , exponent ): output = np . power ( x - knot , exponent ) output [ x <= knot ] = 0 return output Transforming the x values [0,10] with a knot at 4, power 1. Everything below 4 is zeroed out, values above 4 increase at slope 1. (If we had applied \"square the data\" we'd see a parabola below) In [20]: xvals = np . arange ( 0 , 10.1 , 0.1 ) plt . plot ( xvals , h ( xvals , 4 , 1 ), color = \"red\" ) plt . title ( \"Truncated linear basis function with knot at x=4\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$(x-4)_+$\" ) #note the use of TeX in the label plt . show () Transforming the x values [0,10] with a knot at 4, power 3. Again, inputs below 4 are zeroed out, the rest grow cubically with their distance from 4. In [21]: plt . plot ( xvals , h ( xvals , 4 , 3 ), color = \"red\" ) plt . title ( \"Truncated cubic basis function with knot at x=4\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$(x-4)_+&#94;3$\" ) plt . show () The sum of three RELUs with different knots and different coefficients can create a complicated shape, and thus fit complicated data. Below we see $3\\cdot RELU(x,\\text{knot=}2) - 4\\cdot RELU(x,\\text{knot=}5) + 0.5\\cdot RELU(x,\\text{knot=}8)$. Can you see how much the slope changes at each knot (i.e. at 2, 5, and 8)? In [22]: plt . plot ( xvals , 3 * h ( xvals , 2 , 1 ) - 4 * h ( xvals , 5 , 1 ) + 0.5 * h ( xvals , 8 , 1 ), color = \"red\" ) plt . title ( \"Piecewise linear spline with knots at x=2, 5, and 8\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . show () Above, but with a starting slope and intercept (intercept=2, starting slope=1) In [23]: plt . plot ( xvals , 2 + xvals + 3 * h ( xvals , 2 , 1 ) - 4 * h ( xvals , 5 , 1 ) + 0.5 * h ( xvals , 8 , 1 ), color = \"red\" ) plt . title ( \"Piecewise linear spline with knots at x=2, 5, and 8 \\n plus a starting slope and intercept\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . show () Using OLS, we can find optimal coefficients for RELUs with pre-specified knots, just like we can find optimal coefficients for $x&#94;2$ and $x&#94;3$ In [24]: # generate some fake data to fit x = np . arange ( 0.1 , 10 , 9.9 / 100 ) from scipy.stats import norm y = norm . ppf ( x / 10 ) + np . random . normal ( 0 , 0.4 , 100 ) Notice that we can apply the function h(x) directly in the formula, because we defined it earlier in the notebook. In [25]: fitted_spline_model = sm . ols ( 'y~x+h(x,2,1)+h(x,5,1)+h(x,8,1)' , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"3 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , linewidth = 2 , label = \"Spline with knots at 2,5,8\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () More knots In [26]: fitted_spline_model = sm . ols ( 'y~x+h(x,1,1)+h(x,2,1)+h(x,3.5,1)+h(x,5,1)+h(x,6.5,1)+h(x,8,1)' , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"6 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Linear Spline with knots at \\n 1, 2, 3.5, 5, 6.5, 8\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () More knots (writing out all the knots in this formula is getting old...) In [27]: fitted_spline_model = sm . ols ( 'y~x+h(x,1,1)+h(x,2,1)+h(x,3,1)+h(x,4,1)+h(x,5,1)+h(x,6,1)+h(x,7,1)+h(x,8,1)+h(x,9,1)' , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"9 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Linear Spline with 9 knots\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () Using code to write out the formula this time. Comments provided for those who are new to python In [28]: n_knots = 25 # make a list of strings that each look like 'h(x,?,1)' where ? takes the values in np.linspace(0,10,n_knots) components = [ 'h(x, {} ,1)' . format ( x ) for x in np . linspace ( 0 , 10 , n_knots )] # glue all the strings in 'components' together with \" + \" between each formula = ' + ' . join ( components ) # paste a 'y ~ x + ' in front of the above. Now we've got a full formula! final_formula = 'y ~ x + ' + formula final_formula Out[28]: 'y ~ x + h(x,0.0,1) + h(x,0.4166666666666667,1) + h(x,0.8333333333333334,1) + h(x,1.25,1) + h(x,1.6666666666666667,1) + h(x,2.0833333333333335,1) + h(x,2.5,1) + h(x,2.916666666666667,1) + h(x,3.3333333333333335,1) + h(x,3.75,1) + h(x,4.166666666666667,1) + h(x,4.583333333333334,1) + h(x,5.0,1) + h(x,5.416666666666667,1) + h(x,5.833333333333334,1) + h(x,6.25,1) + h(x,6.666666666666667,1) + h(x,7.083333333333334,1) + h(x,7.5,1) + h(x,7.916666666666667,1) + h(x,8.333333333333334,1) + h(x,8.75,1) + h(x,9.166666666666668,1) + h(x,9.583333333333334,1) + h(x,10.0,1)' In [29]: fitted_spline_model = sm . ols ( final_formula , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"25 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Linear Spline with 25 knots\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () Cubic splines, instead of linear. Still using code to write the formula. [Knots at 2,5,8, each one cubic; starting intercept, slope, acceleration, and jerk] In [30]: components = [ 'h(x, {} ,3)' . format ( x ) for x in [ 2 , 5 , 8 ]] formula = ' + ' . join ( components ) final_formula = 'y~x + np.power(x,2) + np.power(x,3) + ' + formula fitted_spline_model = sm . ols ( final_formula , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"3 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Cubic Spline with 3 knots\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () As above, but with more knots (a more flexible fit) In [31]: components = [ 'h(x, {} ,3)' . format ( x ) for x in [ 1 , 2 , 3.5 , 5 , 6.5 , 8 ]] formula = ' + ' . join ( components ) final_formula = 'y~x + np.power(x,2) + np.power(x,3) + ' + formula fitted_spline_model = sm . ols ( final_formula , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"6 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Cubic Spline with 3 knots\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () More knots In [32]: n_knots = 9 components = [ 'h(x, {} ,3)' . format ( x ) for x in np . linspace ( 0 , 10 , n_knots )] formula = ' + ' . join ( components ) final_formula = 'y~x + np.power(x,2) + np.power(x,3) + ' + formula fitted_spline_model = sm . ols ( final_formula , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"9 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Cubic Spline with 9 knots\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () Even more knots. As with the linear splines, this looks like it is overfitting a lot. In [33]: n_knots = 25 components = [ 'h(x, {} ,3)' . format ( x ) for x in np . linspace ( 0 , 10 , n_knots )] formula = ' + ' . join ( components ) final_formula = 'y~x + np.power(x,2) + np.power(x,3) + ' + formula fitted_spline_model = sm . ols ( final_formula , data = { 'x' : x , 'y' : y }) . fit () plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) plt . title ( \"25 knots\" ) plt . xlabel ( \"$x$\" ) plt . ylabel ( \"$y$\" ) plt . plot ( x , fitted_spline_model . predict (), color = \"darkblue\" , label = \"Cubic Spline with 25 knots\" ) plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" , label = \"Truth\" ) plt . legend () plt . show () It turns out that the number of knots matters far more than where they are placed. You can play with the code above to verify if you wish. Smoothing splines Smoothing splines, as described in class, minimize the sum of squared errors subject to a penalty that depends on the wiggliness of the function. The resulting solution is a cubic spline with knots at every data value that is regularized according to a smoothing parameter. We use the csaps library to implement smoothing splines. The smoothing parameter takes on values between 0 and 1, and is the weight attached to the error sum of squares of the weighted average between the error sum of squares and the wiggliness penalty. A smoothing parameter of 0 correspondends to a least-squares fit, and a parameter of 1 corresponds to connecting-the-dots. In [34]: import matplotlib.pyplot as plt from csaps import csaps Read in csaps to carry out smoothing splines. In [35]: for cur_smoothing in [ 0 , 0.2 , 0.4 , 0.6 , 0.8 , 0.95 ]: diab_sort = diab diab_sort [ 'age' ] = diab_sort [ 'age' ] + np . random . normal ( 0 , 0.001 , len ( diab_sort )) # ties not allowed (and the data is already sorted) diab_sort = diab . sort_values ([ 'age' ]) # need to re-sort x = diab_sort [ 'age' ] y = diab_sort [ 'y' ] xs = np . linspace ( min ( x ), max ( x ), len ( x )) ys = csaps ( diab_sort [ 'age' ], diab_sort [ 'y' ], xs , smooth = cur_smoothing ) plt . plot ( x , y , 'o' , xs , ys , '-' ) plt . title ( \"Smoothing spline, smoothing parameter = {} \" . format ( cur_smoothing )) plt . xlabel ( \"Age\" ) plt . ylabel ( \"y\" ) plt . show () We can select the smoothing parameter by CV. In [36]: from sklearn.model_selection import KFold candidate_smoothings = [ 0 , 0.2 , 0.4 , 0.6 , 0.8 , 0.95 ] kf = KFold ( n_splits = 5 , random_state = 47 , shuffle = True ) scores = np . zeros (( 5 , len ( candidate_smoothings ))) for i , ( train_index , test_index ) in enumerate ( kf . split ( diab_sort )): train_df = diab_sort . iloc [ train_index ,:] test_df = diab_sort . iloc [ test_index ,:] for j , cur_smoothing in enumerate ( candidate_smoothings ): train_df_sort = train_df . sort_values ([ 'age' ]) test_df_sort = test_df . sort_values ([ 'age' ]) x = train_df_sort [ 'age' ] y = train_df_sort [ 'y' ] xs = test_df_sort [ 'age' ] ys = csaps ( x , y , xs , smooth = cur_smoothing ) scores [ i , j ] = sum (( test_df [ 'y' ] - ys ) ** 2 ) np . mean ( scores , axis = 0 ) Out[36]: array([3.82575939, 3.56948936, 3.84662234, 4.16899586, 4.6888201 , 5.70534349]) In [37]: best_s = candidate_smoothings [ np . argmin ( np . mean ( scores , axis = 0 ))] x = diab_sort [ 'age' ] y = diab_sort [ 'y' ] xs = np . linspace ( min ( x ), max ( x ), len ( x )) ys = csaps ( diab_sort [ 'age' ], diab_sort [ 'y' ], xs , smooth = best_s ) plt . plot ( x , y , 'o' , xs , ys , '-' ) plt . title ( \"Smoothing spline, optimal smoothing parameter = {} \" . format ( best_s )) plt . xlabel ( \"Age\" ) plt . ylabel ( \"y\" ) plt . show () B-Splines Back to the diabetes data. First, let's see the data In [38]: diab_sort = diab diab_sort [ 'age' ] = diab_sort [ 'age' ] + np . random . normal ( 0 , 0.001 , len ( diab_sort )) # again, ties not allowed diab_sort = diab_sort . sort_values ([ 'age' ]) ax = diab_sort . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares cubic fit\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) plt . show () Get quartiles In [39]: quarts = diab_sort [ 'age' ] . quantile ([ 0.25 , 0.5 , 0.75 ]) . values . reshape ( - 1 ) Build a Bspline model. Call splrep (spline representation) to find the knots and coefficients that smooth the given data, then call BSpline to build something that can predict on given values. Notice that when we're done b_spline_model can be called like a function In [40]: from scipy.interpolate import splrep from scipy.interpolate import BSpline t , c , k = splrep ( diab_sort [ 'age' ] . values , diab_sort [ 'y' ] . values , t = quarts ) b_spline_model = BSpline ( t , c , k ) b_spline_model ( 7 ) Out[40]: array(4.99125149) LSQUnivariateSpline fits splines to data, using user-specified knots In [41]: from scipy.interpolate import LSQUnivariateSpline natural_spline_model = LSQUnivariateSpline ( diab_sort [ 'age' ] . values , diab_sort [ 'y' ] . values , quarts ) In [42]: ax = diab_sort . plot . scatter ( x = 'age' , y = 'y' , c = 'grey' , title = \"Diabetes data\" ) ax . plot ( diab_sort [ 'age' ], b_spline_model ( diab_sort [ 'age' ]), label = \"B-spline, knots at quartiles\" ) plt . legend () plt . show () ax = diab_sort . plot . scatter ( x = 'age' , y = 'y' , c = 'grey' , title = \"Diabetes data\" ) ax . plot ( diab_sort [ 'age' ], natural_spline_model ( diab_sort [ 'age' ]), label = \"Natural Spline, knots at quartiles\" ) plt . legend () plt . show () GAMs Generalized Aditive Models essentially provide spline-like fits when there are multiple input variables. We use the PyGam library, which relies on B-splines (really penalized B-splines, a.k.a. P-splines) as the smoother of choice. Here we work with Kyphosis data, on 81 children who received a corrective spinal surgery. Each row records the child's age, the number of vertebrae operated on, the first vertebrae involved in the operation, and whether the operation was a success or experienced complications. In [43]: from sklearn.model_selection import train_test_split kyphosis = pd . read_csv ( \"data/kyphosis.csv\" ) kyphosis [ \"outcome\" ] = 1 * ( kyphosis [ \"Kyphosis\" ] == \"present\" ) kyph_train , kyph_test = train_test_split ( kyphosis , test_size =. 2 , stratify = kyphosis [ 'outcome' ]) kyph_train . describe () Out[43]: Age Number Start outcome count 64.000000 64.000000 64.000000 64.000000 mean 77.109375 4.140625 11.281250 0.203125 std 58.120869 1.698432 4.926342 0.405505 min 1.000000 2.000000 1.000000 0.000000 25% 19.250000 3.000000 8.750000 0.000000 50% 75.500000 4.000000 13.000000 0.000000 75% 128.500000 5.000000 16.000000 0.000000 max 206.000000 10.000000 17.000000 1.000000 Using pygam is a lot like using the formula interface for statsmodels, but with raw code instead of with strings. Instead of 's(Age)+s(Number)+s(Start)' we have s(0)+s(1)+s(2) (the corresponding column indices). s is for 'smooth'. Each smooth accepts a lam parameter specifying the degree of smoothing. As before, larger lambdas mean smoother curves In [44]: from pygam import LogisticGAM , s X = kyph_train [[ \"Age\" , \"Number\" , \"Start\" ]] y = kyph_train [ \"outcome\" ] kyph_gam = LogisticGAM ( s ( 0 ) + s ( 1 , lam = 0.5 ) + s ( 2 )) . fit ( X , y ) Correct classification rate is pretty good! In [45]: kyph_gam . accuracy ( X , y ) Out[45]: 0.921875 Summary of GAM fit on training data: In [46]: kyph_gam . summary () LogisticGAM =============================================== ========================================================== Distribution: BinomialDist Effective DoF: 13.6787 Link Function: LogitLink Log Likelihood: -12.3667 Number of Samples: 64 AIC: 52.0909 AICc: 61.6165 UBRE: 2.9849 Scale: 1.0 Pseudo R-Squared: 0.6171 ========================================================================================================== Feature Function Lambda Rank EDoF P > x Sig. Code ================================= ==================== ============ ============ ============ ============ s(0) [0.6] 20 8.1 2.46e-01 s(1) [0.5] 20 4.3 1.59e-01 s(2) [0.6] 20 1.2 6.99e-02 . intercept 1 0.0 1.86e-02 * ========================================================================================================== Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem which can cause p-values to appear significant when they are not. WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with known smoothing parameters, but when smoothing parameters have been estimated, the p-values are typically lower than they should be, meaning that the tests reject the null too readily. /home/chris/anaconda3/envs/cs109a/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. Please do not make inferences based on these values! Collaborate on a solution, and stay up to date at: github.com/dswah/pyGAM/issues/163 \"\"\"Entry point for launching an IPython kernel. GAMs provide plots of the effect of increasing each variable (conditional on / adjusted for the other variables) In [47]: res = kyph_gam . deviance_residuals ( X , y ) for i , term in enumerate ( kyph_gam . terms ): if term . isintercept : continue XX = kyph_gam . generate_X_grid ( term = i ) pdep , confi = kyph_gam . partial_dependence ( term = i , X = XX , width = 0.95 ) pdep2 , _ = kyph_gam . partial_dependence ( term = i , X = X , width = 0.95 ) plt . figure () plt . plot ( XX [:, term . feature ], pdep ) plt . plot ( XX [:, term . feature ], confi , c = 'r' , ls = '--' ) plt . title ( X . columns . values [ term . feature ]) plt . show () Above, we see that as age ranges from 1 to 200 the chance of success first increases and then decreases, though with wide error bars. Moreover, we see that the Number of the vertebrae appears to have no effect or a mild effect until it reaches 12, and then drastically lowers. Below, we try a model with only \"Age\" and \"Start\". In just a moment we'll compare the two models via cross-validation. In [48]: X = kyph_train [[ \"Age\" , \"Number\" , \"Start\" ]] y = kyph_train [ \"outcome\" ] small_kyph_gam = LogisticGAM ( s ( 0 ) + s ( 2 )) . fit ( X , y ) res = small_kyph_gam . deviance_residuals ( X , y ) for i , term in enumerate ( small_kyph_gam . terms ): if term . isintercept : continue XX = small_kyph_gam . generate_X_grid ( term = i ) pdep , confi = small_kyph_gam . partial_dependence ( term = i , X = XX , width = 0.95 ) pdep2 , _ = small_kyph_gam . partial_dependence ( term = i , X = X , width = 0.95 ) plt . figure () plt . plot ( XX [:, term . feature ], pdep ) plt . plot ( XX [:, term . feature ], confi , c = 'r' , ls = '--' ) plt . title ( X . columns . values [ term . feature ]) plt . show () In [49]: from sklearn.metrics import accuracy_score X_test = kyph_test [[ \"Age\" , \"Number\" , \"Start\" ]] y_test = kyph_test [ \"outcome\" ] acc = accuracy_score ( y_test , kyph_gam . predict ( X_test )) acc_small = accuracy_score ( y_test , small_kyph_gam . predict ( X_test )) print ( \"Test Accuracy m1: {:0.2f} , m2: {:0.2f} \" . format ( acc , acc_small )) Test Accuracy m1: 0.82, m2: 0.76 We find that the richer model has a higher accuracy on the test set (it gets rid of one false positive present in the smaller model). Depending on taste, this may or may not be enough to declare the larger model the better. A cross-validation (below) would make better use of the small data than a simple train-test split. (We again stratify on the outcome to ensure all test sets have a representative number of kyphosis cases) In [50]: from sklearn.model_selection import StratifiedKFold kf = StratifiedKFold ( n_splits = 5 , random_state = 47 , shuffle = True ) scores = np . zeros (( 5 , 2 )) for i , ( train_index , test_index ) in enumerate ( kf . split ( kyphosis , kyphosis [ 'outcome' ])): train_df = kyphosis . iloc [ train_index ,:] test_df = kyphosis . iloc [ test_index ,:] # with all three (inserting lower smoothing on 'number' to prevent errors while fitting) cur_model_all = LogisticGAM ( s ( 0 ) + s ( 1 , lam =. 5 ) + s ( 2 )) . fit ( train_df [[ 'Age' , 'Number' , 'Start' ]], train_df [ 'outcome' ]) scores [ i , 0 ] = accuracy_score ( test_df [ 'outcome' ], cur_model_all . predict ( test_df [[ 'Age' , 'Number' , 'Start' ]])) # dropping 'number' cur_model_some = LogisticGAM ( s ( 0 ) + s ( 1 )) . fit ( train_df [[ 'Age' , 'Number' , 'Start' ]], train_df [ 'outcome' ]) scores [ i , 1 ] = accuracy_score ( test_df [ 'outcome' ], cur_model_some . predict ( test_df [[ 'Age' , 'Number' , 'Start' ]])) print ( \"Average accuracy\" , np . mean ( scores , axis = 0 )) plt . scatter ([ 0 ] * 5 , scores [:, 0 ]) plt . scatter ([ 1 ] * 5 , scores [:, 1 ]) plt . xlabel ( \"Model\" ) plt . ylabel ( \"CV Accuracy\" ) plt . show () Average accuracy [0.71617647 0.74044118] As a naive average, it appears that the second model (without 'Number') is has the better accuracy. However, plotting the various CV scores shows that the two models aren't particularly different. As the test is inconclusive, one might choose the model without 'number' as it does not appear to be noticeably weaker, or one might stick with the richer model as it does not noticeably overfit.",
        "tags": "Lectures",
        "url": "lectures/lecture01/notebook/"
    }, {
        "title": "Sections 02:",
        "text": "",
        "tags": "sections",
        "url": "sections/section02/"
    }, {
        "title": "Sections 06:",
        "text": "Slides",
        "tags": "section",
        "url": "section/section06/"
    }, {
        "title": "Sections 07:",
        "text": "Slides",
        "tags": "section",
        "url": "section/section07/"
    }, {
        "title": "CS109b: Advanced Topics in Data Science",
        "text": "Spring 2021 Pavlos Protopapas , Mark Glickman , and Chris Tanner Additional Instructor: Eleni Kaxiras Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, transformers, language models, autoencoders, and generative models as well as basic Bayesian methods, nonlinear statistical models, and unsupervised learning. Helpline: cs109b2021@gmail.com Lectures: Mon , Wed , & Fri 9:00‚Äê10:15 am Sections: Fri 10:30 am (starting 3/5) Advanced Sections: Weds 12 pm (starting 3/10) Office Hours: TBD Course material can be viewed in the public GitHub repository . Previous Material 2020 2019 2018",
        "tags": "pages",
        "url": "pages/cs109b-advanced-topics-in-data-science/"
    }, {
        "title": "??",
        "text": "Data Science 2: Advanced Topics in Data Science Section 3: Recurrent Neural Networks Harvard University Spring 2021 Instructors : Mark Glickman, Pavlos Protopapas, and Chris Tanner Authors : Chris Gumb and Eleni Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Learning Objectives By the end of this lab, you should understand: how to perform basic preprocessing on text data the layers used in keras to construct RNNs and its variants (GRU, LSTM) how the model's task (i.e., many-to-1, many-to-many) affects architecture choices Notebook Contents IMDB Review Dataset Preprocessing Text Data Tokenization Padding Numerical Encoding Movie Review Sentiment Analysis Naive FFNN Embedding Layer 1D CNN Vanilla RNN Vanishing/Exploding Gradients GRU LSTM BiDirectional Layer Deep RNNs TimeDistributed Layer RepeatVector Layer CNN + RNN Heavy Metal Lyric Generator Creating Input/Target Pairs LambdaCallback Arithmetic /w RNN In [3]: import tensorflow as tf from tensorflow.keras.datasets import imdb from tensorflow.keras.models import Sequential , Model , load_model from tensorflow.keras.layers import BatchNormalization , Bidirectional , Dense , Embedding , GRU , LSTM , SimpleRNN , \\ Input , TimeDistributed , Dropout , RepeatVector from tensorflow.keras.layers import Conv1D , Conv2D , Flatten , MaxPool1D , MaxPool2D , Lambda from tensorflow.keras.callbacks import EarlyStopping , LambdaCallback , ModelCheckpoint from tensorflow.keras.initializers import Constant from tensorflow.keras.preprocessing import sequence from sklearn.model_selection import train_test_split import tensorflow_datasets from matplotlib import pyplot as plt import numpy as np import pandas as pd import re , sys # fix random seed for reproducibility np . random . seed ( 109 ) Case Study: IMDB Review Classifier Let's frame our discussion of RNNS around the example a text classifier. Specifically, We'll build and evaluate various models that all attempt to descriminate between positive and negative reviews through the Internet Movie Database (IMDB). The dataset is again made available to us through the tensorflow datasets API. In [4]: import tensorflow_datasets In [5]: ( train , test ), info = tensorflow_datasets . load ( 'imdb_reviews' , split = [ 'train' , 'test' ], with_info = True ) The helpful info object provides details about the dataset. In [6]: info Out[6]: tfds.core.DatasetInfo( name='imdb_reviews', full_name='imdb_reviews/plain_text/1.0.0', description=\"\"\" Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. \"\"\", config_description=\"\"\" Plain text \"\"\", homepage='http://ai.stanford.edu/~amaas/data/sentiment/', data_path='/home/10914655/tensorflow_datasets/imdb_reviews/plain_text/1.0.0', download_size=80.23 MiB, dataset_size=129.83 MiB, features=FeaturesDict({ 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2), 'text': Text(shape=(), dtype=tf.string), }), supervised_keys=('text', 'label'), splits={ 'test': , 'train': , 'unsupervised': , }, citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011, author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher}, title = {Learning Word Vectors for Sentiment Analysis}, booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, month = {June}, year = {2011}, address = {Portland, Oregon, USA}, publisher = {Association for Computational Linguistics}, pages = {142--150}, url = {http://www.aclweb.org/anthology/P11-1015} }\"\"\", ) We see that the dataset consists of text reviews and binary good/bad labels. Here are two examples: In [7]: labels = { 0 : 'bad' , 1 : 'good' } seen = { 'bad' : False , 'good' : False } for review in train : label = review [ 'label' ] . numpy () if not seen [ labels [ label ]]: print ( f \"text: \\n { review [ 'text' ] . numpy () . decode () } \\n \" ) print ( f \"label: { labels [ label ] } \\n \" ) seen [ labels [ label ]] = True if all ( val == True for val in seen . values ()): break text: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it. label: bad text: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received. label: good Great! But unfortunately, computers can read! üìñ--ü§ñ‚ùì Preprocessing Text Data Computers have no built-in knowledge of language and cannot understand text data in any rich way that humans do -- at least not without some help! The first crucial step in natural language processing is to clean and preprocess your data so that your algorithms and models can make use of it. We'll look at a few preprocess steps: - tokenization - padding - numerical encoding Depending on your NLP task, you may want to take additional preprocessing steps which we will not cover here. These can include: converting all characters to lowercase treating each punctuation mark as a token (e.g., , . ! ? are each separate tokens) removing punctuation altogether separating each sentence with a unique symbol (e.g., and ) removing words that are incredibly common (e.g., function words, (in)definite articles). These are referred to as 'stopwords'). Lemmatizing (replacing words with their 'dictionary entry form') Stemming (removing grammatical morphemes) Useful NLP Python libraries such as NLTK and spaCy provide built in methods for many of these preprocessing steps. Tokenization Tokens are the atomic units of meaning which our model will be working with. What should these units be? These could be characters, words, or even sentences. For our movie review classifier we will be working at the word level. For this example we will process just a subset of the original dataset. In [8]: SAMPLE_SIZE = 10 subset = list ( train . take ( SAMPLE_SIZE )) subset [ 0 ] Out[8]: {'label': , 'text': } The TFDS format allows for the construction of efficient preprocessing pipelines. But for our own preprocessing example we will be primarily working with Python list objects. This gives us a chance to practice the Python list comprehension which is a powerful tool to have at your disposal. It will serve you well when processing arbitrary text which may not already be in a nice TFDS format (such as in the HW üòâ). We'll convert our data subset into X and y lists. In [9]: X = [ x [ 'text' ] . numpy () . decode () for x in subset ] y = [ x [ 'label' ] . numpy () for x in subset ] In [10]: print ( f 'X has { len ( X ) } reviews' ) print ( f 'y has { len ( y ) } labels' ) X has 10 reviews y has 10 labels In [11]: N_CHARS = 20 print ( f 'First { N_CHARS } characters of all reviews: \\n { [ x [: 20 ] + \"...\" for x in X ] } \\n ' ) print ( f 'All labels: \\n { y } ' ) First 20 characters of all reviews: ['This was an absolute...', 'I have been known to...', 'Mann photographs the...', 'This is the kind of ...', 'As others have menti...', 'This is a film which...', 'Okay, you have: Each observation in X is a review. A review is a str object which we can think of as a sequence of characters. This is indeed how Python treats strings as made clear by how we are printing 'slices' of each review in the code cell above. We'll see a bit later that you can in fact sucessfully train a neural network on text data at the character level. But for the moment we will work at the word level, treating the word level. This means our observations should be organized as sequences of words rather than sequences of characters. In [12]: # list comprehensions again to the rescue! X = [ x . split () for x in X ] # The same thing can be accomplished with: # list(map(str.split, X)) # but that is much harder to parse! O_o Now let's look at the first 10 tokens in the first 2 reviews. In [13]: X [ 0 ][: 10 ], X [ 1 ][: 10 ] Out[13]: (['This', 'was', 'an', 'absolutely', 'terrible', 'movie.', \"Don't\", 'be', 'lured', 'in'], ['I', 'have', 'been', 'known', 'to', 'fall', 'asleep', 'during', 'films,', 'but']) Padding Let's take a look at the lengths of the reviews in our subset. In [14]: [ len ( x ) for x in X ] Out[14]: [116, 112, 132, 88, 81, 289, 557, 111, 223, 127] If we were training our RNN one sentence at a time, it would be okay to have sentences of varying lengths. However, as with any neural network, it can be sometimes be advantageous to train inputs in batches. When doing so with RNNs, our input tensors need to be of the same length/dimensions. Here are two examples of tokenized reviews padded to have a length of 5. ['I', 'loved', 'it', ' ', ' '] ['It', 'stinks', ' ', ' ', ' '] Now let's pad our own examples. Note that 'padding' in this context also means truncating sequences that are longer than our specified max length. In [15]: MAX_LEN = 500 PAD = ' ' # truncate X = [ x [: MAX_LEN ] for x in X ] # pad for x in X : while len ( x ) < MAX_LEN : x . append ( PAD ) In [16]: [ len ( x ) for x in X ] Out[16]: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500] Now all reviews are of a uniform length! Numerical Encoding If each review in our dataset is an observation, then the features of each observation are the tokens, in this case, words. But these words are still strings. Our machine learning methods require us to be able to multiple our features by weights. If we want to use these words as inputs for a neural network we'll have to convert them into some numerical representation. One solution is to create a one-to-one mapping between unique words and integers. If the five sentences below were our entire corpus, our conversion would look this: i have books - [1, 4, 2] interesting books are useful [11,2,9,8] i have computers [1,4,3] computers are interesting and useful [3,5,11,10,8] books and computers are both valuable. [2,10,3,9,13,12] bye bye [7,7] I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13 To accomplish this we'll first need to know what all the unique words are in our dataset. In [17]: all_tokens = [ word for review in X for word in review ] In [18]: # sanity check len ( all_tokens ), sum ([ len ( x ) for x in X ]) Out[18]: (5000, 5000) Casting our list of words into a set is a great way to get all the unique words in the data. In [19]: vocab = sorted ( set ( all_tokens )) print ( 'Unique Words:' , len ( vocab )) Unique Words: 892 Now we need to create a mapping from words to integers. For this will a dictionary comprehension . In [20]: word2idx = { word : idx for idx , word in enumerate ( vocab )} In [21]: word2idx Out[21]: {'\"Absolute': 0, '\"Bohlen\"-Fan': 1, '\"Brideshead': 2, '\"Candy\"?).': 3, '\"City': 4, '\"Dieter': 5, '\"Dieter\"': 6, '\"Dragonfly\"': 7, '\"I\\'ve': 8, '\"Lady.\" Ah,': 43, '/>And': 44, '/>But': 45, '/>Canadian': 46, '/>David': 47, '/>First': 48, '/>Henceforth,': 49, '/>Joanna': 50, '/>Journalist': 51, '/>Nothing': 52, '/>OK,': 53, '/>Penelope': 54, '/>Peter': 55, '/>Second': 56, '/>So': 57, '/>Susan': 58, '/>Thank': 59, '/>Third': 60, '/>To': 61, '/>When': 62, '/>Wrong!': 63, '/>and': 64, '1-dimensional': 65, '14': 66, '1950s': 67, '20': 68, ' ': 69, ' We repeat the process, this time mapping integers to words. In [22]: idx2word = { idx : word for idx , word in enumerate ( vocab )} In [23]: idx2word Out[23]: {0: '\"Absolute', 1: '\"Bohlen\"-Fan', 2: '\"Brideshead', 3: '\"Candy\"?).', 4: '\"City', 5: '\"Dieter', 6: '\"Dieter\"', 7: '\"Dragonfly\"', 8: '\"I\\'ve', 9: '\"Lady.\" Ah,', 44: '/>And', 45: '/>But', 46: '/>Canadian', 47: '/>David', 48: '/>First', 49: '/>Henceforth,', 50: '/>Joanna', 51: '/>Journalist', 52: '/>Nothing', 53: '/>OK,', 54: '/>Penelope', 55: '/>Peter', 56: '/>Second', 57: '/>So', 58: '/>Susan', 59: '/>Thank', 60: '/>Third', 61: '/>To', 62: '/>When', 63: '/>Wrong!', 64: '/>and', 65: '1-dimensional', 66: '14', 67: '1950s', 68: '20', 69: ' ', 70: ' Now, perform the mapping to encode the observations in our subset. Note the use of nested list comprehensions ! In [24]: X_proc = [[ word2idx [ word ] for word in review ] for review in X ] X_proc [ 0 ][: 10 ], X_proc [ 1 ][: 10 ] Out[24]: ([211, 851, 272, 233, 793, 587, 109, 303, 557, 517], [131, 495, 308, 536, 819, 436, 289, 406, 449, 327]) X_proc is a list of lists but if we are going to feed it into a keras model we should convert both it and y into numpy arrays. In [25]: X_proc = np . hstack ( X_proc ) . reshape ( - 1 , MAX_LEN ) y = np . array ( y ) X_proc , y Out[25]: (array([[211, 851, 272, ..., 69, 69, 69], [131, 495, 308, ..., 69, 69, 69], [160, 649, 799, ..., 69, 69, 69], ..., [206, 445, 525, ..., 69, 69, 69], [131, 687, 552, ..., 69, 69, 69], [201, 810, 622, ..., 69, 69, 69]]), array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0])) Now, just to prove that we've successfully processed the data, we perform a test train split and feed it into a FNN. In [26]: X_train , X_test , y_train , y_test = train_test_split ( X_proc , y , test_size = 0.2 , stratify = y ) In [27]: model = Sequential () model . add ( Dense ( 8 , activation = 'relu' , input_dim = MAX_LEN )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 5 , batch_size = 2 , verbose = 2 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 8) 4008 _________________________________________________________________ dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 4,017 Trainable params: 4,017 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/5 4/4 - 2s - loss: 187.6442 - accuracy: 0.2500 - val_loss: 149.4720 - val_accuracy: 0.5000 Epoch 2/5 4/4 - 0s - loss: 16.7689 - accuracy: 0.7500 - val_loss: 332.2443 - val_accuracy: 0.5000 Epoch 3/5 4/4 - 0s - loss: 21.6830 - accuracy: 0.7500 - val_loss: 360.5525 - val_accuracy: 0.5000 Epoch 4/5 4/4 - 0s - loss: 11.4073 - accuracy: 0.7500 - val_loss: 362.2109 - val_accuracy: 0.5000 Epoch 5/5 4/4 - 0s - loss: 1.7824e-10 - accuracy: 1.0000 - val_loss: 346.8892 - val_accuracy: 0.5000 Accuracy: 50.00% It worked! But our subset was very small so we shouldn't get excited about the results above. The IMDB dataset is very popular so keras also includes an alternative method for loading the data. This method can save us a lot of time for many reasons: Cleaned text with less meaningless punctuation Pre-tokenized and numerically encoded Allows us to specify maximum vocabulary size In [29]: from tensorflow.keras.datasets import imdb import warnings warnings . filterwarnings ( 'ignore' ) In [30]: # We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small MAX_VOCAB = 10000 INDEX_FROM = 3 # word index offset ( X_train , y_train ), ( X_test , y_test ) = imdb . load_data ( num_words = MAX_VOCAB , index_from = INDEX_FROM ) get_word_index will load a json object we can store in a dictionary. This gives us the word-to-integer mapping. In [31]: word2idx = imdb . get_word_index ( path = 'imdb_word_index.json' ) word2idx = { k :( v + INDEX_FROM ) for k , v in word2idx . items ()} word2idx [ \" \" ] = 0 word2idx [ \" \" ] = 1 word2idx [ \" \" ] = 2 word2idx [ \" \" ] = 3 word2idx Out[31]: {'fawn': 34704, 'tsukino': 52009, 'nunnery': 52010, 'sonja': 16819, 'vani': 63954, 'woods': 1411, 'spiders': 16118, 'hanging': 2348, 'woody': 2292, 'trawling': 52011, \"hold's\": 52012, 'comically': 11310, 'localized': 40833, 'disobeying': 30571, \"'royale\": 52013, \"harpo's\": 40834, 'canet': 52014, 'aileen': 19316, 'acurately': 52015, \"diplomat's\": 52016, 'rickman': 25245, 'arranged': 6749, 'rumbustious': 52017, 'familiarness': 52018, \"spider'\": 52019, 'hahahah': 68807, \"wood'\": 52020, 'transvestism': 40836, \"hangin'\": 34705, 'bringing': 2341, 'seamier': 40837, 'wooded': 34706, 'bravora': 52021, 'grueling': 16820, 'wooden': 1639, 'wednesday': 16821, \"'prix\": 52022, 'altagracia': 34707, 'circuitry': 52023, 'crotch': 11588, 'busybody': 57769, \"tart'n'tangy\": 52024, 'burgade': 14132, 'thrace': 52026, \"tom's\": 11041, 'snuggles': 52028, 'francesco': 29117, 'complainers': 52030, 'templarios': 52128, '272': 40838, '273': 52031, 'zaniacs': 52133, '275': 34709, 'consenting': 27634, 'snuggled': 40839, 'inanimate': 15495, 'uality': 52033, 'bronte': 11929, 'errors': 4013, 'dialogs': 3233, \"yomada's\": 52034, \"madman's\": 34710, 'dialoge': 30588, 'usenet': 52036, 'videodrome': 40840, \"kid'\": 26341, 'pawed': 52037, \"'girlfriend'\": 30572, \"'pleasure\": 52038, \"'reloaded'\": 52039, \"kazakos'\": 40842, 'rocque': 52040, 'mailings': 52041, 'brainwashed': 11930, 'mcanally': 16822, \"tom''\": 52042, 'kurupt': 25246, 'affiliated': 21908, 'babaganoosh': 52043, \"noe's\": 40843, 'quart': 40844, 'kids': 362, 'uplifting': 5037, 'controversy': 7096, 'kida': 21909, 'kidd': 23382, \"error'\": 52044, 'neurologist': 52045, 'spotty': 18513, 'cobblers': 30573, 'projection': 9881, 'fastforwarding': 40845, 'sters': 52046, \"eggar's\": 52047, 'etherything': 52048, 'gateshead': 40846, 'airball': 34711, 'unsinkable': 25247, 'stern': 7183, \"cervi's\": 52049, 'dnd': 40847, 'dna': 11589, 'insecurity': 20601, \"'reboot'\": 52050, 'trelkovsky': 11040, 'jaekel': 52051, 'sidebars': 52052, \"sforza's\": 52053, 'distortions': 17636, 'mutinies': 52054, 'sermons': 30605, '7ft': 40849, 'boobage': 52055, \"o'bannon's\": 52056, 'populations': 23383, 'chulak': 52057, 'mesmerize': 27636, 'quinnell': 52058, 'yahoo': 10310, 'meteorologist': 52060, 'beswick': 42580, 'boorman': 15496, 'voicework': 40850, \"ster'\": 52061, 'blustering': 22925, 'hj': 52062, 'intake': 27637, 'morally': 5624, 'jumbling': 40852, 'bowersock': 52063, \"'porky's'\": 52064, 'gershon': 16824, 'ludicrosity': 40853, 'coprophilia': 52065, 'expressively': 40854, \"india's\": 19503, \"post's\": 34713, 'wana': 52066, 'wang': 5286, 'wand': 30574, 'wane': 25248, 'edgeways': 52324, 'titanium': 34714, 'pinta': 40855, 'want': 181, 'pinto': 30575, 'whoopdedoodles': 52068, 'tchaikovsky': 21911, 'travel': 2106, \"'victory'\": 52069, 'copious': 11931, 'gouge': 22436, \"chapters'\": 52070, 'barbra': 6705, 'uselessness': 30576, \"wan'\": 52071, 'assimilated': 27638, 'petiot': 16119, 'most\\x85and': 52072, 'dinosaurs': 3933, 'wrong': 355, 'seda': 52073, 'stollen': 52074, 'sentencing': 34715, 'ouroboros': 40856, 'assimilates': 40857, 'colorfully': 40858, 'glenne': 27639, 'dongen': 52075, 'subplots': 4763, 'kiloton': 52076, 'chandon': 23384, \"effect'\": 34716, 'snugly': 27640, 'kuei': 40859, 'welcomed': 9095, 'dishonor': 30074, 'concurrence': 52078, 'stoicism': 23385, \"guys'\": 14899, \"beroemd'\": 52080, 'butcher': 6706, \"melfi's\": 40860, 'aargh': 30626, 'playhouse': 20602, 'wickedly': 11311, 'fit': 1183, 'labratory': 52081, 'lifeline': 40862, 'screaming': 1930, 'fix': 4290, 'cineliterate': 52082, 'fic': 52083, 'fia': 52084, 'fig': 34717, 'fmvs': 52085, 'fie': 52086, 'reentered': 52087, 'fin': 30577, 'doctresses': 52088, 'fil': 52089, 'zucker': 12609, 'ached': 31934, 'counsil': 52091, 'paterfamilias': 52092, 'songwriter': 13888, 'shivam': 34718, 'hurting': 9657, 'effects': 302, 'slauther': 52093, \"'flame'\": 52094, 'sommerset': 52095, 'interwhined': 52096, 'whacking': 27641, 'bartok': 52097, 'barton': 8778, 'frewer': 21912, \"fi'\": 52098, 'ingrid': 6195, 'stribor': 30578, 'approporiately': 52099, 'wobblyhand': 52100, 'tantalisingly': 52101, 'ankylosaurus': 52102, 'parasites': 17637, 'childen': 52103, \"jenkins'\": 52104, 'metafiction': 52105, 'golem': 17638, 'indiscretion': 40863, \"reeves'\": 23386, \"inamorata's\": 57784, 'brittannica': 52107, 'adapt': 7919, \"russo's\": 30579, 'guitarists': 48249, 'abbott': 10556, 'abbots': 40864, 'lanisha': 17652, 'magickal': 40866, 'mattter': 52108, \"'willy\": 52109, 'pumpkins': 34719, 'stuntpeople': 52110, 'estimate': 30580, 'ugghhh': 40867, 'gameplay': 11312, \"wern't\": 52111, \"n'sync\": 40868, 'sickeningly': 16120, 'chiara': 40869, 'disturbed': 4014, 'portmanteau': 40870, 'ineffectively': 52112, \"duchonvey's\": 82146, \"nasty'\": 37522, 'purpose': 1288, 'lazers': 52115, 'lightened': 28108, 'kaliganj': 52116, 'popularism': 52117, \"damme's\": 18514, 'stylistics': 30581, 'mindgaming': 52118, 'spoilerish': 46452, \"'corny'\": 52120, 'boerner': 34721, 'olds': 6795, 'bakelite': 52121, 'renovated': 27642, 'forrester': 27643, \"lumiere's\": 52122, 'gaskets': 52027, 'needed': 887, 'smight': 34722, 'master': 1300, \"edie's\": 25908, 'seeber': 40871, 'hiya': 52123, 'fuzziness': 52124, 'genesis': 14900, 'rewards': 12610, 'enthrall': 30582, \"'about\": 40872, \"recollection's\": 52125, 'mutilated': 11042, 'fatherlands': 52126, \"fischer's\": 52127, 'positively': 5402, '270': 34708, 'ahmed': 34723, 'zatoichi': 9839, 'bannister': 13889, 'anniversaries': 52130, \"helm's\": 30583, \"'work'\": 52131, 'exclaimed': 34724, \"'unfunny'\": 52132, '274': 52032, 'feeling': 547, \"wanda's\": 52134, 'dolan': 33269, '278': 52136, 'peacoat': 52137, 'brawny': 40873, 'mishra': 40874, 'worlders': 40875, 'protags': 52138, 'skullcap': 52139, 'dastagir': 57599, 'affairs': 5625, 'wholesome': 7802, 'hymen': 52140, 'paramedics': 25249, 'unpersons': 52141, 'heavyarms': 52142, 'affaire': 52143, 'coulisses': 52144, 'hymer': 40876, 'kremlin': 52145, 'shipments': 30584, 'pixilated': 52146, \"'00s\": 30585, 'diminishing': 18515, 'cinematic': 1360, 'resonates': 14901, 'simplify': 40877, \"nature'\": 40878, 'temptresses': 40879, 'reverence': 16825, 'resonated': 19505, 'dailey': 34725, '2\\x85': 52147, 'treize': 27644, 'majo': 52148, 'kiya': 21913, 'woolnough': 52149, 'thanatos': 39800, 'sandoval': 35734, 'dorama': 40882, \"o'shaughnessy\": 52150, 'tech': 4991, 'fugitives': 32021, 'teck': 30586, \"'e'\": 76128, 'doesn't': 40884, 'purged': 52152, 'saying': 660, \"martians'\": 41098, 'norliss': 23421, 'dickey': 27645, 'dicker': 52155, \"'sependipity\": 52156, 'padded': 8425, 'ordell': 57795, \"sturges'\": 40885, 'independentcritics': 52157, 'tempted': 5748, \"atkinson's\": 34727, 'hounded': 25250, 'apace': 52158, 'clicked': 15497, \"'humor'\": 30587, \"martino's\": 17180, \"'supporting\": 52159, 'warmongering': 52035, \"zemeckis's\": 34728, 'lube': 21914, 'shocky': 52160, 'plate': 7479, 'plata': 40886, 'sturgess': 40887, \"nerds'\": 40888, 'plato': 20603, 'plath': 34729, 'platt': 40889, 'mcnab': 52162, 'clumsiness': 27646, 'altogether': 3902, 'massacring': 42587, 'bicenntinial': 52163, 'skaal': 40890, 'droning': 14363, 'lds': 8779, 'jaguar': 21915, \"cale's\": 34730, 'nicely': 1780, 'mummy': 4591, \"lot's\": 18516, 'patch': 10089, 'kerkhof': 50205, \"leader's\": 52164, \"'movie\": 27647, 'uncomfirmed': 52165, 'heirloom': 40891, 'wrangle': 47363, 'emotion\\x85': 52166, \"'stargate'\": 52167, 'pinoy': 40892, 'conchatta': 40893, 'broeke': 41131, 'advisedly': 40894, \"barker's\": 17639, 'descours': 52169, 'lots': 775, 'lotr': 9262, 'irs': 9882, 'lott': 52170, 'xvi': 40895, 'irk': 34731, 'irl': 52171, 'ira': 6890, 'belzer': 21916, 'irc': 52172, 'ire': 27648, 'requisites': 40896, 'discipline': 7696, 'lyoko': 52964, 'extend': 11313, 'nature': 876, \"'dickie'\": 52173, 'optimist': 40897, 'lapping': 30589, 'superficial': 3903, 'vestment': 52174, 'extent': 2826, 'tendons': 52175, \"heller's\": 52176, 'quagmires': 52177, 'miyako': 52178, 'moocow': 20604, \"coles'\": 52179, 'lookit': 40898, 'ravenously': 52180, 'levitating': 40899, 'perfunctorily': 52181, 'lookin': 30590, \"lot'\": 40901, 'lookie': 52182, 'fearlessly': 34873, 'libyan': 52184, 'fondles': 40902, 'gopher': 35717, 'wearying': 40904, \"nz's\": 52185, 'minuses': 27649, 'puposelessly': 52186, 'shandling': 52187, 'decapitates': 31271, 'humming': 11932, \"'nother\": 40905, 'smackdown': 21917, 'underdone': 30591, 'frf': 40906, 'triviality': 52188, 'fro': 25251, 'bothers': 8780, \"'kensington\": 52189, 'much': 76, 'muco': 34733, 'wiseguy': 22618, \"richie's\": 27651, 'tonino': 40907, 'unleavened': 52190, 'fry': 11590, \"'tv'\": 40908, 'toning': 40909, 'obese': 14364, 'sensationalized': 30592, 'spiv': 40910, 'spit': 6262, 'arkin': 7367, 'charleton': 21918, 'jeon': 16826, 'boardroom': 21919, 'doubts': 4992, 'spin': 3087, 'hepo': 53086, 'wildcat': 27652, 'venoms': 10587, 'misconstrues': 52194, 'mesmerising': 18517, 'misconstrued': 40911, 'rescinds': 52195, 'prostrate': 52196, 'majid': 40912, 'climbed': 16482, 'canoeing': 34734, 'majin': 52198, 'animie': 57807, 'sylke': 40913, 'conditioned': 14902, 'waddell': 40914, '3\\x85': 52199, 'hyperdrive': 41191, 'conditioner': 34735, 'bricklayer': 53156, 'hong': 2579, 'memoriam': 52201, 'inventively': 30595, \"levant's\": 25252, 'portobello': 20641, 'remand': 52203, 'mummified': 19507, 'honk': 27653, 'spews': 19508, 'visitations': 40915, 'mummifies': 52204, 'cavanaugh': 25253, 'zeon': 23388, \"jungle's\": 40916, 'viertel': 34736, 'frenchmen': 27654, 'torpedoes': 52205, 'schlessinger': 52206, 'torpedoed': 34737, 'blister': 69879, 'cinefest': 52207, 'furlough': 34738, 'mainsequence': 52208, 'mentors': 40917, 'academic': 9097, 'stillness': 20605, 'academia': 40918, 'lonelier': 52209, 'nibby': 52210, \"losers'\": 52211, 'cineastes': 40919, 'corporate': 4452, 'massaging': 40920, 'bellow': 30596, 'absurdities': 19509, 'expetations': 53244, 'nyfiken': 40921, 'mehras': 75641, 'lasse': 52212, 'visability': 52213, 'militarily': 33949, \"elder'\": 52214, 'gainsbourg': 19026, 'hah': 20606, 'hai': 13423, 'haj': 34739, 'hak': 25254, 'hal': 4314, 'ham': 4895, 'duffer': 53262, 'haa': 52216, 'had': 69, 'advancement': 11933, 'hag': 16828, \"hand'\": 25255, 'hay': 13424, 'mcnamara': 20607, \"mozart's\": 52217, 'duffel': 30734, 'haq': 30597, 'har': 13890, 'has': 47, 'hat': 2404, 'hav': 40922, 'haw': 30598, 'figtings': 52218, 'elders': 15498, 'underpanted': 52219, 'pninson': 52220, 'unequivocally': 27655, \"barbara's\": 23676, \"bello'\": 52222, 'indicative': 13000, 'yawnfest': 40923, 'hexploitation': 52223, \"loder's\": 52224, 'sleuthing': 27656, \"justin's\": 32625, \"'ball\": 52225, \"'summer\": 52226, \"'demons'\": 34938, \"mormon's\": 52228, \"laughton's\": 34740, 'debell': 52229, 'shipyard': 39727, 'unabashedly': 30600, 'disks': 40404, 'crowd': 2293, 'crowe': 10090, \"vancouver's\": 56437, 'mosques': 34741, 'crown': 6630, 'culpas': 52230, 'crows': 27657, 'surrell': 53347, 'flowless': 52232, 'sheirk': 52233, \"'three\": 40926, \"peterson'\": 52234, 'ooverall': 52235, 'perchance': 40927, 'bottom': 1324, 'chabert': 53366, 'sneha': 52236, 'inhuman': 13891, 'ichii': 52237, 'ursla': 52238, 'completly': 30601, 'moviedom': 40928, 'raddick': 52239, 'brundage': 51998, 'brigades': 40929, 'starring': 1184, \"'goal'\": 52240, 'caskets': 52241, 'willcock': 52242, \"threesome's\": 52243, \"mosque'\": 52244, \"cover's\": 52245, 'spaceships': 17640, 'anomalous': 40930, 'ptsd': 27658, 'shirdan': 52246, 'obscenity': 21965, 'lemmings': 30602, 'duccio': 30603, \"levene's\": 52247, \"'gorby'\": 52248, \"teenager's\": 25258, 'marshall': 5343, 'honeymoon': 9098, 'shoots': 3234, 'despised': 12261, 'okabasho': 52249, 'fabric': 8292, 'cannavale': 18518, 'raped': 3540, \"tutt's\": 52250, 'grasping': 17641, 'despises': 18519, \"thief's\": 40931, 'rapes': 8929, 'raper': 52251, \"eyre'\": 27659, 'walchek': 52252, \"elmo's\": 23389, 'perfumes': 40932, 'spurting': 21921, \"exposition'\\x85\": 52253, 'denoting': 52254, 'thesaurus': 34743, \"shoot'\": 40933, 'bonejack': 49762, 'simpsonian': 52256, 'hebetude': 30604, \"hallow's\": 34744, 'desperation\\x85': 52257, 'incinerator': 34745, 'congratulations': 10311, 'humbled': 52258, \"else's\": 5927, 'trelkovski': 40848, \"rape'\": 52259, \"'chapters'\": 59389, '1600s': 52260, 'martian': 7256, 'nicest': 25259, 'eyred': 52262, 'passenger': 9460, 'disgrace': 6044, 'moderne': 52263, 'barrymore': 5123, 'yankovich': 52264, 'moderns': 40934, 'studliest': 52265, 'bedsheet': 52266, 'decapitation': 14903, 'slurring': 52267, \"'nunsploitation'\": 52268, \"'character'\": 34746, 'cambodia': 9883, 'rebelious': 52269, 'pasadena': 27660, 'crowne': 40935, \"'bedchamber\": 52270, 'conjectural': 52271, 'appologize': 52272, 'halfassing': 52273, 'paycheque': 57819, 'palms': 20609, \"'islands\": 52274, 'hawked': 40936, 'palme': 21922, 'conservatively': 40937, 'larp': 64010, 'palma': 5561, 'smelling': 21923, 'aragorn': 13001, 'hawker': 52275, 'hawkes': 52276, 'explosions': 3978, 'loren': 8062, \"pyle's\": 52277, 'shootout': 6707, \"mike's\": 18520, \"driscoll's\": 52278, 'cogsworth': 40938, \"britian's\": 52279, 'childs': 34747, \"portrait's\": 52280, 'chain': 3629, 'whoever': 2500, 'puttered': 52281, 'childe': 52282, 'maywether': 52283, 'chair': 3039, \"rance's\": 52284, 'machu': 34748, 'ballet': 4520, 'grapples': 34749, 'summerize': 76155, 'freelance': 30606, \"andrea's\": 52286, '\\x91very': 52287, 'coolidge': 45882, 'mache': 18521, 'balled': 52288, 'grappled': 40940, 'macha': 18522, 'underlining': 21924, 'macho': 5626, 'oversight': 19510, 'machi': 25260, 'verbally': 11314, 'tenacious': 21925, 'windshields': 40941, 'paychecks': 18560, 'jerk': 3399, \"good'\": 11934, 'prancer': 34751, 'prances': 21926, 'olympus': 52289, 'lark': 21927, 'embark': 10788, 'gloomy': 7368, 'jehaan': 52290, 'turaqui': 52291, \"child'\": 20610, 'locked': 2897, 'pranced': 52292, 'exact': 2591, 'unattuned': 52293, 'minute': 786, 'skewed': 16121, 'hodgins': 40943, 'skewer': 34752, 'think\\x85': 52294, 'rosenstein': 38768, 'helmit': 52295, 'wrestlemanias': 34753, 'hindered': 16829, \"martha's\": 30607, 'cheree': 52296, \"pluckin'\": 52297, 'ogles': 40944, 'heavyweight': 11935, 'aada': 82193, 'chopping': 11315, 'strongboy': 61537, 'hegemonic': 41345, 'adorns': 40945, 'xxth': 41349, 'nobuhiro': 34754, 'capit√£es': 52301, 'kavogianni': 52302, 'antwerp': 13425, 'celebrated': 6541, 'roarke': 52303, 'baggins': 40946, 'cheeseburgers': 31273, 'matras': 52304, \"nineties'\": 52305, \"'craig'\": 52306, 'celebrates': 13002, 'unintentionally': 3386, 'drafted': 14365, 'climby': 52307, '303': 52308, 'oldies': 18523, 'climbs': 9099, 'honour': 9658, 'plucking': 34755, '305': 30077, 'address': 5517, 'menjou': 40947, \"'freak'\": 42595, 'dwindling': 19511, 'benson': 9461, 'white's': 52310, 'shamelessness': 40948, 'impacted': 21928, 'upatz': 52311, 'cusack': 3843, \"flavia's\": 37570, 'effette': 52312, 'influx': 34756, 'boooooooo': 52313, 'dimitrova': 52314, 'houseman': 13426, 'bigas': 25262, 'boylen': 52315, 'phillipenes': 52316, 'fakery': 40949, \"grandpa's\": 27661, 'darnell': 27662, 'undergone': 19512, 'handbags': 52318, 'perished': 21929, 'pooped': 37781, 'vigour': 27663, 'opposed': 3630, 'etude': 52319, \"caine's\": 11802, 'doozers': 52320, 'photojournals': 34757, 'perishes': 52321, 'constrains': 34758, 'migenes': 40951, 'consoled': 30608, 'alastair': 16830, 'wvs': 52322, 'ooooooh': 52323, 'approving': 34759, 'consoles': 40952, 'disparagement': 52067, 'futureistic': 52325, 'rebounding': 52326, \"'date\": 52327, 'gregoire': 52328, 'rutherford': 21930, 'americanised': 34760, 'novikov': 82199, 'following': 1045, 'munroe': 34761, \"morita'\": 52329, 'christenssen': 52330, 'oatmeal': 23109, 'fossey': 25263, 'livered': 40953, 'listens': 13003, \"'marci\": 76167, \"otis's\": 52333, 'thanking': 23390, 'maude': 16022, 'extensions': 34762, 'ameteurish': 52335, \"commender's\": 52336, 'agricultural': 27664, 'convincingly': 4521, 'fueled': 17642, 'mahattan': 54017, \"paris's\": 40955, 'vulkan': 52339, 'stapes': 52340, 'odysessy': 52341, 'harmon': 12262, 'surfing': 4255, 'halloran': 23497, 'unbelieveably': 49583, \"'offed'\": 52342, 'quadrant': 30610, 'inhabiting': 19513, 'nebbish': 34763, 'forebears': 40956, 'skirmish': 34764, 'ocassionally': 52343, \"'resist\": 52344, 'impactful': 21931, 'spicier': 52345, 'touristy': 40957, \"'football'\": 52346, 'webpage': 40958, 'exurbia': 52348, 'jucier': 52349, 'professors': 14904, 'structuring': 34765, 'jig': 30611, 'overlord': 40959, 'disconnect': 25264, 'sniffle': 82204, 'slimeball': 40960, 'jia': 40961, 'milked': 16831, 'banjoes': 40962, 'jim': 1240, 'workforces': 52351, 'jip': 52352, 'rotweiller': 52353, 'mundaneness': 34766, \"'ninja'\": 52354, \"dead'\": 11043, \"cipriani's\": 40963, 'modestly': 20611, \"professor'\": 52355, 'shacked': 40964, 'bashful': 34767, 'sorter': 23391, 'overpowering': 16123, 'workmanlike': 18524, 'henpecked': 27665, 'sorted': 18525, \"j≈çb's\": 52357, \"'always\": 52358, \"'baptists\": 34768, 'dreamcatchers': 52359, \"'silence'\": 52360, 'hickory': 21932, 'fun\\x97yet': 52361, 'breakumentary': 52362, 'didn': 15499, 'didi': 52363, 'pealing': 52364, 'dispite': 40965, \"italy's\": 25265, 'instability': 21933, 'quarter': 6542, 'quartet': 12611, 'padm√©': 52365, \"'bleedmedry\": 52366, 'pahalniuk': 52367, 'honduras': 52368, 'bursting': 10789, \"pablo's\": 41468, 'irremediably': 52370, 'presages': 40966, 'bowlegged': 57835, 'dalip': 65186, 'entering': 6263, 'newsradio': 76175, 'presaged': 54153, \"giallo's\": 27666, 'bouyant': 40967, 'amerterish': 52371, 'rajni': 18526, 'leeves': 30613, 'macauley': 34770, 'seriously': 615, 'sugercoma': 52372, 'grimstead': 52373, \"'fairy'\": 52374, 'zenda': 30614, \"'twins'\": 52375, 'realisation': 17643, 'highsmith': 27667, 'raunchy': 7820, 'incentives': 40968, 'flatson': 52377, 'snooker': 35100, 'crazies': 16832, 'crazier': 14905, 'grandma': 7097, 'napunsaktha': 52378, 'workmanship': 30615, 'reisner': 52379, \"sanford's\": 61309, '\\x91do√±a': 52380, 'modest': 6111, \"everything's\": 19156, 'hamer': 40969, \"couldn't'\": 52382, 'quibble': 13004, 'socking': 52383, 'tingler': 21934, 'gutman': 52384, 'lachlan': 40970, 'tableaus': 52385, 'headbanger': 52386, 'spoken': 2850, 'cerebrally': 34771, \"'road\": 23493, 'tableaux': 21935, \"proust's\": 40971, 'periodical': 40972, \"shoveller's\": 52388, 'tamara': 25266, 'affords': 17644, 'concert': 3252, \"yara's\": 87958, 'someome': 52389, 'lingering': 8427, \"abraham's\": 41514, 'beesley': 34772, 'cherbourg': 34773, 'kagan': 28627, 'snatch': 9100, \"miyazaki's\": 9263, 'absorbs': 25267, \"koltai's\": 40973, 'tingled': 64030, 'crossroads': 19514, 'rehab': 16124, 'falworth': 52392, 'sequals': 52393, ...} In [32]: idx2word = { v : k for k , v in word2idx . items ()} idx2word Out[32]: {34704: 'fawn', 52009: 'tsukino', 52010: 'nunnery', 16819: 'sonja', 63954: 'vani', 1411: 'woods', 16118: 'spiders', 2348: 'hanging', 2292: 'woody', 52011: 'trawling', 52012: \"hold's\", 11310: 'comically', 40833: 'localized', 30571: 'disobeying', 52013: \"'royale\", 40834: \"harpo's\", 52014: 'canet', 19316: 'aileen', 52015: 'acurately', 52016: \"diplomat's\", 25245: 'rickman', 6749: 'arranged', 52017: 'rumbustious', 52018: 'familiarness', 52019: \"spider'\", 68807: 'hahahah', 52020: \"wood'\", 40836: 'transvestism', 34705: \"hangin'\", 2341: 'bringing', 40837: 'seamier', 34706: 'wooded', 52021: 'bravora', 16820: 'grueling', 1639: 'wooden', 16821: 'wednesday', 52022: \"'prix\", 34707: 'altagracia', 52023: 'circuitry', 11588: 'crotch', 57769: 'busybody', 52024: \"tart'n'tangy\", 14132: 'burgade', 52026: 'thrace', 11041: \"tom's\", 52028: 'snuggles', 29117: 'francesco', 52030: 'complainers', 52128: 'templarios', 40838: '272', 52031: '273', 52133: 'zaniacs', 34709: '275', 27634: 'consenting', 40839: 'snuggled', 15495: 'inanimate', 52033: 'uality', 11929: 'bronte', 4013: 'errors', 3233: 'dialogs', 52034: \"yomada's\", 34710: \"madman's\", 30588: 'dialoge', 52036: 'usenet', 40840: 'videodrome', 26341: \"kid'\", 52037: 'pawed', 30572: \"'girlfriend'\", 52038: \"'pleasure\", 52039: \"'reloaded'\", 40842: \"kazakos'\", 52040: 'rocque', 52041: 'mailings', 11930: 'brainwashed', 16822: 'mcanally', 52042: \"tom''\", 25246: 'kurupt', 21908: 'affiliated', 52043: 'babaganoosh', 40843: \"noe's\", 40844: 'quart', 362: 'kids', 5037: 'uplifting', 7096: 'controversy', 21909: 'kida', 23382: 'kidd', 52044: \"error'\", 52045: 'neurologist', 18513: 'spotty', 30573: 'cobblers', 9881: 'projection', 40845: 'fastforwarding', 52046: 'sters', 52047: \"eggar's\", 52048: 'etherything', 40846: 'gateshead', 34711: 'airball', 25247: 'unsinkable', 7183: 'stern', 52049: \"cervi's\", 40847: 'dnd', 11589: 'dna', 20601: 'insecurity', 52050: \"'reboot'\", 11040: 'trelkovsky', 52051: 'jaekel', 52052: 'sidebars', 52053: \"sforza's\", 17636: 'distortions', 52054: 'mutinies', 30605: 'sermons', 40849: '7ft', 52055: 'boobage', 52056: \"o'bannon's\", 23383: 'populations', 52057: 'chulak', 27636: 'mesmerize', 52058: 'quinnell', 10310: 'yahoo', 52060: 'meteorologist', 42580: 'beswick', 15496: 'boorman', 40850: 'voicework', 52061: \"ster'\", 22925: 'blustering', 52062: 'hj', 27637: 'intake', 5624: 'morally', 40852: 'jumbling', 52063: 'bowersock', 52064: \"'porky's'\", 16824: 'gershon', 40853: 'ludicrosity', 52065: 'coprophilia', 40854: 'expressively', 19503: \"india's\", 34713: \"post's\", 52066: 'wana', 5286: 'wang', 30574: 'wand', 25248: 'wane', 52324: 'edgeways', 34714: 'titanium', 40855: 'pinta', 181: 'want', 30575: 'pinto', 52068: 'whoopdedoodles', 21911: 'tchaikovsky', 2106: 'travel', 52069: \"'victory'\", 11931: 'copious', 22436: 'gouge', 52070: \"chapters'\", 6705: 'barbra', 30576: 'uselessness', 52071: \"wan'\", 27638: 'assimilated', 16119: 'petiot', 52072: 'most\\x85and', 3933: 'dinosaurs', 355: 'wrong', 52073: 'seda', 52074: 'stollen', 34715: 'sentencing', 40856: 'ouroboros', 40857: 'assimilates', 40858: 'colorfully', 27639: 'glenne', 52075: 'dongen', 4763: 'subplots', 52076: 'kiloton', 23384: 'chandon', 34716: \"effect'\", 27640: 'snugly', 40859: 'kuei', 9095: 'welcomed', 30074: 'dishonor', 52078: 'concurrence', 23385: 'stoicism', 14899: \"guys'\", 52080: \"beroemd'\", 6706: 'butcher', 40860: \"melfi's\", 30626: 'aargh', 20602: 'playhouse', 11311: 'wickedly', 1183: 'fit', 52081: 'labratory', 40862: 'lifeline', 1930: 'screaming', 4290: 'fix', 52082: 'cineliterate', 52083: 'fic', 52084: 'fia', 34717: 'fig', 52085: 'fmvs', 52086: 'fie', 52087: 'reentered', 30577: 'fin', 52088: 'doctresses', 52089: 'fil', 12609: 'zucker', 31934: 'ached', 52091: 'counsil', 52092: 'paterfamilias', 13888: 'songwriter', 34718: 'shivam', 9657: 'hurting', 302: 'effects', 52093: 'slauther', 52094: \"'flame'\", 52095: 'sommerset', 52096: 'interwhined', 27641: 'whacking', 52097: 'bartok', 8778: 'barton', 21912: 'frewer', 52098: \"fi'\", 6195: 'ingrid', 30578: 'stribor', 52099: 'approporiately', 52100: 'wobblyhand', 52101: 'tantalisingly', 52102: 'ankylosaurus', 17637: 'parasites', 52103: 'childen', 52104: \"jenkins'\", 52105: 'metafiction', 17638: 'golem', 40863: 'indiscretion', 23386: \"reeves'\", 57784: \"inamorata's\", 52107: 'brittannica', 7919: 'adapt', 30579: \"russo's\", 48249: 'guitarists', 10556: 'abbott', 40864: 'abbots', 17652: 'lanisha', 40866: 'magickal', 52108: 'mattter', 52109: \"'willy\", 34719: 'pumpkins', 52110: 'stuntpeople', 30580: 'estimate', 40867: 'ugghhh', 11312: 'gameplay', 52111: \"wern't\", 40868: \"n'sync\", 16120: 'sickeningly', 40869: 'chiara', 4014: 'disturbed', 40870: 'portmanteau', 52112: 'ineffectively', 82146: \"duchonvey's\", 37522: \"nasty'\", 1288: 'purpose', 52115: 'lazers', 28108: 'lightened', 52116: 'kaliganj', 52117: 'popularism', 18514: \"damme's\", 30581: 'stylistics', 52118: 'mindgaming', 46452: 'spoilerish', 52120: \"'corny'\", 34721: 'boerner', 6795: 'olds', 52121: 'bakelite', 27642: 'renovated', 27643: 'forrester', 52122: \"lumiere's\", 52027: 'gaskets', 887: 'needed', 34722: 'smight', 1300: 'master', 25908: \"edie's\", 40871: 'seeber', 52123: 'hiya', 52124: 'fuzziness', 14900: 'genesis', 12610: 'rewards', 30582: 'enthrall', 40872: \"'about\", 52125: \"recollection's\", 11042: 'mutilated', 52126: 'fatherlands', 52127: \"fischer's\", 5402: 'positively', 34708: '270', 34723: 'ahmed', 9839: 'zatoichi', 13889: 'bannister', 52130: 'anniversaries', 30583: \"helm's\", 52131: \"'work'\", 34724: 'exclaimed', 52132: \"'unfunny'\", 52032: '274', 547: 'feeling', 52134: \"wanda's\", 33269: 'dolan', 52136: '278', 52137: 'peacoat', 40873: 'brawny', 40874: 'mishra', 40875: 'worlders', 52138: 'protags', 52139: 'skullcap', 57599: 'dastagir', 5625: 'affairs', 7802: 'wholesome', 52140: 'hymen', 25249: 'paramedics', 52141: 'unpersons', 52142: 'heavyarms', 52143: 'affaire', 52144: 'coulisses', 40876: 'hymer', 52145: 'kremlin', 30584: 'shipments', 52146: 'pixilated', 30585: \"'00s\", 18515: 'diminishing', 1360: 'cinematic', 14901: 'resonates', 40877: 'simplify', 40878: \"nature'\", 40879: 'temptresses', 16825: 'reverence', 19505: 'resonated', 34725: 'dailey', 52147: '2\\x85', 27644: 'treize', 52148: 'majo', 21913: 'kiya', 52149: 'woolnough', 39800: 'thanatos', 35734: 'sandoval', 40882: 'dorama', 52150: \"o'shaughnessy\", 4991: 'tech', 32021: 'fugitives', 30586: 'teck', 76128: \"'e'\", 40884: 'doesn't', 52152: 'purged', 660: 'saying', 41098: \"martians'\", 23421: 'norliss', 27645: 'dickey', 52155: 'dicker', 52156: \"'sependipity\", 8425: 'padded', 57795: 'ordell', 40885: \"sturges'\", 52157: 'independentcritics', 5748: 'tempted', 34727: \"atkinson's\", 25250: 'hounded', 52158: 'apace', 15497: 'clicked', 30587: \"'humor'\", 17180: \"martino's\", 52159: \"'supporting\", 52035: 'warmongering', 34728: \"zemeckis's\", 21914: 'lube', 52160: 'shocky', 7479: 'plate', 40886: 'plata', 40887: 'sturgess', 40888: \"nerds'\", 20603: 'plato', 34729: 'plath', 40889: 'platt', 52162: 'mcnab', 27646: 'clumsiness', 3902: 'altogether', 42587: 'massacring', 52163: 'bicenntinial', 40890: 'skaal', 14363: 'droning', 8779: 'lds', 21915: 'jaguar', 34730: \"cale's\", 1780: 'nicely', 4591: 'mummy', 18516: \"lot's\", 10089: 'patch', 50205: 'kerkhof', 52164: \"leader's\", 27647: \"'movie\", 52165: 'uncomfirmed', 40891: 'heirloom', 47363: 'wrangle', 52166: 'emotion\\x85', 52167: \"'stargate'\", 40892: 'pinoy', 40893: 'conchatta', 41131: 'broeke', 40894: 'advisedly', 17639: \"barker's\", 52169: 'descours', 775: 'lots', 9262: 'lotr', 9882: 'irs', 52170: 'lott', 40895: 'xvi', 34731: 'irk', 52171: 'irl', 6890: 'ira', 21916: 'belzer', 52172: 'irc', 27648: 'ire', 40896: 'requisites', 7696: 'discipline', 52964: 'lyoko', 11313: 'extend', 876: 'nature', 52173: \"'dickie'\", 40897: 'optimist', 30589: 'lapping', 3903: 'superficial', 52174: 'vestment', 2826: 'extent', 52175: 'tendons', 52176: \"heller's\", 52177: 'quagmires', 52178: 'miyako', 20604: 'moocow', 52179: \"coles'\", 40898: 'lookit', 52180: 'ravenously', 40899: 'levitating', 52181: 'perfunctorily', 30590: 'lookin', 40901: \"lot'\", 52182: 'lookie', 34873: 'fearlessly', 52184: 'libyan', 40902: 'fondles', 35717: 'gopher', 40904: 'wearying', 52185: \"nz's\", 27649: 'minuses', 52186: 'puposelessly', 52187: 'shandling', 31271: 'decapitates', 11932: 'humming', 40905: \"'nother\", 21917: 'smackdown', 30591: 'underdone', 40906: 'frf', 52188: 'triviality', 25251: 'fro', 8780: 'bothers', 52189: \"'kensington\", 76: 'much', 34733: 'muco', 22618: 'wiseguy', 27651: \"richie's\", 40907: 'tonino', 52190: 'unleavened', 11590: 'fry', 40908: \"'tv'\", 40909: 'toning', 14364: 'obese', 30592: 'sensationalized', 40910: 'spiv', 6262: 'spit', 7367: 'arkin', 21918: 'charleton', 16826: 'jeon', 21919: 'boardroom', 4992: 'doubts', 3087: 'spin', 53086: 'hepo', 27652: 'wildcat', 10587: 'venoms', 52194: 'misconstrues', 18517: 'mesmerising', 40911: 'misconstrued', 52195: 'rescinds', 52196: 'prostrate', 40912: 'majid', 16482: 'climbed', 34734: 'canoeing', 52198: 'majin', 57807: 'animie', 40913: 'sylke', 14902: 'conditioned', 40914: 'waddell', 52199: '3\\x85', 41191: 'hyperdrive', 34735: 'conditioner', 53156: 'bricklayer', 2579: 'hong', 52201: 'memoriam', 30595: 'inventively', 25252: \"levant's\", 20641: 'portobello', 52203: 'remand', 19507: 'mummified', 27653: 'honk', 19508: 'spews', 40915: 'visitations', 52204: 'mummifies', 25253: 'cavanaugh', 23388: 'zeon', 40916: \"jungle's\", 34736: 'viertel', 27654: 'frenchmen', 52205: 'torpedoes', 52206: 'schlessinger', 34737: 'torpedoed', 69879: 'blister', 52207: 'cinefest', 34738: 'furlough', 52208: 'mainsequence', 40917: 'mentors', 9097: 'academic', 20605: 'stillness', 40918: 'academia', 52209: 'lonelier', 52210: 'nibby', 52211: \"losers'\", 40919: 'cineastes', 4452: 'corporate', 40920: 'massaging', 30596: 'bellow', 19509: 'absurdities', 53244: 'expetations', 40921: 'nyfiken', 75641: 'mehras', 52212: 'lasse', 52213: 'visability', 33949: 'militarily', 52214: \"elder'\", 19026: 'gainsbourg', 20606: 'hah', 13423: 'hai', 34739: 'haj', 25254: 'hak', 4314: 'hal', 4895: 'ham', 53262: 'duffer', 52216: 'haa', 69: 'had', 11933: 'advancement', 16828: 'hag', 25255: \"hand'\", 13424: 'hay', 20607: 'mcnamara', 52217: \"mozart's\", 30734: 'duffel', 30597: 'haq', 13890: 'har', 47: 'has', 2404: 'hat', 40922: 'hav', 30598: 'haw', 52218: 'figtings', 15498: 'elders', 52219: 'underpanted', 52220: 'pninson', 27655: 'unequivocally', 23676: \"barbara's\", 52222: \"bello'\", 13000: 'indicative', 40923: 'yawnfest', 52223: 'hexploitation', 52224: \"loder's\", 27656: 'sleuthing', 32625: \"justin's\", 52225: \"'ball\", 52226: \"'summer\", 34938: \"'demons'\", 52228: \"mormon's\", 34740: \"laughton's\", 52229: 'debell', 39727: 'shipyard', 30600: 'unabashedly', 40404: 'disks', 2293: 'crowd', 10090: 'crowe', 56437: \"vancouver's\", 34741: 'mosques', 6630: 'crown', 52230: 'culpas', 27657: 'crows', 53347: 'surrell', 52232: 'flowless', 52233: 'sheirk', 40926: \"'three\", 52234: \"peterson'\", 52235: 'ooverall', 40927: 'perchance', 1324: 'bottom', 53366: 'chabert', 52236: 'sneha', 13891: 'inhuman', 52237: 'ichii', 52238: 'ursla', 30601: 'completly', 40928: 'moviedom', 52239: 'raddick', 51998: 'brundage', 40929: 'brigades', 1184: 'starring', 52240: \"'goal'\", 52241: 'caskets', 52242: 'willcock', 52243: \"threesome's\", 52244: \"mosque'\", 52245: \"cover's\", 17640: 'spaceships', 40930: 'anomalous', 27658: 'ptsd', 52246: 'shirdan', 21965: 'obscenity', 30602: 'lemmings', 30603: 'duccio', 52247: \"levene's\", 52248: \"'gorby'\", 25258: \"teenager's\", 5343: 'marshall', 9098: 'honeymoon', 3234: 'shoots', 12261: 'despised', 52249: 'okabasho', 8292: 'fabric', 18518: 'cannavale', 3540: 'raped', 52250: \"tutt's\", 17641: 'grasping', 18519: 'despises', 40931: \"thief's\", 8929: 'rapes', 52251: 'raper', 27659: \"eyre'\", 52252: 'walchek', 23389: \"elmo's\", 40932: 'perfumes', 21921: 'spurting', 52253: \"exposition'\\x85\", 52254: 'denoting', 34743: 'thesaurus', 40933: \"shoot'\", 49762: 'bonejack', 52256: 'simpsonian', 30604: 'hebetude', 34744: \"hallow's\", 52257: 'desperation\\x85', 34745: 'incinerator', 10311: 'congratulations', 52258: 'humbled', 5927: \"else's\", 40848: 'trelkovski', 52259: \"rape'\", 59389: \"'chapters'\", 52260: '1600s', 7256: 'martian', 25259: 'nicest', 52262: 'eyred', 9460: 'passenger', 6044: 'disgrace', 52263: 'moderne', 5123: 'barrymore', 52264: 'yankovich', 40934: 'moderns', 52265: 'studliest', 52266: 'bedsheet', 14903: 'decapitation', 52267: 'slurring', 52268: \"'nunsploitation'\", 34746: \"'character'\", 9883: 'cambodia', 52269: 'rebelious', 27660: 'pasadena', 40935: 'crowne', 52270: \"'bedchamber\", 52271: 'conjectural', 52272: 'appologize', 52273: 'halfassing', 57819: 'paycheque', 20609: 'palms', 52274: \"'islands\", 40936: 'hawked', 21922: 'palme', 40937: 'conservatively', 64010: 'larp', 5561: 'palma', 21923: 'smelling', 13001: 'aragorn', 52275: 'hawker', 52276: 'hawkes', 3978: 'explosions', 8062: 'loren', 52277: \"pyle's\", 6707: 'shootout', 18520: \"mike's\", 52278: \"driscoll's\", 40938: 'cogsworth', 52279: \"britian's\", 34747: 'childs', 52280: \"portrait's\", 3629: 'chain', 2500: 'whoever', 52281: 'puttered', 52282: 'childe', 52283: 'maywether', 3039: 'chair', 52284: \"rance's\", 34748: 'machu', 4520: 'ballet', 34749: 'grapples', 76155: 'summerize', 30606: 'freelance', 52286: \"andrea's\", 52287: '\\x91very', 45882: 'coolidge', 18521: 'mache', 52288: 'balled', 40940: 'grappled', 18522: 'macha', 21924: 'underlining', 5626: 'macho', 19510: 'oversight', 25260: 'machi', 11314: 'verbally', 21925: 'tenacious', 40941: 'windshields', 18560: 'paychecks', 3399: 'jerk', 11934: \"good'\", 34751: 'prancer', 21926: 'prances', 52289: 'olympus', 21927: 'lark', 10788: 'embark', 7368: 'gloomy', 52290: 'jehaan', 52291: 'turaqui', 20610: \"child'\", 2897: 'locked', 52292: 'pranced', 2591: 'exact', 52293: 'unattuned', 786: 'minute', 16121: 'skewed', 40943: 'hodgins', 34752: 'skewer', 52294: 'think\\x85', 38768: 'rosenstein', 52295: 'helmit', 34753: 'wrestlemanias', 16829: 'hindered', 30607: \"martha's\", 52296: 'cheree', 52297: \"pluckin'\", 40944: 'ogles', 11935: 'heavyweight', 82193: 'aada', 11315: 'chopping', 61537: 'strongboy', 41345: 'hegemonic', 40945: 'adorns', 41349: 'xxth', 34754: 'nobuhiro', 52301: 'capit√£es', 52302: 'kavogianni', 13425: 'antwerp', 6541: 'celebrated', 52303: 'roarke', 40946: 'baggins', 31273: 'cheeseburgers', 52304: 'matras', 52305: \"nineties'\", 52306: \"'craig'\", 13002: 'celebrates', 3386: 'unintentionally', 14365: 'drafted', 52307: 'climby', 52308: '303', 18523: 'oldies', 9099: 'climbs', 9658: 'honour', 34755: 'plucking', 30077: '305', 5517: 'address', 40947: 'menjou', 42595: \"'freak'\", 19511: 'dwindling', 9461: 'benson', 52310: 'white's', 40948: 'shamelessness', 21928: 'impacted', 52311: 'upatz', 3843: 'cusack', 37570: \"flavia's\", 52312: 'effette', 34756: 'influx', 52313: 'boooooooo', 52314: 'dimitrova', 13426: 'houseman', 25262: 'bigas', 52315: 'boylen', 52316: 'phillipenes', 40949: 'fakery', 27661: \"grandpa's\", 27662: 'darnell', 19512: 'undergone', 52318: 'handbags', 21929: 'perished', 37781: 'pooped', 27663: 'vigour', 3630: 'opposed', 52319: 'etude', 11802: \"caine's\", 52320: 'doozers', 34757: 'photojournals', 52321: 'perishes', 34758: 'constrains', 40951: 'migenes', 30608: 'consoled', 16830: 'alastair', 52322: 'wvs', 52323: 'ooooooh', 34759: 'approving', 40952: 'consoles', 52067: 'disparagement', 52325: 'futureistic', 52326: 'rebounding', 52327: \"'date\", 52328: 'gregoire', 21930: 'rutherford', 34760: 'americanised', 82199: 'novikov', 1045: 'following', 34761: 'munroe', 52329: \"morita'\", 52330: 'christenssen', 23109: 'oatmeal', 25263: 'fossey', 40953: 'livered', 13003: 'listens', 76167: \"'marci\", 52333: \"otis's\", 23390: 'thanking', 16022: 'maude', 34762: 'extensions', 52335: 'ameteurish', 52336: \"commender's\", 27664: 'agricultural', 4521: 'convincingly', 17642: 'fueled', 54017: 'mahattan', 40955: \"paris's\", 52339: 'vulkan', 52340: 'stapes', 52341: 'odysessy', 12262: 'harmon', 4255: 'surfing', 23497: 'halloran', 49583: 'unbelieveably', 52342: \"'offed'\", 30610: 'quadrant', 19513: 'inhabiting', 34763: 'nebbish', 40956: 'forebears', 34764: 'skirmish', 52343: 'ocassionally', 52344: \"'resist\", 21931: 'impactful', 52345: 'spicier', 40957: 'touristy', 52346: \"'football'\", 40958: 'webpage', 52348: 'exurbia', 52349: 'jucier', 14904: 'professors', 34765: 'structuring', 30611: 'jig', 40959: 'overlord', 25264: 'disconnect', 82204: 'sniffle', 40960: 'slimeball', 40961: 'jia', 16831: 'milked', 40962: 'banjoes', 1240: 'jim', 52351: 'workforces', 52352: 'jip', 52353: 'rotweiller', 34766: 'mundaneness', 52354: \"'ninja'\", 11043: \"dead'\", 40963: \"cipriani's\", 20611: 'modestly', 52355: \"professor'\", 40964: 'shacked', 34767: 'bashful', 23391: 'sorter', 16123: 'overpowering', 18524: 'workmanlike', 27665: 'henpecked', 18525: 'sorted', 52357: \"j≈çb's\", 52358: \"'always\", 34768: \"'baptists\", 52359: 'dreamcatchers', 52360: \"'silence'\", 21932: 'hickory', 52361: 'fun\\x97yet', 52362: 'breakumentary', 15499: 'didn', 52363: 'didi', 52364: 'pealing', 40965: 'dispite', 25265: \"italy's\", 21933: 'instability', 6542: 'quarter', 12611: 'quartet', 52365: 'padm√©', 52366: \"'bleedmedry\", 52367: 'pahalniuk', 52368: 'honduras', 10789: 'bursting', 41468: \"pablo's\", 52370: 'irremediably', 40966: 'presages', 57835: 'bowlegged', 65186: 'dalip', 6263: 'entering', 76175: 'newsradio', 54153: 'presaged', 27666: \"giallo's\", 40967: 'bouyant', 52371: 'amerterish', 18526: 'rajni', 30613: 'leeves', 34770: 'macauley', 615: 'seriously', 52372: 'sugercoma', 52373: 'grimstead', 52374: \"'fairy'\", 30614: 'zenda', 52375: \"'twins'\", 17643: 'realisation', 27667: 'highsmith', 7820: 'raunchy', 40968: 'incentives', 52377: 'flatson', 35100: 'snooker', 16832: 'crazies', 14905: 'crazier', 7097: 'grandma', 52378: 'napunsaktha', 30615: 'workmanship', 52379: 'reisner', 61309: \"sanford's\", 52380: '\\x91do√±a', 6111: 'modest', 19156: \"everything's\", 40969: 'hamer', 52382: \"couldn't'\", 13004: 'quibble', 52383: 'socking', 21934: 'tingler', 52384: 'gutman', 40970: 'lachlan', 52385: 'tableaus', 52386: 'headbanger', 2850: 'spoken', 34771: 'cerebrally', 23493: \"'road\", 21935: 'tableaux', 40971: \"proust's\", 40972: 'periodical', 52388: \"shoveller's\", 25266: 'tamara', 17644: 'affords', 3252: 'concert', 87958: \"yara's\", 52389: 'someome', 8427: 'lingering', 41514: \"abraham's\", 34772: 'beesley', 34773: 'cherbourg', 28627: 'kagan', 9100: 'snatch', 9263: \"miyazaki's\", 25267: 'absorbs', 40973: \"koltai's\", 64030: 'tingled', 19514: 'crossroads', 16124: 'rehab', 52392: 'falworth', 52393: 'sequals', ...} We can see that the text data is already preprocessed for us. In [33]: print ( 'Number of reviews' , len ( X_train )) print ( 'Length of first and fifth review before padding' , len ( X_train [ 0 ]) , len ( X_train [ 4 ])) print ( 'First review' , X_train [ 0 ]) print ( 'First label' , y_train [ 0 ]) Number of reviews 25000 Length of first and fifth review before padding 218 147 First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] First label 1 Here's an example review using the index-to-word mapping we created from the loaded JSON file to view the a review in its original form. In [34]: def show_review ( x ): review = ' ' . join ([ idx2word [ idx ] for idx in x ]) print ( review ) show_review ( X_train [ 0 ]) this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert is an amazing actor and now the same being director father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also to the two little boy's that played the of norman and paul they were just brilliant children are often left out of the list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all The only thing what isn't done for us is the padding. Looking at the distribution of lengths will help us determine what a reasonable length to pad to will be. In [35]: plt . hist ([ len ( x ) for x in X_train ]) plt . title ( 'review lengths' ); We saw one way of doing this earlier, but Keras actually has a built in pad_sequences helper function. This handles both padding and truncating. By default padding is added to the beginning of a sequence. Q : Why might we want to truncate? Why might we want to pad from the beginning? In [36]: from tensorflow.keras.preprocessing.sequence import pad_sequences In [37]: MAX_LEN = 500 X_train = pad_sequences ( X_train , maxlen = MAX_LEN ) X_test = pad_sequences ( X_test , maxlen = MAX_LEN ) print ( 'Length of first and fifth review after padding' , len ( X_train [ 0 ]) , len ( X_train [ 4 ])) Length of first and fifth review after padding 500 500 Model 1: Naive Feed-Forward Network Let us build a single-layer feed-forward net with a hidden layer of 250 nodes. Each input would be a 500-dim vector of tokens since we padded all our sequences to size 500. Q : How would you calculate the number of parameters in this network? In [40]: model = Sequential ( name = 'Naive_FFNN' ) model . add ( Dense ( 250 , activation = 'relu' , input_dim = MAX_LEN )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 10 , batch_size = 128 , verbose = 2 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"Naive_FFNN\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 250) 125250 _________________________________________________________________ dense_3 (Dense) (None, 1) 251 ================================================================= Total params: 125,501 Trainable params: 125,501 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/10 196/196 - 1s - loss: 178.4060 - accuracy: 0.4996 - val_loss: 91.7812 - val_accuracy: 0.4996 Epoch 2/10 196/196 - 0s - loss: 48.6640 - accuracy: 0.5822 - val_loss: 48.4361 - val_accuracy: 0.5026 Epoch 3/10 196/196 - 0s - loss: 17.7305 - accuracy: 0.6612 - val_loss: 31.7317 - val_accuracy: 0.5022 Epoch 4/10 196/196 - 0s - loss: 7.5028 - accuracy: 0.7264 - val_loss: 21.0285 - val_accuracy: 0.5017 Epoch 5/10 196/196 - 0s - loss: 3.9465 - accuracy: 0.7623 - val_loss: 15.6753 - val_accuracy: 0.5025 Epoch 6/10 196/196 - 0s - loss: 2.2523 - accuracy: 0.7980 - val_loss: 12.4736 - val_accuracy: 0.5039 Epoch 7/10 196/196 - 0s - loss: 1.4916 - accuracy: 0.8150 - val_loss: 10.7774 - val_accuracy: 0.5057 Epoch 8/10 196/196 - 0s - loss: 1.1314 - accuracy: 0.8334 - val_loss: 9.6000 - val_accuracy: 0.5060 Epoch 9/10 196/196 - 0s - loss: 0.8617 - accuracy: 0.8504 - val_loss: 8.9963 - val_accuracy: 0.5055 Epoch 10/10 196/196 - 0s - loss: 0.7458 - accuracy: 0.8602 - val_loss: 8.7728 - val_accuracy: 0.5083 Accuracy: 50.83% Q : Why was the performance so poor? How could we improve our tokenization? Model 2: Feed-Forward Network /w Embeddings One can view the embedding process as a linear projection from one vector space to another. For NLP, we usually use embeddings to project the sparse one-hot encodings of words on to a lower-dimensional continuous space so that the input surface is 'dense' and possibly smooth. Thus, one can view this embedding layer process as just a transformation from $\\mathbb{R}&#94;{inp}$ to $\\mathbb{R}&#94;{emb}$ This not only reduces dimensionality but also allows semantic similarities between tokens to be captured by 'similiarities' between the embedding vectors. This was not possible with one-hot encoding as all vectors there were orthogonal to one another. It is also possible to load pretrained embeddings that were learned from giant corpora. This would be an instance of transfer learning. If you are interested in learning more, start with the astromonically impactful papers of word2vec and GloVe . In Keras we use the Embedding layer: tf.keras.layers.Embedding( input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs ) We'll need to specify the input_dim and output_dim . If working with sequences, as we are, you'll also need to set the input_length . In [42]: EMBED_DIM = 100 model = Sequential ( name = 'FFNN_EMBED' ) model . add ( Embedding ( MAX_VOCAB , EMBED_DIM , input_length = MAX_LEN )) model . add ( Flatten ()) model . add ( Dense ( 250 , activation = 'relu' )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 2 , batch_size = 128 , verbose = 2 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"FFNN_EMBED\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 500, 100) 1000000 _________________________________________________________________ flatten_1 (Flatten) (None, 50000) 0 _________________________________________________________________ dense_6 (Dense) (None, 250) 12500250 _________________________________________________________________ dense_7 (Dense) (None, 1) 251 ================================================================= Total params: 13,500,501 Trainable params: 13,500,501 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/2 196/196 - 6s - loss: 0.6433 - accuracy: 0.6078 - val_loss: 0.3630 - val_accuracy: 0.8497 Epoch 2/2 196/196 - 6s - loss: 0.2349 - accuracy: 0.9025 - val_loss: 0.2977 - val_accuracy: 0.8747 Accuracy: 87.47% Model 3: 1-Dimensional Convolutional Network Text can be thought of as 1-dimensional sequence (a single, long vector) and we can apply 1D Convolutions over a set of word embeddings. More information on convolutions on text data can be found on this blog . If you want to learn more, read this published and well-cited paper from Eleni's friend, Byron Wallace. Q : Why do we use Conv1D if our input, a sequence of word embeddings, is 2D? In [43]: model = Sequential ( name = '1D_CNN' ) model . add ( Embedding ( MAX_VOCAB , EMBED_DIM , input_length = MAX_LEN )) model . add ( Conv1D ( filters = 200 , kernel_size = 3 , padding = 'same' , activation = 'relu' )) model . add ( MaxPool1D ( pool_size = 2 )) model . add ( Flatten ()) model . add ( Dense ( 250 , activation = 'relu' )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , epochs = 2 , batch_size = 128 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"1D_CNN\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_2 (Embedding) (None, 500, 100) 1000000 _________________________________________________________________ conv1d (Conv1D) (None, 500, 200) 60200 _________________________________________________________________ max_pooling1d (MaxPooling1D) (None, 250, 200) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 50000) 0 _________________________________________________________________ dense_8 (Dense) (None, 250) 12500250 _________________________________________________________________ dense_9 (Dense) (None, 1) 251 ================================================================= Total params: 13,560,701 Trainable params: 13,560,701 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/2 196/196 [==============================] - 9s 34ms/step - loss: 0.5958 - accuracy: 0.6403 Epoch 2/2 196/196 [==============================] - 7s 34ms/step - loss: 0.1796 - accuracy: 0.9358 Accuracy: 88.69% Model 4: Simple RNN At a high-level, an RNN is similar to a feed-forward neural network (FFNN) in that there is an input layer, a hidden layer, and an output layer. The input layer is fully connected to the hidden layer, and the hidden layer is fully connected to the output layer. However, the crux of what makes it a recurrent neural network is that the hidden layer for a given time t is not only based on the input layer at time t but also the hidden layer from time t-1 . Here's a popular blog post on The Unreasonable Effectiveness of Recurrent Neural Networks . In Keras, the vanilla RNN unit is implemented the SimpleRNN layer: tf.keras.layers.SimpleRNN( units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs ) As you can see, recurrent layers in Keras take many arguments. We only need to be concerned with units , which specifies the size of the hidden state, and return_sequences , which will be discussed shortly. For the moment is it fine to leave this set to the default of False . Due to the limitations of the vanilla RNN unit (more on that next) it tends not to be used much in practice. For this reason it seems that the Keras developers neglected to implement GPU acceleration for this layer! Notice how much slower the trainig is even for a network with far fewer parameters. In [45]: model = Sequential ( name = 'SimpleRNN' ) model . add ( Embedding ( MAX_VOCAB , EMBED_DIM , input_length = MAX_LEN )) model . add ( SimpleRNN ( 100 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , epochs = 3 , batch_size = 128 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"SimpleRNN\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_3 (Embedding) (None, 500, 100) 1000000 _________________________________________________________________ simple_rnn (SimpleRNN) (None, 100) 20100 _________________________________________________________________ dense_10 (Dense) (None, 1) 101 ================================================================= Total params: 1,020,201 Trainable params: 1,020,201 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/3 196/196 [==============================] - 53s 267ms/step - loss: 0.6720 - accuracy: 0.5660 Epoch 2/3 196/196 [==============================] - 52s 266ms/step - loss: 0.5283 - accuracy: 0.7444 Epoch 3/3 196/196 [==============================] - 52s 265ms/step - loss: 0.3406 - accuracy: 0.8588 Accuracy: 83.13% Vanishing/Exploding Gradients We need to backpropogate through every time step to calculate the gradients used for our weight updates. This requires the use of the chain rule which amounts to repeated multiplications. This can cause two types of problems. First, this product can quickly 'explode,' becoming large, causing destructive updates to the model and numerical overflow. One hack to solve this problem is to clip the gradient at some threshold. Alternatively, the gradient can 'vanish,' getting smaller and smaller as the gradient moves backwards in time. Gradient clipping will not help us here. If we can't propogate gradients suffuciently far back in time then our network will be unable to learn long temporal dependencies. This problem motivates the architecture of the GRU and LSTM units as substitutes for the 'vanilla' RNN. For a more detailed look at the vanishing/exploding gradient problem, please see Marios's excellent Advanced Section . Model 5: GRU $X_{t}$: input $U$, $V$, and $\\beta$: parameter matrices and vector $\\tilde{h_t}$: candidate activation vector $h_{t}$: output vector $R_t$: reset gate $Z_t$: update gate The gates of the GRU allow for the gradients to flow more freely to previous time steps, helping to mitigate the vanishing gradient problem. In Keras, the GRU layer is used in exactly the same way as the SimpleRNN layer. tf.keras.layers.GRU( units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, time_major=False, reset_after=True, **kwargs ) Here we just swap it in to the previous architecture. Note how much faster it trains with GPU excelleration than the simple RNN! In [48]: model = Sequential ( name = 'GRU' ) model . add ( Embedding ( MAX_VOCAB , EMBED_DIM , input_length = MAX_LEN )) model . add ( GRU ( 100 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , epochs = 3 , batch_size = 64 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"GRU\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_6 (Embedding) (None, 500, 100) 1000000 _________________________________________________________________ gru_1 (GRU) (None, 100) 60600 _________________________________________________________________ dense_13 (Dense) (None, 1) 101 ================================================================= Total params: 1,060,701 Trainable params: 1,060,701 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/3 391/391 [==============================] - 13s 30ms/step - loss: 0.5626 - accuracy: 0.6781 Epoch 2/3 391/391 [==============================] - 12s 30ms/step - loss: 0.2510 - accuracy: 0.9011 Epoch 3/3 391/391 [==============================] - 12s 30ms/step - loss: 0.1757 - accuracy: 0.9349 Accuracy: 88.02% Model 6: LSTM The LSTM lacks the GRU's 'short cut' connection (see GRU's $h_t$ above). The LSTM also has a distinct 'cell state' in addition to the hidden state. Futher reading: Understanding LSTM Networks LSTM: A Search Space Odyssey An Empirical Exploration of Recurrent Network Architectures Again, Kera's LSTM works like all the other recurrent layers. tf.keras.layers.LSTM( units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, time_major=False, unroll=False, **kwargs ) In [47]: model = Sequential ( name = 'LSTM' ) model . add ( Embedding ( MAX_VOCAB , EMBED_DIM , input_length = MAX_LEN )) model . add ( LSTM ( 100 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , epochs = 3 , batch_size = 64 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"LSTM\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_5 (Embedding) (None, 500, 100) 1000000 _________________________________________________________________ lstm (LSTM) (None, 100) 80400 _________________________________________________________________ dense_12 (Dense) (None, 1) 101 ================================================================= Total params: 1,080,501 Trainable params: 1,080,501 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/3 391/391 [==============================] - 14s 33ms/step - loss: 0.5209 - accuracy: 0.7265 Epoch 2/3 391/391 [==============================] - 13s 33ms/step - loss: 0.3275 - accuracy: 0.8671 Epoch 3/3 391/391 [==============================] - 13s 33ms/step - loss: 0.2021 - accuracy: 0.9268 Accuracy: 86.39% BiDirectional Layer We may want our model to learn dependencies in either direction. A BiDirectional RNN consists of two separate recurrent units. One processing the sequence from left to right, the other processes that same sequence but in reverse, from right to left. The output of the two units are then merged together (typically concatenated) and feed to the next layer of the network. Creating a Bidirection RNN in Keras is quite simple. We just 'wrap' a recurrent layer in the Bidirectional layer. The default behavior is to concatenate the output from each direction. tf.keras.layers.Bidirectional( layer, merge_mode='concat', weights=None, backward_layer=None, **kwargs ) Example: model = Sequential() ... model.add(Bidirectional(SimpleRNN(n_nodes)) ... Deep RNNs We may want to stack RNN layers one after another. But there is a problem. A recurrent layer expects to be given a sequence as input, and yet we can see that the recurrent layer in each of our models above outputs a single vector. This is because the default behavior of Keras's recurrent layers is to suppress the output until the final time step. If we want to have two recurrent units in a row then the first will have to given an output after each time step, thus providing a sequence to the 2nd recurrent layer. We can have our recurrent layers output at each time step setting return_sequences=True . Example: model = Sequential() ... model.add(LSTM(100, return_sequences=True)) model.add(LSTM(100) ... TimeDistributed Layer TimeDistributed is a 'wrapper' that applies a layer to all time steps of an input sequence. tf.keras.layers.TimeDistributed( layer, **kwargs ) We use TimeDistributed when we want to input a sequence into a layer that doesn't normally expect a time dimension, such as Dense . In [146]: model = Sequential () model . add ( TimeDistributed ( Dense ( 8 ), input_shape = ( 3 , 5 ))) input_array = np . random . randint ( 10 , size = ( 1 , 3 , 5 )) print ( \"Shape of input : \" , input_array . shape ) model . compile ( 'rmsprop' , 'mse' ) output_array = model . predict ( input_array ) print ( \"Shape of output : \" , output_array . shape ) Shape of input : (1, 3, 5) Shape of output : (1, 3, 8) RepeatVector Layer RepeatVector repeats the vector a specified number of times. Dimension changes from (batch_size, number_of_elements) to (batch_size, number_of_repetitions, number_of_elements) This effectively generates a sequence from a single input. In [88]: model = Sequential () model . add ( Dense ( 2 , input_dim = 1 )) model . add ( RepeatVector ( 3 )) model . summary () Model: \"sequential_9\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_37 (Dense) (None, 2) 4 _________________________________________________________________ repeat_vector_5 (RepeatVecto (None, 3, 2) 0 ================================================================= Total params: 4 Trainable params: 4 Non-trainable params: 0 _________________________________________________________________ Model 7: CNN + RNN CNNs are good at learning spatial features, and sentences can be thought of as 1-D spatial vectors (dimensionality is determined by the number of words in the sentence). We can then take the features learned by the CNN (after a maxpooling layer) and feed them into an RNN! We expect the CNN to be able to pick out invariant features across the 1-D spatial structure (i.e., sentence) that characterize good and bad sentiment. This learned spatial features may then be learned as sequences by a reccurent layer. The classification step is then performed by a final dense layer. Exercise: Build a CNN + Deep, BiDirectional GRU Model Let's put together everything we've learned so far. Create a network with: word embeddings in a 100-dimensional space conv layer with 32 filters, kernels of width 3, 'same' padding, and ReLU activate max pooling of size 2 2 bidirectional GRU layers, each with 50 units per direction dense output layer for binary classification In [39]: model = Sequential ( name = 'CNN_GRU' ) # your code here model . add ( Embedding ( MAX_VOCAB , 100 , input_length = MAX_LEN )) model . add ( Conv1D ( filters = 32 , kernel_size = 3 , padding = 'same' , activation = 'relu' )) model . add ( MaxPool1D ( pool_size = 2 )) model . add ( Bidirectional ( GRU ( 50 , return_sequences = True ))) model . add ( Bidirectional ( GRU ( 50 ))) model . add ( Dense ( 1 , activation = 'sigmoid' )) In [40]: model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) model . fit ( X_train , y_train , epochs = 3 , batch_size = 64 ) scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( \"Accuracy: %.2f%% \" % ( scores [ 1 ] * 100 )) Model: \"CNN_GRU\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 500, 100) 1000000 _________________________________________________________________ conv1d (Conv1D) (None, 500, 32) 9632 _________________________________________________________________ max_pooling1d (MaxPooling1D) (None, 250, 32) 0 _________________________________________________________________ bidirectional (Bidirectional (None, 250, 100) 25200 _________________________________________________________________ bidirectional_1 (Bidirection (None, 100) 45600 _________________________________________________________________ dense_2 (Dense) (None, 1) 101 ================================================================= Total params: 1,080,533 Trainable params: 1,080,533 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/3 391/391 [==============================] - 27s 43ms/step - loss: 0.5076 - accuracy: 0.7144 Epoch 2/3 391/391 [==============================] - 17s 43ms/step - loss: 0.1790 - accuracy: 0.9341 Epoch 3/3 391/391 [==============================] - 17s 43ms/step - loss: 0.1019 - accuracy: 0.9653 Accuracy: 87.90% What is the worst movie review in the test set according to your model? üçÖ In [41]: preds = model . predict_proba ( X_test ) worst_review = X_test [ preds . argmin ()] show_review ( worst_review ) steven seagal has made a really dull bad and boring movie steven seagal plays a doctor this movie has got a few action scenes but they are poorly directed and have nothing to do with the rest of the movie a group of american nazis spread a lethal virus which is able to wipe out the state of montana wesley seagal s character tries desperately to find a cure and that is the story of the the is an extremely boring film because nothing happens it is filled with boring dialogue and illogical gaps between events and stupid actors steven seagal has totally up in this movie and i would not recommend this to my worst enemy 3 10 What is the best movie review in the test set according to your model? üèÜ In [78]: best_review = X_test [ preds . argmax ()] show_review ( best_review ) village with their artist uncle john after the death of their parents sister and john's brother simon has given up trying to convince john to allow he and susan to take care of the children and have to using private detectives to catch him in either behavior or unemployed and therefore unable to care for the children properly susan finally decides to take matters into her own hands and goes to village herself posing as an actress to try to gain information and or him to see reason what she discovers however is that she not only likes the free and artistic lifestyle john and his friends are living and that the girls are being brought up well but that she is quickly falling in love with john inevitably her true identity is discovered and she is faced with the task of convincing everyone on both sides of the custody debate who should belong with whom br br i really enjoyed this film and found that its very short running time 70 minutes was the perfect length to spin this simple but endearing story hopkins one of the great 1930's 1940's actresses is delightful in this film her energy style and wholesome beauty really lend themselves to creating an endearing character even though you know that she's pulling a fast one on the people she quickly befriends this is the earliest film i've seen ray in and he was actually young and non looking and apparently three years younger than his co star his energy and manner in wise girl were a refreshing change to the demeanor he affects in his usual darker films honestly though i am usually not remotely a fan of child actors i really enjoyed the two young girls who played they were and were really the of the film unfortunately i can't dig up any other films that either of them were subsequently in after this one which is a shame since both a large amount of natural talent br br wise girl was a film that was made three years after the hollywood code was and to some extent this was clear by the quick happy ending and the pie in the sky and ease with which the characters lived the alleged co was in fact a gorgeous de where the artists lived for free or for trade and everything is tied up very nicely throughout fortunately this was a light enough film and the characters were charming enough to make for its and short and i was able to just take wise girl for what it was a good old fashioned love story that was as entertaining as it was endearing unfortunately films of the romantic comedy drama genre today are considerably less intelligent and entertaining or i wouldn't find myself continuously returning to the classics 7 10 End of Exercise: Please return to the main room Heavy Metal Lyric Generator Here we'll design an RNN to generate song lyrics character by character! The model will take in a sequences of 40 character 'windows' of text and predict the most probable next character. This new character is then appended to the original sequence, the first character is dropped, and this new sequene is fed back into the model. We can repeat this process for as long as we like to generate output of arbitrary length. In [89]: metal_df = pd . read_csv ( 'data/metal_lyrics_PG.csv' ) In [90]: metal_df . shape Out[90]: (4785, 4) How to we know these are heavy metal lyrics? In [91]: metal_df [ metal_df . lyrics . str . contains ( 'elves' )] Out[91]: song year artist lyrics 116 vinvm-sabbati 2001 behemoth waters running down\\nby the silver moon rays\\n... 197 gaya-s-dream 1993 the-gathering open the gates of the past\\nwith the key to ou... 202 generations 2013 answer-with-metal look around, the air is full with fear and we ... 250 dark-of-the-sun 2006 arch-enemy like insects of the night, we are drawn into t... 258 shadows-and-dust 2006 arch-enemy at the mercy of our conscience\\nconfined withi... ... ... ... ... ... 4589 scorn 2006 allegiance likes rats we strip the earth\\nrampant animals... 4600 armies-of-valinor 2007 galadriel into the battle we ride again\\nagainst the dar... 4609 new-priesthood 2006 dark-angel history's shown you that answers can't be foun... 4704 ride-for-glory 2007 dragonland \\n\"we yearn for the battle and the glory...but... 4782 principle-of-speed 2007 drifter i made an experience\\nit just happens once in ... 107 rows √ó 4 columns Ok, I'm convinced. In [92]: n_samples = 1000 lyrics_sample = metal_df . sample ( n = n_samples , random_state = 109 ) . lyrics . values In [93]: raw_text = ' \\n ' . join ( lyrics_sample ) # remove bad chars raw_text = re . sub ( r \"[&#94;\\s\\w']\" , \"\" , raw_text ) chars = set ( sorted ( raw_text )) char2idx = dict (( c , i ) for i , c in enumerate ( chars )) idx2char = dict (( i , c ) for i , c in enumerate ( chars )) n_chars = len ( raw_text ) n_vocab = len ( chars ) print ( f 'Sample Corpus Length: { len ( raw_text ) } ' ) Sample Corpus Length: 720944 Creating Input/Target Pairs We need to slice up our lyric data to create input and target pairs that can be to our model for its supervised prediction task. Each input with be a sequence of seq_len characters. This can be though of as a sliding window across the concatenated lyric data. The response is the character after the end of that window in the training data. In [94]: # prepare the dataset of input to output pairs encoded as integers seq_len = 40 seqs = [] targets = [] for i in range ( 0 , n_chars - seq_len ): seq = raw_text [ i : i + seq_len ] target = raw_text [ i + seq_len ] seqs . append ([ char2idx [ char ] for char in seq ]) targets . append ( char2idx [ target ]) n_seqs = len ( seqs ) print ( \"Total Char Sequences: \" , n_seqs ) Total Char Sequences: 720904 We can create a one-hot encoding by indexing into an n_vocab sized identity matrix using the character index values. In [95]: X = np . reshape ( seqs , ( - 1 , seq_len )) eye = np . eye ( n_vocab ) X = eye [ seqs ] y = eye [ targets ] In [96]: X . shape , y . shape Out[96]: ((720904, 40, 29), (720904, 29)) In [97]: # remove some large variables from memory del metal_df del lyrics_sample del seqs LambdaCallback The loss score is usually not the best way to judge if our language model is learning to generate 'quality' test. It would be better if we could periodically see examples of the kind of text it can generate as it trains so we can judge for ourselves. The LambdaCallback allows us to execute arbitary functions at different points in the training process and why they are useful when evaluating generative models. We'll use it to generate some sample text at the end of every other epoch. In [98]: from tensorflow.keras.callbacks import LambdaCallback In [99]: def on_epoch_end ( epoch , _ ): # only triggers on every 2nd epoch if (( epoch + 1 ) % 2 == 0 ): # select a random seed sequence start = np . random . randint ( 0 , len ( X ) - 1 ) seq = X [ start ] seed = '' . join ([ idx2char [ np . argmax ( x )] for x in seq ]) print ( f \"---Seed: \\\" { repr ( seed ) } \\\" ---\" ) print ( f \" { seed } \" , end = '' ) # generate characters for i in range ( 200 ): x = seq . reshape ( 1 , seq_len , - 1 ) pred = model . predict ( x , verbose = 0 )[ 0 ] # sampling gives us more 'serendipity' than argmax # index = np.argmax(pred) index = np . random . choice ( n_vocab , p = pred ) result = idx2char [ index ] sys . stdout . write ( result ) # shift sequence over seq [: - 1 ] = seq [ 1 :] seq [ - 1 ] = eye [ index ] print () generate_text = LambdaCallback ( on_epoch_end = on_epoch_end ) We then add the LambdaCallback to the callbacks list along with ModelCheckpoint and EarlyStopping to be passed to the fit() method at train time. In [100]: # define the checkpoint model_name = 'metal-char' filepath = f 'models/ { model_name } .hdf5' checkpoint = ModelCheckpoint ( filepath , monitor = 'loss' , verbose = 1 , save_weights_only = False , save_best_only = True , mode = 'min' ) es = EarlyStopping ( monitor = 'loss' , patience = 3 , verbose = 0 , mode = 'auto' , restore_best_weights = True ) callbacks_list = [ checkpoint , generate_text , es ] Exercise: Build a Character Based Lyric Generator Architecture Bidirection LSTM with a hidden dimension of 128 in each direction BatchNormalization to speed up training (Don't tell Pavlos!) Dense output layer to predict the next character In [11]: # your code here hidden_dim = 128 model = Sequential () model . add ( Bidirectional ( LSTM ( hidden_dim ), input_shape = ( seq_len , n_vocab ))) model . add ( BatchNormalization ()) model . add ( Dense ( n_vocab , activation = 'softmax' )) In [12]: model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' ) model . summary () Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional (Bidirectional (None, 256) 161792 _________________________________________________________________ batch_normalization (BatchNo (None, 256) 1024 _________________________________________________________________ dense (Dense) (None, 29) 7453 ================================================================= Total params: 170,269 Trainable params: 169,757 Non-trainable params: 512 _________________________________________________________________ In [16]: model . fit ( X , y , epochs = 30 , batch_size = 128 , callbacks = callbacks_list ) Epoch 1/30 5633/5633 [==============================] - 47s 7ms/step - loss: 2.1992 Epoch 00001: loss improved from inf to 2.02026, saving model to models/metal-char.hdf5 Epoch 2/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.7666 Epoch 00002: loss improved from 2.02026 to 1.72630, saving model to models/metal-char.hdf5 ---Seed: \"\"eeze but why\\nwon't you believe\\nwon't you\"\"--- eeze but why won't you believe won't you will how kyon't now it you've goe th so noundapop and so shand will foring hpres and like heredging return all the stong love a cryttgin fe i'm noblows the glort comes evermeht ard a gong holortions Epoch 3/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.6288 Epoch 00003: loss improved from 1.72630 to 1.60815, saving model to models/metal-char.hdf5 Epoch 4/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.5469 Epoch 00004: loss improved from 1.60815 to 1.53622, saving model to models/metal-char.hdf5 ---Seed: \"'l know me inside and outside\\ni am here t'\"--- l know me inside and outside i am here the scarture for some no andon gounder time nair a flose the the shorious of perpore and glow but arourd she we just you bark for go darkness's need the missing oh your were the sinsh away the sky thei Epoch 5/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.4940 Epoch 00005: loss improved from 1.53622 to 1.48602, saving model to models/metal-char.hdf5 Epoch 6/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.4527 Epoch 00006: loss improved from 1.48602 to 1.44772, saving model to models/metal-char.hdf5 ---Seed: \"'y your kindred watch us pronouncing sent'\"--- y your kindred watch us pronouncing sently life we go the conceliw a watmallch age here release wrock agard to the haunhown your life worntly way too just right yer so many you makes me am now homence the geas to get away take it down gol Epoch 7/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.4195 Epoch 00007: loss improved from 1.44772 to 1.41843, saving model to models/metal-char.hdf5 Epoch 8/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.3948 Epoch 00008: loss improved from 1.41843 to 1.39306, saving model to models/metal-char.hdf5 ---Seed: \"'here\\ndespite what you may think jesus wi'\"--- here despite what you may think jesus with that just sunkness memories remember free this i can never knees i am drifted laying here infore through the sign i can't be the completeh something burning stamber cross reach to our arms than her Epoch 9/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.3715 Epoch 00009: loss improved from 1.39306 to 1.37268, saving model to models/metal-char.hdf5 Epoch 10/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.3516 Epoch 00010: loss improved from 1.37268 to 1.35401, saving model to models/metal-char.hdf5 ---Seed: \"'ence what does it mean\\nare you satisfied'\"--- ence what does it mean are you satisfied are yelcary our voodes breathled with weak the pullable do ac truth there's a neck reach we let me good yoursele relefcion around freedom noin the burnt to beer with home sail becauses let down it ha Epoch 11/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.3380 Epoch 00011: loss improved from 1.35401 to 1.33822, saving model to models/metal-char.hdf5 Epoch 12/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.3220 Epoch 00012: loss improved from 1.33822 to 1.32432, saving model to models/metal-char.hdf5 ---Seed: \"' know now all the reasons\\nand all the co'\"--- know now all the reasons and all the corcely game time with me but image tears the the mairin take hault me ashameeds at lay in the ocean of time leave my hope repouration you're my tears of your heart like the day is let the way driving t Epoch 13/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.3081 Epoch 00013: loss improved from 1.32432 to 1.31071, saving model to models/metal-char.hdf5 Epoch 14/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2938 Epoch 00014: loss improved from 1.31071 to 1.29885, saving model to models/metal-char.hdf5 ---Seed: \"'ars\\nnow beneath\\nelectrical skies\\nartille'\"--- ars now beneath electrical skies artilleds i tollimily fall it comes dopt all to eternity eternity the humand as the flaughhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahh hhoh Epoch 15/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2828 Epoch 00015: loss improved from 1.29885 to 1.28791, saving model to models/metal-char.hdf5 Epoch 16/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2699 Epoch 00016: loss improved from 1.28791 to 1.27619, saving model to models/metal-char.hdf5 ---Seed: \"'ady to strike\\ncall for us and you will s'\"--- ady to strike call for us and you will sounds shin frozes into the rold you will open now you wake up why will come us to me open death i cannot toll me i am not envence i could is sidt the sun away from the tender on the cloud silfit exuce Epoch 17/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2591 Epoch 00017: loss improved from 1.27619 to 1.26540, saving model to models/metal-char.hdf5 Epoch 18/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2475 Epoch 00018: loss improved from 1.26540 to 1.25442, saving model to models/metal-char.hdf5 ---Seed: \"\"'ve never been hurt\\nact like you don't n\"\"--- 've never been hurt act like you don't never gike and read i know you sustappine everypherians to my fingers hard to go away you feel ignorade suffer to your vicions free put the grave tils murriors wind skies of sangring imputting building Epoch 19/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2388 Epoch 00019: loss improved from 1.25442 to 1.24440, saving model to models/metal-char.hdf5 Epoch 20/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2306 Epoch 00020: loss improved from 1.24440 to 1.23521, saving model to models/metal-char.hdf5 ---Seed: \"'bered by offsprings\\nwitness the end of m'\"--- bered by offsprings witness the end of madical preceine but exists havelen't mind too much to avoid walk harnts of the just throne an a land lay it leaves the beauth around myself this hopely staity that a long shall it fears the grances of Epoch 21/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2214 Epoch 00021: loss improved from 1.23521 to 1.22585, saving model to models/metal-char.hdf5 Epoch 22/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.2067 Epoch 00022: loss improved from 1.22585 to 1.21731, saving model to models/metal-char.hdf5 ---Seed: \"' deep world of darkness so infinite and '\"--- deep world of darkness so infinite and shelter like spring distones tormore the winds spow i never asween but when the reasons puning it i don't know we'll shane this girl enthroid what do you pane break what you can led by once was too Epoch 23/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1998 Epoch 00023: loss improved from 1.21731 to 1.20829, saving model to models/metal-char.hdf5 Epoch 24/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1934 Epoch 00024: loss improved from 1.20829 to 1.19972, saving model to models/metal-char.hdf5 ---Seed: \"'ed\\nascend\\nto darkness we sail\\neternal re'\"--- ed ascend to darkness we sail eternal remember the when is the humand time is hurt with each in chance spread the indectimeom the journey begin we'll be fixano just exploit for us into the bett it returned by both our kinn light i am still Epoch 25/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1828 Epoch 00025: loss improved from 1.19972 to 1.19151, saving model to models/metal-char.hdf5 Epoch 26/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1740 Epoch 00026: loss improved from 1.19151 to 1.18459, saving model to models/metal-char.hdf5 ---Seed: \"'christian society\\nwomen were anathematiz'\"--- christian society women were anathematized of goes guestival powers of apotheration fass of my north no fiust gliding the provelosms of distrocked snowledged who now and this impromise fullles me stays before we melding denied will reveag Epoch 27/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1674 Epoch 00027: loss improved from 1.18459 to 1.17692, saving model to models/metal-char.hdf5 Epoch 28/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1592 Epoch 00028: loss improved from 1.17692 to 1.16979, saving model to models/metal-char.hdf5 ---Seed: \"'only void just about everywhere\\nso i din'\"--- only void just about everywhere so i dinacl fear for rain and carvable we shared by it give us the way that a commour and darows from althounds now always creatured down and beside the soul life i never be seen your nevered in things and tw Epoch 29/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1546 Epoch 00029: loss improved from 1.16979 to 1.16355, saving model to models/metal-char.hdf5 Epoch 30/30 5633/5633 [==============================] - 39s 7ms/step - loss: 1.1478 Epoch 00030: loss improved from 1.16355 to 1.15678, saving model to models/metal-char.hdf5 ---Seed: \"'ng\\nwhen the life forsaken again is the o'\"--- ng when the life forsaken again is the one and dough i can't take her feel myself spected sucame alone my last is always we pride on me the darkness ofwerd the times there's a strength to last the words' dies anate majesty triet the old ste Out[16]: End of Exercise: Please return to the main room In [20]: model = load_model ( f 'models/ { model_name } .hdf5' ) With some helper functions we can generate text from an arbitrary seed string. In [16]: def sample ( preds , temperature = 1.0 ): # helper function to sample an index from a probability array preds = np . asarray ( preds ) . astype ( 'float64' ) preds = np . log ( preds ) / temperature exp_preds = np . exp ( preds ) preds = exp_preds / np . sum ( exp_preds ) probas = np . random . multinomial ( 1 , preds , 1 ) return np . argmax ( probas ) def gen_text_char ( seq , temperature = 0.3 ): print ( \"Seed:\" ) print ( \" \\\" \" , '' . join ([ idx2char [ np . argmax ( x )] for x in seq ]), \" \\\" \" , end = '' ) # generate characters for i in range ( 1000 ): x = seq . reshape ( 1 , seq_len , - 1 ) pred = model . predict ( x , verbose = 0 )[ 0 ] index = sample ( pred , temperature ) # index = np.argmax(pred) result = idx2char [ index ] sys . stdout . write ( result ) # shift sequence over seq [: - 1 ] = seq [ 1 :] seq [ - 1 ] = eye [ index ] print () def text_from_seed ( s , temperature = 0.3 ): s = s . lower () s = re . sub ( r \"[&#94;\\s\\w']\" , \"\" , s ) char2idx = { c : i for i , c in idx2char . items ()} seq = [ char2idx [ c ] for c in s ] if len ( seq ) < seq_len : print ( f 'Seed must be at least { seq_len } characters long!' ) seq = seq [: seq_len ] x = np . copy ( np . array ( seq )) x = eye [ x ] gen_text_char ( x , temperature ) Set a seed string and see where your model takes it. In [30]: seed = \"Sunshine, lollipops and rainbows \\n Everything that's wonderful is what I feel when we're together \" text_from_seed ( seed ) Seed: \" sunshine lollipops and rainbows everythi \"ng is not all the world the shadows of a dead my life in the world in the wind the dark and see the land the soul of a devolut of the darkness we are the ones who says there a blackened with the way the world is speak i want to go back to me the beauty of the darkness and destroyed are so come the reason to the battle with the wind is not all the world i want to love you met you i'm doing to me i want to see you and i don't know i was so were we see to be the one we left behind the fear of the dark in a desert of the ancient life i will not come to me the more i can't see the past and i was the one who was now the season where the streets of my heart i wish i was better one the sea we go to care in the sun of the attic i will find a world in the wind the world in the wind the sands of the halls of a new world i can't see the fire is all i need the silence of the beast never been said and survive on the waves i'm waiting for an answer the land of the sun in the wind to carry on the wind Q : How might you improve upon this simple model architecture? Arithmetic with an RNN Thanks go to Eleni for this code example. In this exercise, we are going to teach addition to our model. Given two numbers (<999), the model outputs their sum (<9999). The input is provided as a string '231+432' and the model will provide its output as ' 663' (Here the empty space is the padding character). We are not going to use any external dataset and are going to construct our own dataset for this exercise. The exercise we attempt to do effectively \"translates\" a sequence of characters '231+432' to another sequence of characters ' 663' and hence, this class of models are called sequence-to-sequence models (aka seq2seq). Such architectures have profound applications in several real-life tasks such as machine translation, summarization, image captioning etc. To be clear, sequence-to-sequence (aka seq2seq) models take as input a sequence of length N and return a sequence of length M, where N and M may or may not differ, and every single observation/input may be of different values, too. For example, machine translation concerns converting text from one natural language to another (e.g., translating English to French). Google Translate is an example, and their system is a seq2seq model. The input (e.g., an English sentence) can be of any length, and the output (e.g., a French sentence) may be of any length. Background knowledge: The earliest and most simple seq2seq model works by having one RNN for the input, just like we've always done, and we refer to it as being an \"encoder.\" The final hidden state of the encoder RNN is fed as input to another RNN that we refer to as the \"decoder.\" The job of the decoder is to generate each token, one word at a time. This may seem really limiting, as it relies on the encoder encapsulating the entire input sequence with just 1 hidden layer. It seems unrealistic that we could encode an entire meaning of a sentence with just one hidden layer. Yet, results even in this simplistic manner can be quite impressive. In fact, these early results were compelling enough that these models immediately replaced the decades of earlier machine translation work. In [134]: from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , RepeatVector , TimeDistributed data generation and preprocessing We can simply generate all the training data we need. In [141]: class CharacterTable ( object ): def __init__ ( self , chars ): self . chars = sorted ( set ( chars )) self . char_indices = { c : i for i , c in enumerate ( self . chars )} self . indices_char = { i : c for i , c in enumerate ( self . chars )} # converts a String of characters into a one-hot embedding/vector def encode ( self , C , num_rows ): x = np . zeros (( num_rows , len ( self . chars ))) for i , c in enumerate ( C ): x [ i , self . char_indices [ c ]] = 1 return x # converts a one-hot embedding/vector into a String of characters def decode ( self , x , calc_argmax = True ): if calc_argmax : x = x . argmax ( axis =- 1 ) return '' . join ( self . indices_char [ x ] for x in x ) In [142]: TRAINING_SIZE = 50000 DIGITS = 3 MAXOUTPUTLEN = DIGITS + 1 MAXLEN = DIGITS + 1 + DIGITS chars = '0123456789+ ' ctable = CharacterTable ( chars ) In [143]: def return_random_digit (): return np . random . choice ( list ( '0123456789' )) # generate a new number of length `DIGITS` def generate_number (): num_digits = np . random . randint ( 1 , DIGITS + 1 ) return int ( '' . join ( return_random_digit () for i in range ( num_digits ))) # generate `TRAINING_SIZE` # of pairs of random numbers def data_generate ( num_examples ): questions = [] answers = [] seen = set () print ( 'Generating data...' ) while len ( questions ) < TRAINING_SIZE : a , b = generate_number (), generate_number () # don't allow duplicates; this is good practice for training, # as we will minimize memorizing seen examples key = tuple ( sorted (( a , b ))) if key in seen : continue seen . add ( key ) # pad the data with spaces so that the length is always MAXLEN. q = ' {} + {} ' . format ( a , b ) query = q + ' ' * ( MAXLEN - len ( q )) ans = str ( a + b ) # answers can be of maximum size DIGITS + 1. ans += ' ' * ( MAXOUTPUTLEN - len ( ans )) questions . append ( query ) answers . append ( ans ) print ( 'Total addition questions:' , len ( questions )) return questions , answers def encode_examples ( questions , answers ): x = np . zeros (( len ( questions ), MAXLEN , len ( chars )), dtype = np . bool ) y = np . zeros (( len ( questions ), DIGITS + 1 , len ( chars )), dtype = np . bool ) for i , sentence in enumerate ( questions ): x [ i ] = ctable . encode ( sentence , MAXLEN ) for i , sentence in enumerate ( answers ): y [ i ] = ctable . encode ( sentence , DIGITS + 1 ) indices = np . arange ( len ( y )) np . random . shuffle ( indices ) return x [ indices ], y [ indices ] In [144]: q , a = data_generate ( TRAINING_SIZE ) x , y = encode_examples ( q , a ) # divides our data into training and validation split_at = len ( x ) - len ( x ) // 10 x_train , x_val , y_train , y_val = x [: split_at ], x [ split_at :], y [: split_at ], y [ split_at :] print ( 'Training Data shape:' ) print ( 'X : ' , x_train . shape ) print ( 'Y : ' , y_train . shape ) print ( 'Sample Question(in encoded form) : ' , x_train [ 0 ], y_train [ 0 ]) print ( 'Sample Question(in decoded form) : ' , ctable . decode ( x_train [ 0 ]), 'Sample Output : ' , ctable . decode ( y_train [ 0 ])) Generating data... Total addition questions: 50000 Training Data shape: X : (45000, 7, 12) Y : (45000, 4, 12) Sample Question(in encoded form) : [[False False False False False False False True False False False False] [False False False False False False False False False False False True] [False False True False False False False False False False False False] [False True False False False False False False False False False False] [False False False False False False False False True False False False] [False False False False False False False False False False True False] [ True False False False False False False False False False False False]] [[False False False False False False False False True False False False] [False False False False False False False True False False False False] [False False False False False False False False False False True False] [ True False False False False False False False False False False False]] Sample Question(in decoded form) : 590+68 Sample Output : 658 In [145]: x_train Out[145]: array([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, True], [False, False, True, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, True, False], [ True, False, False, ..., False, False, False]], [[False, False, False, ..., False, True, False], [False, False, False, ..., True, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, True, False], [ True, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, True, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False]], ..., [[False, False, False, ..., True, False, False], [False, True, False, ..., False, False, False], [False, False, False, ..., True, False, False], ..., [False, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, True, False], [False, True, False, ..., False, False, False], ..., [False, False, True, ..., False, False, False], [False, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False]]]) Build an RNN for Arithmetic Note: Whenever you are initializing a LSTM in Keras, by the default the option return_sequences = False . This means that at the end of the step the next component will only get to see the final hidden layer's values. On the other hand, if you set return_sequences = True , the LSTM component will return the hidden layer at each time step. It means that the next component should be able to consume inputs in that form. Think how this statement is relevant in terms of this model architecture and the TimeDistributed module we just learned. Build an encoder and decoder both single layer 128 nodes and an appropriate dense layer as needed by the model. In [150]: # Hyperaparams HIDDEN_SIZE = 128 BATCH_SIZE = 128 LAYERS = 1 print ( 'Build model...' ) model = Sequential () #ENCODING model . add ( LSTM ( HIDDEN_SIZE , input_shape = ( MAXLEN , len ( chars )))) model . add ( RepeatVector ( MAXOUTPUTLEN )) #DECODING for _ in range ( LAYERS ): # return hidden layer at each time step model . add ( LSTM ( HIDDEN_SIZE , return_sequences = True )) model . add ( TimeDistributed ( Dense ( len ( chars ), activation = 'softmax' ))) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) model . summary () Build model... Model: \"sequential_22\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_16 (LSTM) (None, 128) 72192 _________________________________________________________________ repeat_vector_2 (RepeatVecto (None, 4, 128) 0 _________________________________________________________________ lstm_17 (LSTM) (None, 4, 128) 131584 _________________________________________________________________ time_distributed_1 (TimeDist (None, 4, 12) 1548 ================================================================= Total params: 205,324 Trainable params: 205,324 Non-trainable params: 0 _________________________________________________________________ Let's check how well our model trained. In [151]: for iteration in range ( 1 , 2 ): print () model . fit ( x_train , y_train , batch_size = BATCH_SIZE , epochs = 20 , validation_data = ( x_val , y_val )) # Select 10 samples from the validation set at random so # we can visualize errors. print ( 'Finished iteration ' , iteration ) numcorrect = 0 numtotal = 20 for i in range ( numtotal ): ind = np . random . randint ( 0 , len ( x_val )) rowx , rowy = x_val [ np . array ([ ind ])], y_val [ np . array ([ ind ])] preds = model . predict_classes ( rowx , verbose = 0 ) q = ctable . decode ( rowx [ 0 ]) correct = ctable . decode ( rowy [ 0 ]) guess = ctable . decode ( preds [ 0 ], calc_argmax = False ) print ( 'Question' , q , end = ' ' ) print ( 'True' , correct , end = ' ' ) print ( 'Guess' , guess , end = ' ' ) if guess == correct : print ( 'Good job' ) numcorrect += 1 else : print ( 'Fail' ) print ( 'The model scored ' , numcorrect * 100 / numtotal , ' % i n its test.' ) Epoch 1/20 352/352 [==============================] - 5s 6ms/step - loss: 2.0174 - accuracy: 0.2885 - val_loss: 1.7942 - val_accuracy: 0.3441 Epoch 2/20 352/352 [==============================] - 2s 5ms/step - loss: 1.7772 - accuracy: 0.3434 - val_loss: 1.7091 - val_accuracy: 0.3704 Epoch 3/20 352/352 [==============================] - 2s 5ms/step - loss: 1.6578 - accuracy: 0.3831 - val_loss: 1.5676 - val_accuracy: 0.4189 Epoch 4/20 352/352 [==============================] - 2s 5ms/step - loss: 1.5156 - accuracy: 0.4305 - val_loss: 1.4042 - val_accuracy: 0.4744 Epoch 5/20 352/352 [==============================] - 2s 5ms/step - loss: 1.3777 - accuracy: 0.4846 - val_loss: 1.2784 - val_accuracy: 0.5250 Epoch 6/20 352/352 [==============================] - 2s 5ms/step - loss: 1.2598 - accuracy: 0.5288 - val_loss: 1.1922 - val_accuracy: 0.5512 Epoch 7/20 352/352 [==============================] - 2s 5ms/step - loss: 1.1605 - accuracy: 0.5640 - val_loss: 1.1004 - val_accuracy: 0.5843 Epoch 8/20 352/352 [==============================] - 2s 5ms/step - loss: 1.0719 - accuracy: 0.5927 - val_loss: 1.0159 - val_accuracy: 0.6151 Epoch 9/20 352/352 [==============================] - 2s 5ms/step - loss: 0.9892 - accuracy: 0.6241 - val_loss: 0.9340 - val_accuracy: 0.6425 Epoch 10/20 352/352 [==============================] - 2s 5ms/step - loss: 0.8879 - accuracy: 0.6606 - val_loss: 0.8111 - val_accuracy: 0.6805 Epoch 11/20 352/352 [==============================] - 2s 5ms/step - loss: 0.7547 - accuracy: 0.7132 - val_loss: 0.6573 - val_accuracy: 0.7530 Epoch 12/20 352/352 [==============================] - 2s 5ms/step - loss: 0.6126 - accuracy: 0.7778 - val_loss: 0.5363 - val_accuracy: 0.8079 Epoch 13/20 352/352 [==============================] - 2s 5ms/step - loss: 0.4939 - accuracy: 0.8360 - val_loss: 0.4236 - val_accuracy: 0.8686 Epoch 14/20 352/352 [==============================] - 2s 5ms/step - loss: 0.3952 - accuracy: 0.8824 - val_loss: 0.3391 - val_accuracy: 0.9003 Epoch 15/20 352/352 [==============================] - 2s 5ms/step - loss: 0.3146 - accuracy: 0.9160 - val_loss: 0.2851 - val_accuracy: 0.9208 Epoch 16/20 352/352 [==============================] - 2s 5ms/step - loss: 0.2535 - accuracy: 0.9382 - val_loss: 0.2221 - val_accuracy: 0.9458 Epoch 17/20 352/352 [==============================] - 2s 5ms/step - loss: 0.2063 - accuracy: 0.9535 - val_loss: 0.1934 - val_accuracy: 0.9529 Epoch 18/20 352/352 [==============================] - 2s 5ms/step - loss: 0.1786 - accuracy: 0.9584 - val_loss: 0.1608 - val_accuracy: 0.9613 Epoch 19/20 352/352 [==============================] - 2s 5ms/step - loss: 0.1441 - accuracy: 0.9707 - val_loss: 0.1314 - val_accuracy: 0.9708 Epoch 20/20 352/352 [==============================] - 2s 5ms/step - loss: 0.1210 - accuracy: 0.9758 - val_loss: 0.1156 - val_accuracy: 0.9750 Finished iteration 1 Question 579+42 True 621 Guess 621 Good job Question 778+40 True 818 Guess 818 Good job Question 34+574 True 608 Guess 608 Good job Question 5+553 True 558 Guess 558 Good job Question 2+27 True 29 Guess 29 Good job Question 506+30 True 536 Guess 536 Good job Question 51+714 True 765 Guess 765 Good job Question 258+31 True 289 Guess 289 Good job Question 9+70 True 79 Guess 89 Fail Question 14+83 True 97 Guess 97 Good job Question 59+378 True 437 Guess 437 Good job Question 94+836 True 930 Guess 920 Fail Question 875+483 True 1358 Guess 1358 Good job Question 482+34 True 516 Guess 516 Good job Question 257+49 True 306 Guess 306 Good job Question 591+5 True 596 Guess 596 Good job Question 88+771 True 859 Guess 859 Good job Question 248+86 True 334 Guess 334 Good job Question 27+929 True 956 Guess 956 Good job Question 23+854 True 877 Guess 877 Good job The model scored 90.0 % in its test. Possible Experimentation Try changing the hyperparams, use other RNNs, more layers, check if increasing the number of epochs is useful. Try reversing the data from validation set and check if commutative property of addition is learned by the model. Try printing the hidden layer with two inputs that are commutative and check if the hidden representations it learned are same or similar. Do we expect it to be true? If so, why? If not why? You can access the layer using an index with model.layers and layer.output will give the output of that layer. Try doing addition in the RNN model the same way we do by hand. Reverse the order of digits and at each time step, input two digits get an output use the hidden layer and input next two digits and so on.(units in the first time step, tens in the second time step etc.)",
        "tags": "sections",
        "url": "sections/sec03/notebook/"
    }, {
        "title": "Sections 01:",
        "text": "",
        "tags": "sections",
        "url": "sections/section01/"
    }, {"title": "Sections 03:", "text": "", "tags": "sections", "url": "sections/section03/"}]
}