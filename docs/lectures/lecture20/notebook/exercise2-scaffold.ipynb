{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Pavlos Recurrent Unit\n",
    "\n",
    "## Description :\n",
    "The goal of this exercise is to build the **Pavlos Recurrent Unit** discussed in class.\n",
    "\n",
    "<img src=\"../fig/fig1.png\" style=\"width: 500px;\">\n",
    "\n",
    "<img src=\"../fig/fig2.png\" style=\"width: 500px;\">\n",
    "\n",
    "Alternative notation used in the exercise:\n",
    "\n",
    "<img src=\"../fig/fig3.png\" style=\"width: 500px;\">\n",
    "\n",
    "## Instructions:\n",
    "- Read the IMDB dataset from the helper code given.\n",
    "- Take a quick look at your training inputs and labels.\n",
    "- Pad the values to a fix number `max_words` in-order to have sequences of the same size.\n",
    "- Fill in the helper code given to build the PRU cell.\n",
    "- Using the tensorflow.keras Functional API, build, compile and fit the PRU RNN and evaluate it on the test set.\n",
    "- For reference, also refit the model with a vanilla RNN and a GRU.\n",
    "- Again evaluate the model performance on the test set of both models and compare it with the PRU unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pavlos Recurrent Unit   <img src=\"./favicon.ico\" alt=\"Pavlos\" style =width:40px /> \n",
    "\n",
    "In this exercise, we will build the PRU as discussed in class to perform sentiment analysis in tensorflow.keras.\n",
    "We will continue to use the custom dataset from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import RNN\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Input,Dense,Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same dataset as the previous exercise \n",
    "with open('imdb_mini.pkl','rb') as f:\n",
    "    X_train, y_train, X_test, y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the previous exercise, we will pre-preprocess our review sequences\n",
    "# We fix the vocabulary size to 5000 because our custom \n",
    "# dataset was curated with that\n",
    "vocabulary_size = 5000\n",
    "# Max word length for each review will be 500\n",
    "max_words = 500\n",
    "# we set the embedding size to 32\n",
    "embedding_size=32\n",
    "# Pre-padding sequences to max_words lenth\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words,padding='pre')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\r",
      "   8192/1641221 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 147456/1641221 [=>............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 237568/1641221 [===>..........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 311296/1641221 [====>.........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 401408/1641221 [======>.......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 409600/1641221 [======>.......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 499712/1641221 [========>.....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 540672/1641221 [========>.....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 581632/1641221 [=========>....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 630784/1641221 [==========>...................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 688128/1641221 [===========>..................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 737280/1641221 [============>.................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 794624/1641221 [=============>................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 851968/1641221 [==============>...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 876544/1641221 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 925696/1641221 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 966656/1641221 [================>.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1032192/1641221 [=================>............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1089536/1641221 [==================>...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1155072/1641221 [====================>.........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1220608/1641221 [=====================>........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1269760/1641221 [======================>.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1351680/1641221 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1400832/1641221 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1466368/1641221 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1515520/1641221 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1564672/1641221 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1630208/1641221 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1646592/1641221 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# We create the mapping between words and sequences\n",
    "word2id = imdb.get_word_index()\n",
    "# We need to adjust the mapping by 3 because of tensorflow.keras preprocessing\n",
    "# more here: https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "word2id = {k:(v+3) for k,v in word2id.items()}\n",
    "word2id[\"<PAD>\"] = 0\n",
    "word2id[\"<START>\"] = 1\n",
    "word2id[\"<UNK>\"] = 2\n",
    "word2id[\"<UNUSED>\"] = 3\n",
    "\n",
    "# Reversing the key,value pair will give the id2word\n",
    "id2word = {i: word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∏ For the current problem, if the memory state size is 5, what will be the dimension of $W_{xh}$? \n",
    "\n",
    "\n",
    "#### A. (32,32)\n",
    "#### B. (32,5)\n",
    "#### D. (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the helper code below to build the Pavlos Recurrent Unit\n",
    "# We do this by building a PRU cell unit\n",
    "# which we can wrap around tf.keras.layers.RNN\n",
    "# Read more here on layer subclassing https://keras.io/guides/making_new_layers_and_models_via_subclassing/\n",
    "\n",
    "class PRUCell(tf.keras.layers.Layer):\n",
    "    def __init__(self,units,**kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        self.activation = tf.math.tanh\n",
    "        self.recurrent_activation = tf.math.sigmoid\n",
    "        super(PRUCell, self).__init__(**kwargs)\n",
    "        \n",
    "                \n",
    "        # In the build function we initialize the weights\n",
    "        # Which will be used for training        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Initializing weights for candidate Ht\n",
    "        ## W_{XH}\n",
    "        self.kernel_h = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        ## W_{HH}\n",
    "        self.recurrent_kernel_h = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "    \n",
    "        \n",
    "        # Initializing weights for PP gate\n",
    "        ## W_{XPP} \n",
    "        self.kernel_pp = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='PP_kernel')\n",
    "        ## W_{HPP}\n",
    "        self.recurrent_kernel_pp = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='PP_recurrent_kernel')\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "        # Note that we do not include a bias term for ease of understanding\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        ## inputs: X_t \n",
    "        ## states: h_{t-1}\n",
    "        ## self.XXXX contains the weights (see above)\n",
    "        # Previous output comes from states tuple, H_{t-1}\n",
    "        prev_output = states[0]\n",
    "        \n",
    "        # First we compute the PPgate\n",
    "        PP_XW = K.dot(___, ___)\n",
    "        PP_HV = K.dot(___, ___)\n",
    "        PPgate = self.recurrent_activation( ___ +  ___)\n",
    "        \n",
    "        # Now we use the PPgate as per the equation for candidate Ht\n",
    "        nn_XW = K.dot(___, ___)\n",
    "        dotted_output = ___*___\n",
    "        nn_HV = K.dot(dotted_output, ___)\n",
    "        output = self.activation(___ + ___)\n",
    "        return output, [output]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, 5)                 370       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 160,376\n",
      "Trainable params: 160,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our PRU RNN\n",
    "# we will build a simple model similar to the previous exercise\n",
    "# We will use the functional API to do this\n",
    "\n",
    "hidden_state_units = 5 \n",
    "\n",
    "# Specify the input dimensions HINT: It is max_words\n",
    "inputs = Input(shape=(max_words,))\n",
    "# The inputs will go in an embedding layer\n",
    "embedding = Embedding(vocabulary_size,embedding_size, input_length=max_words)(inputs)\n",
    "\n",
    "# The embeddings will be an input to the PRU layer\n",
    "cell = PRUCell(hidden_state_units)\n",
    "layer = RNN(cell)\n",
    "hidden_output = layer(embedding)\n",
    "# The output from the PRU block will go in a dense layer\n",
    "output = Dense(1, activation='sigmoid')(hidden_output)\n",
    "# Connecting the architecture using tf.keras.models.Model\n",
    "pru_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Get the summary to see if your model is built correctly\n",
    "print(pru_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∏ For the current PRU model, how many weights are associated with the **PPgate**?\n",
    "**Bias is not included**\n",
    "\n",
    "\n",
    "#### A. 370\n",
    "#### B. 10\n",
    "#### C. 185\n",
    "#### D. 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer2 = '____'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using 'binary_crossentropy' loss \n",
    "# and 'adam' optimizer, additionally add 'accuracy' metric\n",
    "pru_model.compile(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "40/40 [==============================] - 6s 146ms/step - loss: 0.6923 - accuracy: 0.5330\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 6s 152ms/step - loss: 0.6785 - accuracy: 0.6459\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 6s 144ms/step - loss: 0.6301 - accuracy: 0.6881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14179fa90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with appropriate batch size and number of epochs\n",
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "pru_model.fit(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 14ms/step - loss: 0.6337 - accuracy: 0.6324\n",
      "The accuracy for the PRU model is 63.24%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the custom test set and report the \n",
    "accuracy = pru_model.evaluate(X_test, y_test)[1]\n",
    "print(f'The accuracy for the PRU model is {100*accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üç≤ Adding the bias to the PRU model\n",
    "\n",
    "Go back and add a bias term to the PRUCell (one for the PPGate and the other for $H_t$)\n",
    "\n",
    "Does your model performance improve under the same training conditions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow3) ###\n",
    "# Type your answer within in the quotes given\n",
    "answer3 = '___'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
