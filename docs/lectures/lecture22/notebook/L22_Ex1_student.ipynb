{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Unigram  LM\n",
    "\n",
    "## Description :\n",
    "Text data is unlike the typical \"design matrix\", i.i.d. data that we've often worked with. Here, you'll gain practice working with actual words, as you'll parse, count, and calculate a probability.\n",
    "\n",
    "An individual unigram's likelihood (**unsmoothed**) is defined as:\n",
    "\n",
    "$$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)}{n_o\\left(D_t\\right)}$$\n",
    "\n",
    "where the numerator represents the number of times word $w$ appeared in the training corpus $D_t$.\n",
    "\n",
    "For this exercise, we will define the **smoothed** unigram's likelihood as:\n",
    "\n",
    "$$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)\\ +\\alpha}{n_o\\left(D_t\\right)\\ +\\alpha\\left|V\\right|}$$\n",
    "\n",
    "where $\\alpha$ is a specified real-valued number (doesn't have to be an integer), and $|V|$ is the cardinality of the lexicon (i.e., the number of distinct word types in the vocabulary)\n",
    "\n",
    "The likelihood of a new sequence $H$ is simply defined by the likelihood of each token, multiplied by each other:\n",
    "\n",
    "$$L\\left(H\\right)=\\prod_{w\\ \\in H}^{ }L\\left(w\\right)$$\n",
    "\n",
    "## HINTS :\n",
    "Depending on your approach, these functions could help you:\n",
    "\n",
    "- `re.sub()` (regular expression)\n",
    "- `.split()`\n",
    "- `.lower()`\n",
    "- `.strip()`\n",
    "- `.replace()`\n",
    "- `.sum()`\n",
    "- `defaultdict` data structure\n",
    "- `Counter` data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports some libraries you might find useful\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for our experiments\n",
    "training_file = \"ex1_train.txt\"\n",
    "dev_file = \"ex1_dev.txt\"\n",
    "punctuation = ['.', '!', '?']\n",
    "\n",
    "sample1 = \"I love data science!\"\n",
    "sample2 = \"I love NLP!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `parse_string()` which takes as input a string (e.g., the contents of a file). It should return this text as a list of tokens. Specifically, the tokens should:\n",
    "- be lowercased\n",
    "- be separated by whitespace and any character present in the list of `punctuation`.\n",
    "- include no trailing or preceeding whitespace (none of the returned tokens should be of white space or empty)\n",
    "\n",
    "For example, if the input is **\" I   LOVE daTa!!\"**, it should return **[\"i\", love\", \"data\", \"!\", \"!\"]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_a) ###\n",
    "def parse_string(text):\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return text\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "text = open(training_file).read()\n",
    "tokens = parse_string(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `count_tokens()` that takes a list of tokens and simply outputs a dictionary-style count of the items. For example, if the input is **['run', 'forrest', 'run']**, it should return a `dict`, `defaultdict`, or `Counter` with 2 keys: **{'run':2, 'forrest':1}** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_b) ###\n",
    "def count_tokens(tokens): \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return word_counts\n",
    "\n",
    "# DO NOT EDIT THIS LINE\n",
    "word_counts = count_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calculate_likelihood()` that takes `tokens` (a list of strings) and `word_counts` (dictionary-type) and returns the likelihood of the sequence of tokens. You will run your function with the tokens parsed from the `sample1` string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_c) ###\n",
    "def calculate_likelihood(tokens, word_counts):\n",
    "    total_likelihood = 1\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "        \n",
    "    return total_likelihood\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "sample1_tokens = parse_string(sample1)\n",
    "likelihood = calculate_likelihood(sample1_tokens, word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calculate_smoothed_likelihood()` that is the same as the previous function but includes a smoothing parameter `alpha`. Again, you should return the likelihood of the sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_d) ###\n",
    "\n",
    "def calculate_smoothed_likelihood(alpha, tokens, word_counts):\n",
    "\n",
    "    total_likelihood = 1\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "        \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return total_likelihood\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "sample1_tokens = parse_string(sample1)\n",
    "sample1_likelihood = calculate_smoothed_likelihood(0.5, sample1_tokens, word_counts)\n",
    "\n",
    "sample2_tokens = parse_string(sample2)\n",
    "sample2_likelihood = calculate_smoothed_likelihood(0.5, sample2_tokens, word_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
