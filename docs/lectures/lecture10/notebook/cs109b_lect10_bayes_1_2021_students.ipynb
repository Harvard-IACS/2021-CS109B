{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Bayes PyMC3\n",
    "\n",
    "## Description :\n",
    "Run this notebook in your own environment. Do not forget to download the images/ and data/ folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "## Lecture 10 - Bayesian Analysis and Introduction to pyMC3\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Spring 2021**<br>\n",
    "**Instructors:** Pavlos Protopapas, Mark Glickman, and Chris Tanner<br>\n",
    "**Additional Instructor and content:** Eleni Angelaki Kaxiras<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "# from pymc3 import summary\n",
    "import arviz as az\n",
    "from matplotlib import gridspec\n",
    "# Ignore a common pymc3 warning that comes from library functions, not our code.\n",
    "# Pymc3 may throw additional warnings, but other warnings should be manageable\n",
    "# by following the instructions included within the warning messages.\n",
    "import warnings\n",
    "messages=[\n",
    "    \"Using `from_pymc3` without the model will be deprecated in a future release\",\n",
    "]\n",
    "\n",
    "# or silence all warnings (not recommended)\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "for m in messages:\n",
    "    warnings.filterwarnings(\"ignore\", message=m)\n",
    "print(f\"Using PyMC3 version: {pm.__version__}\")\n",
    "print(f\"Using ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 20000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas trick\n",
    "pd.options.display.max_columns = 50  # or =None -> No Restrictions\n",
    "pd.options.display.max_rows = 200    # or =None -> *Be careful with this* \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* identify and describe some of the most important probability distributions.\n",
    "* apply Bayes Rule in calculating probabilities (and would have had fun in the process).\n",
    "* create probabilistic models in the PyMC3 library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top></a>\n",
    "\n",
    "## Table of Contents\n",
    "1. [Some common families of distributions (review)](#1)\n",
    "2. [The Bayesian Way of Thinking](#2) \n",
    "3. [Bayesian Regression with pyMC3](#3) and [Defining a model in pyMC3](#model)   \n",
    "4. [Bayesian Logistic Regression with pyMC3](#4)<BR><BR>\n",
    "[Appendix](#appe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=1></a>\n",
    "\n",
    "## 1. Some common families of distributions\n",
    "\n",
    "Statistical distributions are characterized by one of more parameters, such as $\\mu$ and $\\sigma^2$ for a Gaussian distribution. \n",
    "\n",
    "\\begin{equation}\n",
    "Y \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})\n",
    "\\end{equation}  \n",
    "<HR>\n",
    "    \n",
    "### Discrete Distributions\n",
    "\n",
    "The **probability mass function (pmf)** of a discrete random variable $Y$ is equal to the probability that our random variable will take a specific value $y$: $f_y=P(Y=y)$ for all $y$. The variable, most of the times, assumes integer values.  Plots for **pmf**s are better understood as stem plots since we have discrete values for the probabilities and not a curve. Probabilities should add up to 1 for proper distributions. \n",
    "\n",
    "- **Bernoulli** for a binary outcome, success has probability $\\theta$, and we only have $one$ trial:<BR>\n",
    "    \n",
    "\\begin{equation}\n",
    "P(Y=k) =  \\theta^k(1-\\theta)^{1-k}\n",
    "\\end{equation}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- **Binomial** for a binary outcome, success has probability $\\theta$, $k$ successes in $n$ trials:\n",
    "\n",
    "Our random variable is $Y$= number of successes in $n$ trials.\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y=k|n,\\theta) =  {{n}\\choose{k}} \\cdot \\theta^k(1-\\theta)^{n-k} \\quad k=0,1,2,...,n\n",
    "\\end{equation}\n",
    "<BR>\n",
    "As a reminder ${{n}\\choose{k}}$ is \"from $n$ choose $k$\":\n",
    "\\begin{equation}\n",
    "{{n}\\choose{k}} = \\frac{n!}{k!(n-k)!} \n",
    "\\end{equation}\n",
    "\n",
    "$EY=n\\theta$, $VarY = np(1-p)$     \n",
    "\n",
    "**Note** : Binomial (1,$p$) = Bernouli ($p$)  \n",
    "<BR>\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b>: Run the code below (that plots the Binomial distribution using <I>stats.binom.pmf</I>) for various values of the probability for a success $\\theta\\in [0,1]$. Look at the ordinate values to verify that they add up to 1.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabity of success\n",
    "theta = 0.5\n",
    "n = 5\n",
    "k = np.arange(0, n+1)\n",
    "print(k)\n",
    "pmf = stats.binom.pmf(k, n, theta)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.stem(k, pmf, label=r'n = {}, $\\theta$ = {}'.format(n, theta))\n",
    "plt.xlabel('Y', fontsize=14)\n",
    "plt.ylabel('P(Y)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- **Negative Binomial** for a binary outcome, success has probability $\\theta$, we have $r$ successes in $x$ trials:\n",
    "\n",
    "Our random variable is $X$= number of trials to get to $r$ successes.\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x|r,\\theta) =  {{x-1}\\choose{r-1}} \\cdot \\theta^r(1-\\theta)^{x-r} \\quad x=r,r+1,...\n",
    "\\end{equation}\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Poisson** (counts independent events and has a single parameter $\\lambda$)<bR>\n",
    "    \n",
    "\\begin{equation}\n",
    "P\\left( Y=y|\\lambda \\right) = \\frac{{e^{ - \\lambda } \\lambda ^y }}{{y!}} \\quad y = 0,1,2,...\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b>: Change the value of $\\lambda$ in the Poisson pmf plot below and see how the plot changes. Remember that the y-axis in a discrete probability distribution shows the probability of the random variable having a specific value in the x-axis. We use <I>stats.poisson.pmf(x, $\\lambda$)</I>, where $\\lambda$ is the parameter.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "x = np.arange(0, 10)\n",
    "lam = 4\n",
    "pmf = stats.poisson.pmf(x, lam)\n",
    "plt.stem(x, pmf, label='$\\lambda$ = {}'.format(lam))\n",
    "plt.xlabel('Y', fontsize=12)\n",
    "plt.ylabel('P(Y)', fontsize=12)\n",
    "plt.legend(loc=1)\n",
    "plt.ylim=(-0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discrete Uniform** for a random variable $X\\in(1,N)$:<BR>\n",
    "    \n",
    "\\begin{equation}\n",
    "P(X=x|N) = \\frac{1}{N}, \\quad x=1,2,...,N\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# boring but useful as a prior\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "N = 40\n",
    "x = np.arange(0, N)\n",
    "pmf = stats.randint.pmf(x, 0, N)\n",
    "plt.plot(x, pmf, label='$N$ = {}'.format(N))\n",
    "fontsize=14\n",
    "plt.xlabel('X', fontsize=fontsize)\n",
    "plt.ylabel(f'P(X|{N})', fontsize=fontsize)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Categorical, or Multinulli** (random variables can take any of K possible categories, each having its own probability; this is a generalization of the Bernoulli distribution for a discrete variable with more than two possible outcomes, such as the roll of a die)\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1,x_2,...,x_n) =  \\frac{m}{x_1!\\cdot x_2!\\dotsb x_n!} \\cdot p_1^{x_1}\\cdot p_2^{x_2}\\dotsb p_n^{x_n}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "    \n",
    "### Continuous Distributions\n",
    "\n",
    "The random variable has a **probability density function (pdf)** whose area under the curve equals to 1.\n",
    "- **Uniform** (variable equally likely to be near each value in interval $(a,b)$)\n",
    "\\begin{equation}\n",
    "f(x|a,b) = \\frac{1}{b - a} \\quad x\\in [a,b] \\quad \\text{and 0 elsewhere}.\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b>: Change the value of $\\mu$ in the Uniform PDF and see how the plot changes.</div>\n",
    "   \n",
    "Remember that the y-axis in a continuous probability distribution does not shows the actual probability of the random variable having a specific value in the x-axis because that probability is zero!. Instead, to see the probability that the variable is within a small margin we look at the integral below the curve of the PDF.\n",
    "\n",
    "The uniform is often used as a noninformative prior.\n",
    "\n",
    "```\n",
    "Uniform - numpy.random.uniform(a=0.0, b=1.0, size)\n",
    "```\n",
    "\n",
    "$\\alpha$ and $\\beta$ are our parameters. `size` is how many tries to perform.\n",
    "The mean is $\\mu = \\frac{(a+b)}{2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "a = 0\n",
    "b = 1\n",
    "r = uniform.rvs(loc=a, scale=b-a, size=1000) \n",
    "pdf = uniform.pdf(r,loc=a, scale=b-a)\n",
    "plt.plot(r, pdf,'b-', lw=3, alpha=0.6, label='uniform pdf')\n",
    "plt.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
    "plt.ylabel(r'pdf')\n",
    "plt.xlabel(f'x')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Normal** (a.k.a. Gaussian)\n",
    "\\begin{equation}\n",
    "X \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})\n",
    "\\end{equation} \n",
    "\n",
    "    A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma^{2}$. The link between the two is given by\n",
    "\\begin{equation}\n",
    "\\tau = \\frac{1}{\\sigma^{2}}\n",
    "\\end{equation}\n",
    " - Expected value (mean) $\\mu$\n",
    " - Variance $\\frac{1}{\\tau}$ or $\\sigma^{2}$\n",
    " - Parameters: `mu: float`, `sigma: float` or `tau: float`\n",
    " - Range of values (-$\\infty$, $\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "mus = [0., 0., 0., -2.]\n",
    "sigmas = [0.4, 1., 2., 0.4]\n",
    "for mu, sigma in zip(mus, sigmas):\n",
    "    pdf = stats.norm.pdf(x, mu, sigma)\n",
    "    plt.plot(x, pdf, label=r'$\\mu$ = '+ f'{mu},' + r'$\\sigma$ = ' + f'{sigma}') \n",
    "plt.xlabel('random variable', fontsize=12)\n",
    "plt.ylabel('probability density', fontsize=12)\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Beta** (where the variable ($\\theta$) takes on values in the interval $(0,1)$, and is parametrized by two positive parameters, $\\alpha$ and $\\beta$ that control the shape of the distribution. Note that Beta is a good distribution to use for priors (beliefs) because its range is $[0,1]$ which is the natural range for a probability and because we can model a wide range of functions by changing the $\\alpha$ and $\\beta$ parameters. Its density (pdf) is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:beta} \n",
    "P(\\theta|a,b) = \\frac{1}{B(\\alpha, \\beta)} {\\theta}^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\propto {\\theta}^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}\n",
    "\\end{equation}\n",
    "\n",
    "where the normalisation constant, $B$, is a beta function of $\\alpha$ and $\\beta$,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "B(\\alpha, \\beta) = \\int_{x=0}^1 x^{\\alpha - 1} (1 - x)^{\\beta - 1} dx.\n",
    "\\end{equation}\n",
    " - 'Nice', unimodal distribution\n",
    " - Range of values $[0, 1]$\n",
    "\n",
    "$EX = \\frac{a}{a+b}$,    $VarX = \\frac{ab}{(a+b)^2(a+b+1)}$\n",
    "<BR><BR>\n",
    "<div class=\"exercise\"><b>Exercise</b>: Try out various combinations of $a$ and $b$. We get an amazing set of shapes by tweaking the two parameters $a$ and $b$. Notice that for $a=b=1.$ we get the uniform distribution. As the values increase, we get a curve that looks more and more Gaussian. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "fontsize = 15\n",
    "alphas = [0.5] #, 0.5, 1., 3., 6.]\n",
    "betas = [0.5] #, 1., 1., 3., 6.]\n",
    "x = np.linspace(0, 1, 1000) \n",
    "colors = ['red', 'green', 'blue', 'black', 'pink']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "for a, b, colors in zip(alphas, betas, colors):\n",
    "    plt.plot(x, beta.pdf(x,a,b), c=colors,\n",
    "             label=f'a={a}, b={b}')\n",
    "\n",
    "ax.set_ylim(0, 3)\n",
    "\n",
    "ax.set_xlabel(r'$\\theta$', fontsize=fontsize)\n",
    "ax.set_ylabel(r'P ($\\theta|\\alpha,\\beta)$', fontsize=fontsize)\n",
    "ax.set_title('Beta Distribution', fontsize=fontsize*1.2)\n",
    "\n",
    "ax.legend(loc='best')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"> <b>At home</b>: Prove the formula mentioned in class which gives the probability density for a Beta distribution with parameters $2$ and $5$:<BR>\n",
    "$p(\\theta|2,5) = 30 \\cdot \\theta(1 - \\theta)^4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Code Resources:\n",
    " - Statistical Distributions in numpy/scipy: [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=2></a> [Top](#top)\n",
    "\n",
    "## 2. The Bayesian way of Thinking\n",
    "\n",
    "```\n",
    "Here is my state of knowledge about the situation. Here is some data, I am now going to revise my state of knowledge.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:bayes} \n",
    "P(A|\\textbf{B}) = \\frac{P(\\textbf{B} |A) P(A) }{P(\\textbf{B})} \n",
    "\\end{equation}\n",
    "\n",
    "$P(A|\\textbf{B})$ is the **posterior** distribution, probability(parameters| data). \n",
    "\n",
    "$P(\\textbf{B} |A)$ is the **likelihood** function, how probable is my data for different values of the parameters.\n",
    "\n",
    "$P(A)$ is the marginal probability to observe the data, called the **prior**, this captures our belief about the data before observing it.\n",
    "\n",
    "$P(\\textbf{B})$ is the marginal distribution (sometimes called marginal likelihood). This serves a normalization purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Make a Deal  \n",
    "<img src=\"images/montyhall.png\">\n",
    "    \n",
    "The problem we are about to solve gained fame as part of a game show \"Let's Make A Deal\" hosted by Monty Hall, hence its name. It was first raised by Steve Selvin in American Statistician in 1975.\n",
    "    \n",
    "The game is as follows: there are 3 doors behind **one** of which are the keys to a new car. There is a goat behind each of the other two doors. Let's assume your goal is to get the car and not a goat.\n",
    "\n",
    "You are asked to pick one door, and let's say you pick **Door1**. The host knows where the keys are. Of the two remaining closed doors, he will always open the door that has a goat behind it. He'll say \"I will do you a favor and open **Door2**\". So he opens Door2 inside which there is, of course, a goat. He now asks you, do you want to open the initial Door1 you chose or change to **Door3**? Generally, in this game, when you are presented with this choice should you swap the doors?<BR><BR>\n",
    "**Hint:**\n",
    "- Start by defining the `events` of this probabilities game. One definition is:\n",
    "    \n",
    "    - $A_i$: car is behind door $i$    \n",
    "        \n",
    "    - $B_i$ host opens door $i$\n",
    "    \n",
    "$i\\in[1,2,3]$\n",
    "      \n",
    "- In more math terms, the question is: is the probability of **the price is behind Door 1** higher than the probability of **the price is behind Door2**, given that an event **has occured**?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\" style=\"background-color:#b3e6ff\"><b>Breakout Game</b>: Solve the Monty Hall Paradox using Bayes Rule.<BR></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes rule revisited \n",
    "\n",
    "We have data that we believe come from an underlying distribution of unknown parameters. If we find those parameters, we know everything about the process that generated this data and we can make inferences (create new data).\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \n",
    "\\end{equation}\n",
    "\n",
    "#### But what is $\\theta \\;$?\n",
    "\n",
    "$\\theta$ is an unknown yet fixed set of parameters. In Bayesian inference we express our belief about what $\\theta$ might be and instead of trying to guess $\\theta$ exactly, we look for its **probability distribution**. What that means is that we are looking for the **parameters** of that distribution. For example, for a Poisson distribution our $\\theta$ is only $\\lambda$. In a normal distribution, our $\\theta$ is often just $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=3></a> [Top](#top)\n",
    "\n",
    "## 3. Bayesian Regression with `pyMC3`\n",
    " \n",
    "PyMC3 is a Python library for programming Bayesian analysis, and more specifically, data creation, model definition, model fitting, and posterior analysis. It uses the concept of a `model` which contains assigned parametric statistical distributions to unknown quantities in the model. Within models we define random variables and their distributions. A distribution requires at least a `name` argument, and other `parameters` that define it. You may also use the `logp()` method in the model to build the model log-likelihood function. We define and fit the model.\n",
    "\n",
    "PyMC3 includes a comprehensive set of pre-defined statistical distributions that can be used as model building blocks. They are not meant to be used outside of a `model`, and you can invoke them by using the prefix `pm`, as in `pm.Normal`. \n",
    "\n",
    "For more see: [PyMC3 Quickstart](https://docs.pymc.io/notebooks/api_quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Distributions in `PyMC3`:\n",
    " - Statistical [Distributions in pyMC3](https://docs.pymc.io/api/distributions.html). \n",
    " \n",
    "Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the `help` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(pm.Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Model in PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem is the following: we want to perform multiple linear regression to predict an outcome variable $Y$ which depends on variables $\\bf{x}_1$ and $\\bf{x}_2$.\n",
    "\n",
    "We will model $Y$ as normally distributed observations with an expected value $mu$ that is a linear function of the two predictor variables, $\\bf{x}_1$ and $\\bf{x}_2$.\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\mu = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 \n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma^2$ represents the measurement error (in this example, we will use $\\sigma^2 = 10$)\n",
    "\n",
    "We also choose the parameters to have normal distributions with those parameters set by us.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_i \\sim  \\mathcal{N}(0,\\,10) \\\\\n",
    "\\sigma^2 \\sim  |\\mathcal{N}(0,\\,10)|\n",
    "\\end{eqnarray}   \n",
    "We will artificially create the data to predict on. We will then see if our model predicts them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create some synthetic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "###################################\n",
    "## Hidden true parameter values \n",
    "sigma = 1\n",
    "beta0 = 1\n",
    "beta = [1, 2.5]   \n",
    "####################################\n",
    "\n",
    "# Size of dataset\n",
    "size = 100\n",
    "\n",
    "# Predictor variable\n",
    "x1 = np.linspace(0, 1., size)\n",
    "x2 = np.linspace(0,2., size)\n",
    "\n",
    "# Simulate outcome variable\n",
    "Y = beta0 + beta[0]*x1 + beta[1]*x2 + np.random.randn(size)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape, x1.shape, x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "fontsize=14\n",
    "labelsize=8\n",
    "title='Observed Data (created artificially by ' + r'$Y(x_1,x_2)$)'\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x1, x2, Y)\n",
    "ax.set_xlabel(r'$x_1$', fontsize=fontsize)\n",
    "ax.set_ylabel(r'$x_2$', fontsize=fontsize)\n",
    "ax.set_zlabel(r'$Y$', fontsize=fontsize)\n",
    "\n",
    "ax.tick_params(labelsize=labelsize)\n",
    "\n",
    "fig.suptitle(title, fontsize=fontsize)        \n",
    "fig.tight_layout(pad=.1, w_pad=10.1, h_pad=2.)\n",
    "fig.subplots_adjust(); #top=0.5\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model\n",
    "\n",
    "**Step1:** Formulate the probability model for our data: $Y \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})$\n",
    "\n",
    "```\n",
    "Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=Y)\n",
    "```\n",
    "\n",
    "**Step2:** Choose a prior distribution for our unknown parameters. \n",
    "```\n",
    " beta0 = Normal('beta0', mu=0, sd=10)\n",
    " # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2)\n",
    " # so, in array notation, our beta1 = betas[0], and beta2=betas[1]\n",
    " betas = Normal('betas', mu=0, sd=10, shape=2) \n",
    " sigma = HalfNormal('sigma', sd=1)\n",
    "\n",
    "```\n",
    "**Step3:** Construct the likelihood function.\n",
    "\n",
    "**Step4:** Determine the posterior distribution, **this is our main goal**.\n",
    "\n",
    "**Step5:** Summarize important features of the posterior and/or plot the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as my_linear_model:\n",
    "\n",
    "    # Priors for unknown model parameters, specifically created stochastic random variables \n",
    "    # with Normal prior distributions for the regression coefficients,\n",
    "    # and a half-normal distribution for the standard deviation of the observations.\n",
    "    # These are our parameters.\n",
    "    \n",
    "    # I need to give the prior some initial values for the parameters\n",
    "    mu0 = 0\n",
    "    sd0 = 10\n",
    "    beta0 = pm.Normal('beta0', mu=mu0, sd=sd0)    \n",
    "    # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2)\n",
    "    # so, in array notation, our beta1 = betas[0], and beta2=betas[1]\n",
    "    mub = 0\n",
    "    sdb = 10\n",
    "    betas = pm.Normal('betas', mu=mub, sd=sdb, shape=2) \n",
    "    sds = 1\n",
    "    sigma = pm.HalfNormal('sigma', sd=sds)\n",
    "    \n",
    "    # mu is what is called a deterministic random variable, which implies that \n",
    "    # its value is completely determined by its parents’ values \n",
    "    # (betas and sigma in our case). There is no uncertainty in the \n",
    "    # variable beyond that which is inherent in the parents’ values\n",
    "    \n",
    "    mu = beta0 + betas[0]*x1 + betas[1]*x2\n",
    "    \n",
    "    # Likelihood function = how probable is my observed data?\n",
    "    # This is an observed variable; it is identical to a standard \n",
    "    # stochastic variable, except that its observed argument, \n",
    "    # which passes the data to the variable, indicates that the values for this \n",
    "    # variable were observed, and should not be changed by any \n",
    "    # fitting algorithm applied to the model. \n",
    "    # The data can be passed in the form of a numpy.ndarray or pandas.DataFrame object.\n",
    "    \n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_linear_model.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_linear_model.free_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If our problem was a classification for which we would use Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## do not worry if this does not work, it's just a nice graph to have\n",
    "## you need to install python-graphviz first\n",
    "# conda install -c conda-forge python-graphviz\n",
    "pm.model_to_graphviz(my_linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is sample our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... to be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=appeA></a> \n",
    "\n",
    "## Appendix A: Bayesian Logistic Regression with `pyMC3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the problem above was a classification that required a Logistic Regression, we would use the logistic function ( where $\\beta_0$ is the intercept, and $\\beta_i$ (i=1, 2, 3) determines the shape of the logistic function).\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(Y=1|X_1,X_2,X3) = {\\frac{1}{1 + exp^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3)}}}\n",
    "\\end{equation}\n",
    "\n",
    "Since both $\\beta_0$ and the $\\beta_i$s can be any possitive or negative number, we can model them as gaussian random variables.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 \\sim  \\mathcal{N}(\\mu,\\,\\sigma^2) \\\\\n",
    "\\beta_i \\sim  \\mathcal{N}(\\mu_i,\\,\\sigma_i^2)\n",
    "\\end{eqnarray} \n",
    "\n",
    "In PyMC3 we can model those as:\n",
    "```\n",
    "pm.Normal('beta_0', mu=0, sigma=100)\n",
    "```\n",
    "(where $\\mu$ and $\\sigma^2$ can have some initial values that we assign them, e.g. 0 and 100)\n",
    "\n",
    "The dererministic variable would be:\n",
    "```\n",
    "logitp = beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3\n",
    "```\n",
    "To connect this variable (logit-p) with our observed data, we would use a Bernoulli as our likelihood.\n",
    "```\n",
    "our_likelihood = pm.Bernoulli('our_likelihood', logit_p=logitp, observed=our_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the main difference with Linear Regression is the use of a Bernoulli distribution instead of a Gaussian distribution, and the use of the logistic function instead of the identity function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\" style=\"background-color:#b3e6ff\"><b>Breakout Room Exercise</b>: Write the model above in code. Suppose that your training dataframe (df_train) has the following features:\n",
    "\n",
    "**numerical**\n",
    "- df_train['age']\n",
    "- df_train['weight'] \n",
    "    \n",
    "**categorical**\n",
    "- df_train['hypertension'] \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A reminder of what the logistic function looks like. \n",
    "# Change parameters a and b to see the shape of the curve change\n",
    "b = 5.\n",
    "x = np.linspace(-8, 8, 100)\n",
    "plt.plot(x, 1 / (1 + np.exp(-b*x)))\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y=logistic(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=appe></a> \n",
    "\n",
    "## Appendix B: Is this a fair coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this a fair coin?\n",
    "\n",
    "Let's say you visit the casino in **Monte Carlo**. You want to test your theory that casinos are dubious places where coins have been manipulated to have a larger probability for tails. So you will try to estimate how fair a coin is based on a certain amount of  flips. <BR>\n",
    "You have no prior opinion on the coin's fairness (i.e. what $p$ might be), and begin flipping the coin. You get either Heads ($H$) or Tails ($T$) as our observed data and want to see if your posterior probabilities change as you obtain more data, that is, more coin flips. A nice way to visualize this is to plot the posterior probabilities as we observe more flips (data). \n",
    "\n",
    "We will be using Bayes rule. $\\textbf{D}$ is our data.\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \n",
    "\\end{equation}\n",
    "\n",
    "We start with a non-informative prior, a Beta distribution with (a=b=1.)\n",
    "    \n",
    "\\begin{equation}\n",
    "P(\\theta|\\textbf{k=0}) = Beta(1., 1.) \n",
    "\\end{equation}\n",
    "    \n",
    "Then, as we get new data (say, we observe $k$ heads in $n$ tosses), we update our Beta with new a,b as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "P(\\theta|\\textbf{k}) = Beta(\\alpha + \\textbf{k}, \\beta + (n - \\textbf{k})) \n",
    "\\end{equation}\n",
    "\n",
    "*(the proof is beyond our scope, if interested, see this [Wikipedia article](https://en.wikipedia.org/wiki/Conjugate_prior#Example))*\n",
    "    \n",
    "we can say that $\\alpha$ and $\\beta$ play the roles of a \"prior number of heads\" and \"prior number of tails\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with the priors - here we manually set them but we could be sampling from a separate Beta\n",
    "trials = np.array([0, 1, 3, 5, 10, 15, 20, 100, 200, 300])\n",
    "heads = np.array([0, 1, 2, 4, 8, 10, 10, 50, 180, 150])\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "# for simplicity we set a,b=1\n",
    "plt.figure(figsize=(10,8))\n",
    "for k, N in enumerate(trials):\n",
    "    sx = plt.subplot(len(trials)/2, 2, k+1)\n",
    "    posterior = stats.beta.pdf(x, 1 + heads[k], 1 + trials[k] - heads[k]) \n",
    "    plt.plot(x, posterior, alpha = 0.5, label=f'{trials[k]} tosses\\n {heads[k]} heads');\n",
    "    plt.fill_between(x, 0, posterior, color=\"#348ABD\", alpha=0.4) \n",
    "    plt.legend(loc='upper left', fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.autoscale(tight=True)\n",
    "    \n",
    "plt.suptitle(\"Posterior probabilities for coin flips\", fontsize=15);\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- *Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55* [(https://doi.org/10.7717/peerj-cs.55)](https://doi.org/10.7717/peerj-cs.55)\n",
    "- [Distributions in PyMC3](https://docs.pymc.io/api/distributions.html)\n",
    "- [More Details on Distributions](https://docs.pymc.io/developer_guide.html)\n",
    "\n",
    "This linear regression example is from the original paper on PyMC3: *Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55*\n",
    "\n",
    "#### Cool Reading\n",
    "\n",
    "- How Bayesian Analysis and Lawrence D. Stone found the Wreckage of [Air France Flight AF 447](https://www.jstor.org/stable/pdf/43288452.pdf?refreqid=excelsior%3Ae972f3a7b38b5815e1570b93dad7d269).\n",
    "- Search for the gold on the sunken [SS Central America](https://www.metsci.com/wp-content/uploads/2019/08/Search-for-the-SS-Central-America-Mathematical-Treasure-Hunting-Interfaces-22-1-Jan.-Feb.-1992.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
