{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "Exercise: Setting up a custom environment\n",
    "\n",
    "## Description\n",
    "The aim of this exercise is to learn how to set up a custom environment using <a href=\"https://gym.openai.com/\" target=\"_blank\">OpenAI Gym</a>. OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pinball.\n",
    "\n",
    "As we have discussed in class, the environment will consist of a state, reward and action. For our custom environment, we would like to implement the mouse grid we saw in the lectures. The possible rewards and state given the current state are in the helper file. \n",
    "\n",
    "The overall reward pattern is as follows, every next state gets a plus one reward, a terminal state leads to -10 and the mouse has to start from state 1 again. Every action leading to a previous state is penalized by 1 and reaching the goal i.e. state 16 gets a +100 reward.\n",
    "\n",
    "\n",
    "<img src=\"../fig/fig1.png\" style=\"width: 500px;\">\n",
    "\n",
    "- Create a class `MouseGrid` that inherits from OpenAI Gym's Env. This class will be your environment.\n",
    "- Within the class constructor `__init__`, initialize the observation_space and action_space. \n",
    "    - The observation space is the set of all possible state values, which are 16 in our case.\n",
    "    - The action space is the set of all possible actions one can take in an environment. 1 indicates up, 1 indicates left, 3 indicates right and 4 indicates down.\n",
    "- Set the initial state to be 1 and the reward to be zero.\n",
    "- Define a method step that take the action and returns the reward based on the instructions given in the helper file.\n",
    "- Define a method reset, that reset the class variables to their initial value.\n",
    "- Select the number of episodes.\n",
    "- Create an instance of the MouseGrid environment.\n",
    "- Loop over the total number of episodes.\n",
    "    - Sample an action from the set of possible action and get call the step method.\n",
    "    - At the end of each episode, print the total reward of that episode.\n",
    "\n",
    "NOTE - Remember that the reward can be negative as well. It depends on how the reward system is defined within the environment. \n",
    "\n",
    "In this exercise, we are using a random policy, by randomly sampling from the set of possible actions defined within the environment.\n",
    "\n",
    "\n",
    "## Hints: \n",
    "\n",
    "<a href=\"https://gym.openai.com/docs/#spaces\" target=\"_blank\">Discrete()</a> The Discrete space allows a fixed range of non-negative numbers\n",
    "\n",
    "Env.action_space.sample()\n",
    "\n",
    "To select a value randomly from the action space defined in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete\n",
    "from helper import transition_rules, reward_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Refer to the Hints in the Description before you start ***\n",
    "\n",
    "# Class MouseGrid that inherits from OpenAI Gym Env\n",
    "class MouseGrid(Env):\n",
    "\n",
    "  # Constructor that is called when an instance of the class is created\n",
    "  def __init__(self):\n",
    "\n",
    "    # Set the observation_space i.e the state to 16 discrete values\n",
    "    self.observation_space = ___\n",
    "\n",
    "    # Set the action space to 4 discrete values i.e up, left, right and down \n",
    "    # Action 1: Up\n",
    "    # Action 2: Left\n",
    "    # Action 3: Right\n",
    "    # Action 4: Down\n",
    "    self.action_space = ___\n",
    "\n",
    "    # Set the initial state of the mouse to 1\n",
    "    self.state = ___\n",
    "\n",
    "    # Set the initial reward as 0\n",
    "    self.reward = ___\n",
    "\n",
    "  # Define a function step that inputs an action and updates the reward\n",
    "  def step(self, action):\n",
    "\n",
    "    # Update the action to the action sent as the parameter of this function\n",
    "    self.action = ___\n",
    "\n",
    "    # Set a variable prev_state as the current state of the environment\n",
    "    prev_state = ___\n",
    "\n",
    "    # Update the state based on the current state and action by \n",
    "    # calling the transition_rules function defined in the helper\n",
    "    # file with the current state and action\n",
    "    self.state = ___\n",
    "\n",
    "    # Update the reward by calling the function reward_rules defined in \n",
    "    # the helper file by passing the current state and previous state\n",
    "    # Remember that the reward is cummulative\n",
    "    self.reward = ___\n",
    "\n",
    "    # If the current state is 16 that means our mouse has reached the goal\n",
    "    # For this, we set the call the reward_rules function again\n",
    "    # Set done as True\n",
    "    if self.state==16:\n",
    "      self.reward = self.reward + reward_rules(self.state, prev_state)\n",
    "      done = True\n",
    "\n",
    "    # Else set done as false\n",
    "    else:\n",
    "      done = False\n",
    "\n",
    "    # Return the state, reward and done to indicate whether an episode is complete\n",
    "    return self.state, self.reward, done\n",
    "\n",
    "\n",
    "  # The reset function which is called at the end of each episode\n",
    "  def reset(self):\n",
    "\n",
    "    # Reset the initial state to the start point\n",
    "    self.state = ___\n",
    "\n",
    "    # Reset the reward to 0\n",
    "    self.reward = ___\n",
    "\n",
    "    # Set done as False\n",
    "    done = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET THE AGENT TO TEST THE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow0) ###\n",
    "# Create an instance of the custom environment\n",
    "env = ___\n",
    "\n",
    "# Set the maximum number of episodes to any integer between 10 and 50\n",
    "episodes = ___\n",
    "\n",
    "# Loop over all the steps\n",
    "for i in range(episodes):\n",
    "\n",
    "  # Set done as False to run until the end of the episode\n",
    "  done = False\n",
    "\n",
    "  # Loop over the entire episode\n",
    "  while done!=True:\n",
    "\n",
    "    # Sample an action from the action_space of the environment\n",
    "    action = ___\n",
    "\n",
    "    # Call the step function within the environment\n",
    "    state, reward, done = ___\n",
    "\n",
    "  # Call the reset function at the end of an episode\n",
    "  env.reset()\n",
    "\n",
    "  # Print the reward at the end of each episode\n",
    "  print(\"The reward of this episode is:\",reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Here we are using random sampling to pick an action for a given state. However, if you had a policy, which part of the exercise would you change to incorporate it?\n",
    "\n",
    "#### A. The step() method of the MouseGrid class.\n",
    "#### B. self.action within the __init__() method of the MouseGrid class.\n",
    "#### C. getting an action for each step within the for loop over all episodes.\n",
    "#### D. Call to the step() method in the last cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Which of the following would be an issue if the reset() method is not called at the end of each episode?\n",
    "\n",
    "#### A. The next episode will not run as the state is 16.\n",
    "#### B. The action sampled next will be biased on the previous value.\n",
    "#### C. The reward of the new episode will be combined to the reward of the previous episode.\n",
    "#### D. The reset() method does not affect anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below \n",
    "# There can be multiple correct answers. Replace the options with a hyphen\n",
    "# For example if you think the correct choice is A and D, then type 'A-D'\n",
    "answer2 = '___'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
