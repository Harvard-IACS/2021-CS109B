{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs109b_sec5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dvo124IepUXA",
        "P-fg7Co6pjKt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMmMOVPOQx3B"
      },
      "source": [
        "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science \n",
        "## Section 5: Natural Language Processing\n",
        "\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Spring 2021**<br/>\n",
        "**Instructors**: Mark Glickman, Pavlos Protopapas, and Chris Tanner <br/>\n",
        "**Authors**: Shivas Jayaram \n",
        "\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1ID5PMGCQ4fL",
        "outputId": "c543aae3-37f5-4e62-8f2a-69b0a9e07c10"
      },
      "source": [
        "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
        "HTML(styles)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>\n",
              "blockquote { background: #AEDE94; }\n",
              "h1 { \n",
              "    padding-top: 25px;\n",
              "    padding-bottom: 25px;\n",
              "    text-align: left; \n",
              "    padding-left: 10px;\n",
              "    background-color: #DDDDDD; \n",
              "    color: black;\n",
              "}\n",
              "h2 { \n",
              "    padding-top: 10px;\n",
              "    padding-bottom: 10px;\n",
              "    text-align: left; \n",
              "    padding-left: 5px;\n",
              "    background-color: #EEEEEE; \n",
              "    color: black;\n",
              "}\n",
              "\n",
              "div.exercise {\n",
              "\tbackground-color: #ffcccc;\n",
              "\tborder-color: #E9967A; \t\n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "}\n",
              "div.discussion {\n",
              "\tbackground-color: #ccffcc;\n",
              "\tborder-color: #88E97A;\n",
              "\tborder-left: 5px solid #0A8000; \n",
              "\tpadding: 0.5em;\n",
              "}\n",
              "div.theme {\n",
              "\tbackground-color: #DDDDDD;\n",
              "\tborder-color: #E9967A; \t\n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "\tfont-size: 18pt;\n",
              "}\n",
              "div.gc { \n",
              "\tbackground-color: #AEDE94;\n",
              "\tborder-color: #E9967A; \t \n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "\tfont-size: 12pt;\n",
              "}\n",
              "p.q1 { \n",
              "    padding-top: 5px;\n",
              "    padding-bottom: 5px;\n",
              "    text-align: left; \n",
              "    padding-left: 5px;\n",
              "    background-color: #EEEEEE; \n",
              "    color: black;\n",
              "}\n",
              "header {\n",
              "   padding-top: 35px;\n",
              "    padding-bottom: 35px;\n",
              "    text-align: left; \n",
              "    padding-left: 10px;\n",
              "    background-color: #DDDDDD; \n",
              "    color: black;\n",
              "}\n",
              "</style>\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR4B93_FQ0aG"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this section, you should be able to:\n",
        "* Perform machine translation task to understand the following model architectures:\n",
        "  * Sequence2Sequence model\n",
        "  * Sequence2Sequence with Attention model\n",
        "  * Transformer\n",
        "* Familiarize with key concepts in `Tensorflow` such as:\n",
        "  * Building custom `Layers`\n",
        "  * Custom training loops using `GradientTape`\n",
        "  * Custom `loss` functions\n",
        "  * Adding learning rate schedulers in your `optimizer`\n",
        "  * View model predictions during training using custom `callbacks`\n",
        "* Build a Masked Language model using Mini BERT\n",
        "  * Familiarize with BERT(Bidirectional Encoder Representations from Transformers) architecture\n",
        "  * Undertand the postional encoding in transfomer based models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BwWBmoGOWod"
      },
      "source": [
        "<a id=\"contents\"></a>\n",
        "\n",
        "## Notebook Contents\n",
        "- [**Neural Machine Translation**](#nmt)\n",
        "    - [Overview](#nmt) \n",
        "    - [Sequence 2 Sequence](#nmt)\n",
        "    - [Sequence 2 Sequence with Attention](#nmt)\n",
        "    - [Transfomer](#nmt)\n",
        "- [**Mini BERT Language Model**](#bert)\n",
        "    - [Overview](#bert) \n",
        "    - [Data Generation for Language Model](#bert)\n",
        "    - [Build Mini BERT](#bert)\n",
        "    - [Break Out Room 🎊🎉](#breakout)\n",
        "    - [Training with / without Postional Encoding](#bert)\n",
        "- [**References**](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCeFFZ5SOW_d"
      },
      "source": [
        "## **Setup Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy8nqgf6OXXq"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJYd7A-kQovK"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import shutil\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import collections\n",
        "import unicodedata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.utils.layer_utils import count_params\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBYEAY5lQj6z",
        "outputId": "1b632898-c8f1-4be1-f734-a3b508fd99fe"
      },
      "source": [
        "# download nltk's punkt sentence tokenizer\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-vCYdIpOkkt"
      },
      "source": [
        "**Verify Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-wfBzhYOk32",
        "outputId": "7f72b342-a442-469c-f259-73f3b9d01b06"
      },
      "source": [
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
        "# without building graphs\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "print(\"tensorflow version\", tf.__version__)\n",
        "print(\"keras version\", tf.keras.__version__)\n",
        "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Get the number of replicas \n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "devices = tf.config.experimental.get_visible_devices()\n",
        "print(\"Devices:\", devices)\n",
        "print(tf.config.experimental.list_logical_devices('GPU'))\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/data_performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version 2.4.1\n",
            "keras version 2.4.0\n",
            "Eager Execution Enabled: True\n",
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "Number of replicas: 1\n",
            "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "[]\n",
            "GPU Available:  []\n",
            "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkyvXbOfOm0b"
      },
      "source": [
        "**Utils**\n",
        "\n",
        "Here we define some helper functions that would be used in loading data, saving models and evaluating models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIfFh3IjOpmU"
      },
      "source": [
        "def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n",
        "  if base_path != \"\":\n",
        "    if not os.path.exists(base_path):\n",
        "      os.mkdir(base_path)\n",
        "  packet_file = os.path.basename(packet_url)\n",
        "  with requests.get(packet_url, stream=True, headers=headers) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(os.path.join(base_path,packet_file), 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "  \n",
        "  if extract:\n",
        "    if packet_file.endswith(\".zip\"):\n",
        "      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n",
        "        zfile.extractall(base_path)\n",
        "    else:\n",
        "      packet_name = packet_file.split('.')[0]\n",
        "      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n",
        "        tfile.extractall(base_path)\n",
        "\n",
        "class JsonEncoder(json.JSONEncoder):\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, decimal.Decimal):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return super(JsonEncoder, self).default(obj)\n",
        "\n",
        "def compute_blue_scores(model, inputs, outputs, translate_fn):\n",
        "  actual = []\n",
        "  predicted = []\n",
        "  for idx,test_text in enumerate(inputs):\n",
        "    actual_op = outputs[idx]\n",
        "    \n",
        "    source_text, output_text = translate_fn(model,test_source_text=test_text)\n",
        "    actual.append([actual_op.split()])\n",
        "    predicted.append(output_text.split())\n",
        "    \n",
        "  smooth = SmoothingFunction().method4\n",
        "  blue_scores = {\n",
        "      \"BLEU-1\": corpus_bleu(actual, predicted,smoothing_function=smooth, weights=(1.0, 0, 0, 0)),\n",
        "      \"BLEU-2\": corpus_bleu(actual, predicted,smoothing_function=smooth, weights=(0.5, 0.5, 0, 0)),\n",
        "      \"BLEU-3\": corpus_bleu(actual, predicted,smoothing_function=smooth, weights=(0.3, 0.3, 0.3, 0)),\n",
        "      \"BLEU-4\": corpus_bleu(actual, predicted,smoothing_function=smooth, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  }\n",
        "\n",
        "  return blue_scores\n",
        "\n",
        "def evaluate_save_model(model,test_in, test_out, translate_fn, training_results,execution_time, learning_rate, epochs):\n",
        "    \n",
        "  # Get the model train history\n",
        "  model_train_history = training_results.history\n",
        "  # Get the number of epochs the training was run for\n",
        "  num_epochs = len(model_train_history[\"loss\"])\n",
        "\n",
        "  # Plot training results\n",
        "  fig = plt.figure(figsize=(20,5))\n",
        "  axs = fig.add_subplot(1,3,1)\n",
        "  axs.set_title('Loss')\n",
        "  # Plot all metrics\n",
        "  for metric in [\"loss\"]:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  # Evaluate on test data\n",
        "  blue_scores = compute_blue_scores(model,test_in, test_out, translate_fn)\n",
        "  print(blue_scores)\n",
        "\n",
        "  # Ensure path exists\n",
        "  if not os.path.exists(\"models\"):\n",
        "    os.mkdir(\"models\")\n",
        "  \n",
        "  # Save model history\n",
        "  with open(os.path.join(\"models\",model.name+\"_train_history.json\"), \"w\") as json_file:\n",
        "      json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n",
        "\n",
        "  trainable_parameters = count_params(model.trainable_weights)\n",
        "  non_trainable_parameters = count_params(model.non_trainable_weights)\n",
        "\n",
        "  # Save model metrics\n",
        "  metrics ={\n",
        "      \"trainable_parameters\":trainable_parameters,\n",
        "      \"execution_time\":execution_time,\n",
        "      \"loss\":model_train_history[\"loss\"][-1],\n",
        "      \"BLEU-1\": blue_scores[\"BLEU-1\"],\n",
        "      \"BLEU-2\": blue_scores[\"BLEU-2\"],\n",
        "      \"BLEU-3\": blue_scores[\"BLEU-3\"],\n",
        "      \"BLEU-4\": blue_scores[\"BLEU-4\"],\n",
        "      \"learning_rate\":learning_rate,\n",
        "      \"epochs\":epochs,\n",
        "      \"name\": model.name,\n",
        "      \"id\": int(time.time())\n",
        "  }\n",
        "  with open(os.path.join(\"models\",model.name+\"_metrics.json\"), \"w\") as json_file:\n",
        "      json_file.write(json.dumps(metrics,cls=JsonEncoder))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDCFNK5OxV0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjnhVOzjOxpI"
      },
      "source": [
        "## **Neural Machine Translation** <div id='nmt'>\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIZKC28PB6V"
      },
      "source": [
        "### **Overview**\n",
        "\n",
        "In this section, we're going to perform machine translation from english to french using an encoder decoder architecture. We will explore various model architectures.\n",
        "\n",
        "**The Task:** Perform Neural Machine Translation from english to french."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5GWMgf6PFiR"
      },
      "source": [
        "### **Dataset**\n",
        "\n",
        "The dataset consists of English to French translations from http://www.manythings.org/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HELDUa9xQelD"
      },
      "source": [
        "#### **Download**\n",
        "\n",
        "Download the datasets to colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z6q7p3XQfP0",
        "outputId": "d33f4662-94f4-40c2-fa16-031a78406f65"
      },
      "source": [
        "start_time = time.time()\n",
        "headers = {\n",
        "  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
        "}\n",
        "download_file(\"http://www.manythings.org/anki/fra-eng.zip\", base_path=\"datasets\", extract=True, headers=headers)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Download execution time (mins)\",execution_time)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download execution time (mins) 0.013792125384012859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wubioOfIQnpQ"
      },
      "source": [
        "#### **Load Data**\n",
        "\n",
        "* Read-in data as dataframe.\n",
        "* Data in column`english` is our inputs\n",
        "* Data in column`french` is our outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "XBs7UG0CQxzp",
        "outputId": "86dfc5a2-2da3-43b8-de20-f6c9524f6f40"
      },
      "source": [
        "# Read the fra-eng raw data \n",
        "english_french_data = pd.read_csv('datasets/fra.txt',delimiter='\\t',header=None, names=[\"english\",\"french\",\"att\"])\n",
        "english_french_data = english_french_data[[\"english\",\"french\"]]\n",
        "print(\"Size\",english_french_data.shape)\n",
        "english_french_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size (185583, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>french</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Marche.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Bouge !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  english   french\n",
              "0     Go.     Va !\n",
              "1     Go.  Marche.\n",
              "2     Go.  Bouge !\n",
              "3     Hi.  Salut !\n",
              "4     Hi.   Salut."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gty-HMm1UA2D",
        "outputId": "dd6fa5b7-7f15-47fd-fda3-7c4877a2e912"
      },
      "source": [
        "subset_data = False\n",
        "if subset_data:\n",
        "  english_french_data = english_french_data[:50000]\n",
        "\n",
        "# Read in the english and french texts\n",
        "data_en = english_french_data[\"english\"].values\n",
        "data_fr = english_french_data[\"french\"].values\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_string(s):\n",
        "  s = unicode_to_ascii(s)\n",
        "  s = re.sub(r'([!.?])', r' \\1', s)\n",
        "  s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "  s = re.sub(r'\\s+', r' ', s)\n",
        "  return s\n",
        "\n",
        "data_en = [preprocess_string(data) for data in data_en]\n",
        "# Add start tag for decoder input\n",
        "data_fr_in = ['<start> ' + preprocess_string(data) for data in data_fr]\n",
        "# Add end tag for decoder output\n",
        "data_fr_out = [preprocess_string(data) + ' <end>' for data in data_fr]\n",
        "\n",
        "print(\"data_en len:\",len(data_en))\n",
        "print(\"data_fr_in len:\",len(data_fr_in))\n",
        "print(\"data_fr_out len:\",len(data_fr_out))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_en len: 185583\n",
            "data_fr_in len: 185583\n",
            "data_fr_out len: 185583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVCuGiRVVvU"
      },
      "source": [
        "#### **View Text**\n",
        "\n",
        "Let's take a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M81mYaKPVX2r",
        "outputId": "9159a7ba-0d18-4033-d448-692bba2d0e9a"
      },
      "source": [
        "# Generate a random sample of index\n",
        "data_samples = np.random.randint(0,high=len(data_en)-1, size=6)\n",
        "for i,data_idx in enumerate(data_samples):\n",
        "  print(\"English:\",data_en[data_idx])\n",
        "  print(\"French (input):\",data_fr_in[data_idx])\n",
        "  print(\"French (output):\",data_fr_out[data_idx])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: They found us .\n",
            "French (input): <start> Elles nous ont trouves .\n",
            "French (output): Elles nous ont trouves . <end>\n",
            "\n",
            "\n",
            "English: I don t compromise .\n",
            "French (input): <start> Je ne transige pas .\n",
            "French (output): Je ne transige pas . <end>\n",
            "\n",
            "\n",
            "English: I like to sit in the front of the bus .\n",
            "French (input): <start> J aime m asseoir a l avant du bus .\n",
            "French (output): J aime m asseoir a l avant du bus . <end>\n",
            "\n",
            "\n",
            "English: It may rain in the evening .\n",
            "French (input): <start> Il se peut qu il pleuve dans la soiree .\n",
            "French (output): Il se peut qu il pleuve dans la soiree . <end>\n",
            "\n",
            "\n",
            "English: Is it still raining ?\n",
            "French (input): <start> Pleut il encore ?\n",
            "French (output): Pleut il encore ? <end>\n",
            "\n",
            "\n",
            "English: I wish you luck .\n",
            "French (input): <start> Je te souhaite bonne chance .\n",
            "French (output): Je te souhaite bonne chance . <end>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFg3XHJcN9cu"
      },
      "source": [
        "#### **Split Data** \n",
        "We split data into train and test by randomly selecting 20% as the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbm3IkEVN-OM",
        "outputId": "0db0cda0-c53c-4f6f-ac9f-2ce7d5b77284"
      },
      "source": [
        "test_percent = 0.10\n",
        "\n",
        "# Split data into train / validate\n",
        "split_outputs = train_test_split(data_en, data_fr_in, data_fr_out, test_size=test_percent)\n",
        "train_en, test_en, train_fr_in, test_fr_in, train_fr_out, test_fr_out = split_outputs\n",
        "\n",
        "print(\"train_en count:\",len(train_en))\n",
        "print(\"test_en count:\",len(test_en))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_en count: 167024\n",
            "test_en count: 18559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lokbRBpWO_j"
      },
      "source": [
        "### **Build Data Pipelines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok0lEvJ7WUS1"
      },
      "source": [
        "#### **Text Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCpuhbhUWXWF",
        "outputId": "95b8c01d-fb2a-41a9-8b14-0fccb3b7715d"
      },
      "source": [
        "# Tokenize english\n",
        "en_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(data_en)\n",
        "# Train\n",
        "train_tokens_en = en_tokenizer.texts_to_sequences(train_en)\n",
        "train_tokens_en = keras.preprocessing.sequence.pad_sequences(train_tokens_en,padding='post')\n",
        "# Test\n",
        "test_tokens_en = en_tokenizer.texts_to_sequences(test_en)\n",
        "test_tokens_en = keras.preprocessing.sequence.pad_sequences(test_tokens_en,padding='post')\n",
        "\n",
        "\n",
        "# Tokenize french input and output\n",
        "fr_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(data_fr_in)\n",
        "fr_tokenizer.fit_on_texts(data_fr_out)\n",
        "# Train\n",
        "train_tokens_fr_in = fr_tokenizer.texts_to_sequences(train_fr_in)\n",
        "train_tokens_fr_in = keras.preprocessing.sequence.pad_sequences(train_tokens_fr_in,padding='post')\n",
        "train_tokens_fr_out = fr_tokenizer.texts_to_sequences(train_fr_out)\n",
        "train_tokens_fr_out = keras.preprocessing.sequence.pad_sequences(train_tokens_fr_out,padding='post')\n",
        "# Test\n",
        "test_tokens_fr_in = fr_tokenizer.texts_to_sequences(test_fr_in)\n",
        "test_tokens_fr_in = keras.preprocessing.sequence.pad_sequences(test_tokens_fr_in,padding='post')\n",
        "test_tokens_fr_out = fr_tokenizer.texts_to_sequences(test_fr_out)\n",
        "test_tokens_fr_out = keras.preprocessing.sequence.pad_sequences(test_tokens_fr_out,padding='post')\n",
        "\n",
        "vocabulary_size_en = len(en_tokenizer.word_index)+1\n",
        "print(\"Vocabulary Size english:\",vocabulary_size_en)\n",
        "vocabulary_size_fr = len(fr_tokenizer.word_index)+1\n",
        "print(\"Vocabulary Size french:\",vocabulary_size_fr)\n",
        "\n",
        "print(\"train_tokens_en\",len(train_tokens_en))\n",
        "print(train_tokens_en[:5,:5])\n",
        "print(\"train_tokens_fr_in\",len(train_tokens_fr_in))\n",
        "print(train_tokens_fr_in[:5,:5])\n",
        "print(\"test_tokens_fr_out\",len(test_tokens_fr_out))\n",
        "print(test_tokens_fr_out[:5,:5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size english: 14412\n",
            "Vocabulary Size french: 23568\n",
            "train_tokens_en 167024\n",
            "[[   2   36    3   33    4]\n",
            " [   2   65   25 1240 1039]\n",
            " [   3  555   25 1786    1]\n",
            " [   2   27    8   96   12]\n",
            " [  60   63   13 2440    1]]\n",
            "train_tokens_fr_in 167024\n",
            "[[   2    4   82   11   10]\n",
            " [   2   84   43    5  758]\n",
            " [   2   16   59  263   57]\n",
            " [   2    4   20   25   72]\n",
            " [   2  105   14 1491    1]]\n",
            "test_tokens_fr_out 18559\n",
            "[[  162     8    93  6661  2993]\n",
            " [    4    14    30     9   183]\n",
            " [ 1021    26    79    47     6]\n",
            " [   81   266     5    76 17136]\n",
            " [   31     8    21  2274   276]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdWF2UvvhvR1"
      },
      "source": [
        "#### **Create TF Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnjmGEYIhvwu",
        "outputId": "98402b9d-cf05-4a48-8da1-db3ed7a0ae27"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "TRAIN_SHUFFLE_BUFFER_SIZE = len(train_tokens_en)\n",
        "\n",
        "# Create TF Dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_tokens_en, train_tokens_fr_in, train_tokens_fr_out))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_en, test_fr_in, test_fr_out))\n",
        "\n",
        "#############\n",
        "# Train data\n",
        "#############\n",
        "train_data = train_data.shuffle(buffer_size=TRAIN_SHUFFLE_BUFFER_SIZE)\n",
        "train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(\"train_data\",train_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data <BatchDataset shapes: ((64, 49), (64, 62), (64, 62)), types: (tf.int32, tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEIgx-iZzAnd"
      },
      "source": [
        "Take a batch of data and view some data to verify our data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5alZINOmjUwf",
        "outputId": "de197112-6c30-4da8-bb4f-af2eada5a798"
      },
      "source": [
        "for batch in train_data.take(1):\n",
        "  print(\"train_en\",batch[0][0,:10])\n",
        "  print(\"train_fr_in\",batch[1][0,:10])\n",
        "  print(\"train_fr_out\",batch[2][0,:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_en tf.Tensor([   2   37 1756  257    1    0    0    0    0    0], shape=(10,), dtype=int32)\n",
            "train_fr_in tf.Tensor([   2   19   62   28 3083   11    4  476    1    0], shape=(10,), dtype=int32)\n",
            "train_fr_out tf.Tensor([  19   62   28 3083   11    4  476    1    3    0], shape=(10,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12Ig-rr6uI-"
      },
      "source": [
        "### **Neural Machine Translation Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI8U_iqL6wXj"
      },
      "source": [
        "#### **Sequence 2 Sequence**\n",
        "\n",
        "* We will start with a simple sequence to sequence model\n",
        "* The Encoder will consist of an Embedding and LSTM layer\n",
        "* The Decoder will also consist of an Embedding and LSTM layer\n",
        "* In addtion the Decoder will have a final logits layer with the size of the french vocabulary\n",
        "* Like we saw in lecture the final hidden state of the encoder\n",
        "is the initial state of the decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKfG4hfSevyW"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/lecture_seq2seq.png\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oPtMvlxiNkE"
      },
      "source": [
        "Lets look at the various parts to the Encoder Decoder architecture\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/seq2seq/seq2seq_008.svg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq02idME684Y"
      },
      "source": [
        "##### Build Model\n",
        "\n",
        "We will create build custom `keras.layers` for the Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbH7EiMwkkwR"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"encoder_embedding\")\n",
        "    self.lstm = keras.layers.LSTM(units=lstm_size, return_sequences=True, return_state=True, name=\"encoder_lstm\")\n",
        "  \n",
        "  def call(self, data):\n",
        "    encoder_outputs = self.embedding(data)\n",
        "    encoder_outputs = self.lstm(encoder_outputs)\n",
        "    return encoder_outputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7bbJ1DNklsE"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"decoder_embedding\")\n",
        "    self.lstm = keras.layers.LSTM(units=lstm_size, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "    self.logits = keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "  def call(self, data, initial_state):\n",
        "    decoder_outputs = self.embedding(data)\n",
        "    decoder_outputs = self.lstm(decoder_outputs, initial_state=initial_state)\n",
        "    logits = self.logits(decoder_outputs[0])\n",
        "    return logits, decoder_outputs[1], decoder_outputs[2]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pkdq8kekl17"
      },
      "source": [
        "class Sequence2Sequence(keras.Model):\n",
        "  def __init__(self,source_vocab_size, target_vocab_size, embedding_size, lstm_size):\n",
        "    super(Sequence2Sequence, self).__init__()\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = Encoder(source_vocab_size, embedding_size, lstm_size)\n",
        "    # Decoder\n",
        "    self.decoder = Decoder(target_vocab_size, embedding_size, lstm_size)\n",
        "  \n",
        "  def train_step(self, data):\n",
        "    # Unpack data\n",
        "    x1, x2, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encoder\n",
        "      encoder_outputs = self.encoder(x1)\n",
        "      encoder_states = encoder_outputs[1:]\n",
        "      # Decoder\n",
        "      decoder_outputs = self.decoder(x2, encoder_states)\n",
        "      logits = decoder_outputs[0]\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = self.loss(y, logits)\n",
        "\n",
        "    # Compute gradients\n",
        "    trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    # Return a dict of performance\n",
        "    results = {m.name: m.result() for m in self.metrics}\n",
        "    results.update({\"loss\": loss})\n",
        "    return results"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aj6TnBsTmBJ"
      },
      "source": [
        "##### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4UN9ZNUyWrO"
      },
      "source": [
        "# Function to perform translation using the model\n",
        "def translate_seq2seq(model,test_source_text=None, max_ouput_size=25):\n",
        "\n",
        "  if test_source_text is None:\n",
        "    test_source_text = data_en[np.random.choice(len(data_en))]\n",
        "  test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "\n",
        "  # Encoder\n",
        "  encoder_outputs = model.encoder(tf.constant(test_source_seq))\n",
        "  encoder_states = encoder_outputs[1:]\n",
        "\n",
        "  # Decoder\n",
        "  decoder_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
        "  decoder_state_h, decoder_state_c = encoder_outputs[1:]\n",
        "  output_words = []\n",
        "  output_seq = []\n",
        "  for i in range(max_ouput_size):\n",
        "    decoder_output, decoder_state_h, decoder_state_c = model.decoder(decoder_input, (decoder_state_h, decoder_state_c))\n",
        "    decoder_input = tf.argmax(decoder_output, -1)\n",
        "    output_seq.append(decoder_input.numpy()[0][0])\n",
        "    output_words.append(fr_tokenizer.index_word[output_seq[-1]])\n",
        "    if output_words[-1] == '<end>':\n",
        "        break\n",
        "\n",
        "  output_text = ' '.join(output_words)\n",
        "\n",
        "  return test_source_text, output_text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX5MCua47A9m"
      },
      "source": [
        "##### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyNAXE9bkvHv",
        "outputId": "31526526-a644-4bac-9312-e745470c5f3f"
      },
      "source": [
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "epochs = 20\n",
        "embedding_size = 256\n",
        "lstm_size = 512\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model = Sequence2Sequence(vocabulary_size_en, vocabulary_size_fr, embedding_size, lstm_size)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate, clipnorm=5.0)\n",
        "\n",
        "# Loss\n",
        "crossentropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "def custom_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "  return loss\n",
        "\n",
        "# Callback\n",
        "class DisplayTranslation(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print(\"\\nTranslation:\")\n",
        "    source_text, output_text = translate_seq2seq(self.model)\n",
        "    print(source_text,\" => \", output_text)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=optimizer, loss=custom_loss)\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "training_results = model.fit(\n",
        "        train_data, # train_data.take(20) while testing\n",
        "        epochs=epochs,\n",
        "        callbacks=[DisplayTranslation()],\n",
        "        verbose=1)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2609/2609 [==============================] - 291s 105ms/step - loss: 0.5040\n",
            "\n",
            "Translation:\n",
            "You re safe .  =>  un coup de feu . <end>\n",
            "Epoch 2/20\n",
            "2609/2609 [==============================] - 273s 104ms/step - loss: 0.3864\n",
            "\n",
            "Translation:\n",
            "How about a cup of hot coffee ?  =>  comme je suis en train de faire ca ? <end>\n",
            "Epoch 3/20\n",
            "2609/2609 [==============================] - 275s 105ms/step - loss: 0.3500\n",
            "\n",
            "Translation:\n",
            "I was able to catch the last train because I walked very quickly .  =>  je suis en train de me dire que tu ne veux pas que tu sois la . <end>\n",
            "Epoch 4/20\n",
            "2609/2609 [==============================] - 273s 105ms/step - loss: 0.3290\n",
            "\n",
            "Translation:\n",
            "The road was obstructed by fallen trees .  =>  la maison est un crime . <end>\n",
            "Epoch 5/20\n",
            "2609/2609 [==============================] - 272s 104ms/step - loss: 0.3158\n",
            "\n",
            "Translation:\n",
            "I didn t want any of that .  =>  je ne suis pas sur que ca arrive . <end>\n",
            "Epoch 6/20\n",
            "2609/2609 [==============================] - 273s 105ms/step - loss: 0.3063\n",
            "\n",
            "Translation:\n",
            "You ve changed since I saw you last year .  =>  son visage n a pas pondu de manger . <end>\n",
            "Epoch 7/20\n",
            "2609/2609 [==============================] - 275s 105ms/step - loss: 0.3007\n",
            "\n",
            "Translation:\n",
            "We ve been warned .  =>  le comportement insultant . <end>\n",
            "Epoch 8/20\n",
            "2609/2609 [==============================] - 273s 105ms/step - loss: 0.3022\n",
            "\n",
            "Translation:\n",
            "Does anybody recognize him ?  =>  a peine a l evidence . <end>\n",
            "Epoch 9/20\n",
            "2609/2609 [==============================] - 275s 105ms/step - loss: 0.3059\n",
            "\n",
            "Translation:\n",
            "I m ready for tomorrow .  =>  n importe quoi . <end>\n",
            "Epoch 10/20\n",
            "2609/2609 [==============================] - 274s 105ms/step - loss: 0.2989\n",
            "\n",
            "Translation:\n",
            "What would you do in my place ?  =>  que je puisse faire ca . <end>\n",
            "Epoch 11/20\n",
            "2609/2609 [==============================] - 275s 105ms/step - loss: 0.2992\n",
            "\n",
            "Translation:\n",
            "I wonder what happens if I press this button .  =>  suppose ses dettes desormais dans la voiture . <end>\n",
            "Epoch 12/20\n",
            "2609/2609 [==============================] - 274s 105ms/step - loss: 0.2951\n",
            "\n",
            "Translation:\n",
            "This yearbook is illustrated with a lot of beautiful photographs .  =>  avance quant lorsque je suis censee aimer un medecin . <end>\n",
            "Epoch 13/20\n",
            "2609/2609 [==============================] - 275s 106ms/step - loss: 0.2919\n",
            "\n",
            "Translation:\n",
            "Get real !  =>  verifie le bouc emissaire . <end>\n",
            "Epoch 14/20\n",
            "2609/2609 [==============================] - 275s 105ms/step - loss: 0.2904\n",
            "\n",
            "Translation:\n",
            "Love makes the world go round .  =>  parfois agissez seule . <end>\n",
            "Epoch 15/20\n",
            "2609/2609 [==============================] - 275s 105ms/step - loss: 0.2883\n",
            "\n",
            "Translation:\n",
            "I didn t say it like that .  =>  ferme le gaz pouvait etre arretes ! <end>\n",
            "Epoch 16/20\n",
            "2609/2609 [==============================] - 273s 105ms/step - loss: 0.2875\n",
            "\n",
            "Translation:\n",
            "We waited .  =>  roi . <end>\n",
            "Epoch 17/20\n",
            "2609/2609 [==============================] - 274s 105ms/step - loss: 0.2862\n",
            "\n",
            "Translation:\n",
            "Don t try to trick us .  =>  casse quelque chose a manger . <end>\n",
            "Epoch 18/20\n",
            "2609/2609 [==============================] - 272s 104ms/step - loss: 0.2857\n",
            "\n",
            "Translation:\n",
            "You must think I m really strange .  =>  vous n avez pas besoin de vous parler . <end>\n",
            "Epoch 19/20\n",
            "2609/2609 [==============================] - 273s 105ms/step - loss: 0.2851\n",
            "\n",
            "Translation:\n",
            "My friend says that she s suicidal .  =>  chacun egale a inviter faire un enorme erreur . <end>\n",
            "Epoch 20/20\n",
            "2609/2609 [==============================] - 272s 104ms/step - loss: 0.2850\n",
            "\n",
            "Translation:\n",
            "What s your favorite way to travel ?  =>  de voudras vraiment sages . <end>\n",
            "Training execution time (mins) 91.57990283568701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT7cEislpXqx"
      },
      "source": [
        "##### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "7cseE5p-fwSD",
        "outputId": "e840d0fb-f083-48b2-e2ea-9eaade6fccb9"
      },
      "source": [
        "evaluate_save_model(model, test_en[:1000], test_fr_out[:1000],translate_seq2seq,\n",
        "                    training_results, execution_time, learning_rate, epochs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'BLEU-1': 0.2214277642350044, 'BLEU-2': 0.14720688702093024, 'BLEU-3': 0.030380353391263996, 'BLEU-4': 0.03451211654470146}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIIe1l5epdVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f395cbb-d720-4a74-e4a3-757bf6769d05"
      },
      "source": [
        "# Make some translations using the model\n",
        "for i,data_idx in enumerate(data_samples):\n",
        "  print(\"English:\",data_en[data_idx])\n",
        "  print(\"French (actual):\",data_fr_out[data_idx])\n",
        "  source_text, output_text = translate_seq2seq(model, test_source_text=data_en[data_idx])\n",
        "  print(\"French (prediction):\",output_text)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: They found us .\n",
            "French (actual): Elles nous ont trouves . <end>\n",
            "French (prediction): portent croire de maniere . <end>\n",
            "\n",
            "\n",
            "English: I don t compromise .\n",
            "French (actual): Je ne transige pas . <end>\n",
            "French (prediction): continues de faire face a autre chose . <end>\n",
            "\n",
            "\n",
            "English: I like to sit in the front of the bus .\n",
            "French (actual): J aime m asseoir a l avant du bus . <end>\n",
            "French (prediction): continues de grignoter comme un adulte acharne sur la table . <end>\n",
            "\n",
            "\n",
            "English: It may rain in the evening .\n",
            "French (actual): Il se peut qu il pleuve dans la soiree . <end>\n",
            "French (prediction): reflechir ce matin . <end>\n",
            "\n",
            "\n",
            "English: Is it still raining ?\n",
            "French (actual): Pleut il encore ? <end>\n",
            "French (prediction): allonger mon dejeuner . <end>\n",
            "\n",
            "\n",
            "English: I wish you luck .\n",
            "French (actual): Je te souhaite bonne chance . <end>\n",
            "French (prediction): promis pas que nous gardions nos distances . <end>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xTDIDd_7SFa"
      },
      "source": [
        "#### **Sequence 2 Sequence with Attention**\n",
        "\n",
        "* Next we will add Attention to the decoder\n",
        "* The Encoder will consist of an Embedding and LSTM layer\n",
        "* The Decoder will also consist of an Embedding and LSTM layer\n",
        "* In addition the Decoder will have an Attention layer\n",
        "* In addtion the Decoder will have a final logits layer with the size of the french vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7PVRyfUkwZj"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/lecture_seq2seq_attention.png\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2NxpYz2kw_Q"
      },
      "source": [
        "Lets look at the various parts to the Encoder Decoder architecture\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/seq2seq/seq2seqattention_013.svg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_7HrXDS7Ygc"
      },
      "source": [
        "##### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE2bxJtvUG29"
      },
      "source": [
        "class Attention(keras.layers.Layer):\n",
        "  def __init__(self, lstm_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.wa = keras.layers.Dense(lstm_size)\n",
        "\n",
        "  def call(self, decoder_output, encoder_output):\n",
        "\n",
        "    # Score\n",
        "    score = tf.matmul(decoder_output, self.wa(encoder_output), transpose_b=True)\n",
        "\n",
        "    # alignment = softmax(score)\n",
        "    alignment = tf.nn.softmax(score, axis=2)\n",
        "\n",
        "    # context vector is the weighted average sum of encoder output\n",
        "    context = tf.matmul(alignment, encoder_output)\n",
        "\n",
        "    return context, alignment"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGwJqlUiUKB_"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"encoder_embedding\")\n",
        "    self.lstm = keras.layers.LSTM(units=lstm_size, return_sequences=True, return_state=True, name=\"encoder_lstm\")\n",
        "  \n",
        "  def call(self, data):\n",
        "    encoder_outputs = self.embedding(data)\n",
        "    lstm_out, state_h, state_c = self.lstm(encoder_outputs)\n",
        "    return lstm_out, state_h, state_c"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "599il_L87Y4m"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    self.embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"decoder_embedding\")\n",
        "    self.lstm = keras.layers.LSTM(units=lstm_size, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "    # Add Attention\n",
        "    self.attention = Attention(lstm_size)\n",
        "\n",
        "    self.wc = keras.layers.Dense(units=lstm_size, activation='tanh')\n",
        "    self.ws = keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "  def call(self, data, initial_state, encoder_output):\n",
        "    # Get LSTM outputs\n",
        "    embed = self.embedding(data)\n",
        "    lstm_out, state_h, state_c = self.lstm(embed, initial_state=initial_state)\n",
        "\n",
        "    # Get context and alignment vectors using the Attention layer\n",
        "    context, alignment = self.attention(lstm_out, encoder_output)\n",
        "\n",
        "    # Combine the context vector and the LSTM output\n",
        "    lstm_out = tf.concat([tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
        "\n",
        "    # Pass lstm_out through Dense layer\n",
        "    lstm_out = self.wc(lstm_out)\n",
        "\n",
        "    # Output logits of vocab_size\n",
        "    logits = self.ws(lstm_out)\n",
        "\n",
        "    return logits, state_h, state_c, alignment"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcl0AkU451I"
      },
      "source": [
        "class Sequence2SequenceAttention(keras.Model):\n",
        "  def __init__(self,source_vocab_size, target_vocab_size, embedding_size, lstm_size):\n",
        "    super(Sequence2SequenceAttention, self).__init__()\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = Encoder(source_vocab_size, embedding_size, lstm_size)\n",
        "    # Decoder\n",
        "    self.decoder = Decoder(target_vocab_size, embedding_size, lstm_size)\n",
        "\n",
        "  def train_step(self, data):\n",
        "    # Unpack data\n",
        "    x1, x2, y = data\n",
        "\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_outputs = self.encoder(x1)\n",
        "      encoder_states = encoder_outputs[1:]\n",
        "      decoder_state_h, decoder_state_c = encoder_states\n",
        "\n",
        "      # For every word in y\n",
        "      for i in range(y.shape[1]):\n",
        "          # Decoder input is one word at a time\n",
        "          decoder_input = tf.expand_dims(x2[:, i], 1)\n",
        "          logits, decoder_state_h, decoder_state_c, _ = self.decoder(decoder_input, (decoder_state_h, decoder_state_c), encoder_outputs[0])\n",
        "\n",
        "          # The loss is now accumulated through the whole batch\n",
        "          loss += self.loss(y[:, i], logits)\n",
        "\n",
        "    variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict of performance\n",
        "    results = {m.name: m.result() for m in self.metrics}\n",
        "    results.update({\"loss\": loss / y.shape[1]})\n",
        "    return results"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQMRNkLrT9WE"
      },
      "source": [
        "##### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj3Fc0O26T8s"
      },
      "source": [
        "# Function to perform translation using the model\n",
        "def translate_seq2seqattention(model,test_source_text=None, max_ouput_size=25):\n",
        "\n",
        "  if test_source_text is None:\n",
        "    test_source_text = data_en[np.random.choice(len(data_en))]\n",
        "  test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "\n",
        "  # Encoder\n",
        "  encoder_outputs = model.encoder(tf.constant(test_source_seq))\n",
        "\n",
        "  # Decoder\n",
        "  decoder_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
        "  decoder_state_h, decoder_state_c = encoder_outputs[1:]\n",
        "\n",
        "  output_words = []\n",
        "  output_seq = []\n",
        "  for i in range(max_ouput_size):\n",
        "    logits, decoder_state_h, decoder_state_c, _ = model.decoder(decoder_input, \n",
        "                                                                (decoder_state_h, decoder_state_c), \n",
        "                                                                encoder_outputs[0])\n",
        "    decoder_input = tf.expand_dims(tf.argmax(logits, -1), 0)\n",
        "    output_seq.append(decoder_input.numpy()[0][0])\n",
        "    output_words.append(fr_tokenizer.index_word[output_seq[-1]])\n",
        "    if output_words[-1] == '<end>':\n",
        "        break\n",
        "\n",
        "  output_text = ' '.join(output_words)\n",
        "\n",
        "  return test_source_text, output_text"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0QwfvxE7bKJ"
      },
      "source": [
        "##### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrOYgMVX7bfB",
        "outputId": "44eff185-0713-4c2d-fe5b-eab420145c27"
      },
      "source": [
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "epochs = 20\n",
        "embedding_size = 256\n",
        "lstm_size = 512\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model = Sequence2SequenceAttention(vocabulary_size_en, vocabulary_size_fr, embedding_size, lstm_size)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate, clipnorm=5.0)\n",
        "# Loss\n",
        "crossentropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "def custom_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "  return loss\n",
        "\n",
        "# Callback\n",
        "class DisplayTranslation(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print(\"\\nTranslation:\")\n",
        "    source_text, output_text = translate_seq2seqattention(self.model)\n",
        "    print(source_text,\" => \", output_text)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=optimizer, loss=custom_loss)\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "training_results = model.fit(\n",
        "        train_data, # train_data.take(20) while testing\n",
        "        epochs=epochs,\n",
        "        callbacks=[DisplayTranslation()],\n",
        "        verbose=1)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2609/2609 [==============================] - 842s 295ms/step - loss: 0.9762\n",
            "\n",
            "Translation:\n",
            "It was a bad movie .  =>  ca parviendrons . <end>\n",
            "Epoch 2/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.0220\n",
            "\n",
            "Translation:\n",
            "You can t blame him .  =>  tu n aimait pas son cousin . <end>\n",
            "Epoch 3/20\n",
            "2609/2609 [==============================] - 768s 294ms/step - loss: 1.0424\n",
            "\n",
            "Translation:\n",
            "The accident happened two years ago .  =>  reflechissez tu pas de nous aider mes excuses aider . <end>\n",
            "Epoch 4/20\n",
            "2609/2609 [==============================] - 767s 294ms/step - loss: 1.0578\n",
            "\n",
            "Translation:\n",
            "I don t want you to be hurt .  =>  je ne pense pas faire ca . <end>\n",
            "Epoch 5/20\n",
            "2609/2609 [==============================] - 768s 294ms/step - loss: 1.0742\n",
            "\n",
            "Translation:\n",
            "This may just come in handy someday .  =>  est en danger une encyclopedie . <end>\n",
            "Epoch 6/20\n",
            "2609/2609 [==============================] - 768s 294ms/step - loss: 1.0799\n",
            "\n",
            "Translation:\n",
            "The woman gave birth to a baby girl .  =>  l australie . <end>\n",
            "Epoch 7/20\n",
            "2609/2609 [==============================] - 768s 294ms/step - loss: 1.0995\n",
            "\n",
            "Translation:\n",
            "Take these .  =>  ces derniers anglais sont ces arretees . <end>\n",
            "Epoch 8/20\n",
            "2609/2609 [==============================] - 768s 294ms/step - loss: 1.1114\n",
            "\n",
            "Translation:\n",
            "I gave up eating dessert .  =>  j ai pas prevu de temps . <end>\n",
            "Epoch 9/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1217\n",
            "\n",
            "Translation:\n",
            "It s now .  =>  mon desastre au bal petit nombre . <end>\n",
            "Epoch 10/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1257\n",
            "\n",
            "Translation:\n",
            "She is just going shopping .  =>  elle decrit . <end>\n",
            "Epoch 11/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1188\n",
            "\n",
            "Translation:\n",
            "Come home early .  =>  il est meilleure . <end>\n",
            "Epoch 12/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1235\n",
            "\n",
            "Translation:\n",
            "What he needs most is a good job .  =>  veuillez mal a la discrimination . <end>\n",
            "Epoch 13/20\n",
            "2609/2609 [==============================] - 770s 295ms/step - loss: 1.1248\n",
            "\n",
            "Translation:\n",
            "I washed my feet .  =>  elle a abandonne etudie . <end>\n",
            "Epoch 14/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1264\n",
            "\n",
            "Translation:\n",
            "She thanked him for his help .  =>  elle lui a continue de apprends longuement . <end>\n",
            "Epoch 15/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1337\n",
            "\n",
            "Translation:\n",
            "You have been warned .  =>  ses des fermes de femme . <end>\n",
            "Epoch 16/20\n",
            "2609/2609 [==============================] - 768s 294ms/step - loss: 1.1376\n",
            "\n",
            "Translation:\n",
            "I promise I ll protect you .  =>  je promets qu il tiendrait moi . <end>\n",
            "Epoch 17/20\n",
            "2609/2609 [==============================] - 771s 295ms/step - loss: 1.1417\n",
            "\n",
            "Translation:\n",
            "He sat there with his legs crossed .  =>  etaient souvent . <end>\n",
            "Epoch 18/20\n",
            "2609/2609 [==============================] - 772s 296ms/step - loss: 1.1377\n",
            "\n",
            "Translation:\n",
            "You re charming .  =>  vous a intervenir . <end>\n",
            "Epoch 19/20\n",
            "2609/2609 [==============================] - 771s 295ms/step - loss: 1.1357\n",
            "\n",
            "Translation:\n",
            "I have nothing to say in this regard .  =>  que ca ira ordonnee . <end>\n",
            "Epoch 20/20\n",
            "2609/2609 [==============================] - 769s 295ms/step - loss: 1.1345\n",
            "\n",
            "Translation:\n",
            "He s three years older than her .  =>  dites ce que c est qu il serait ce qu il est mort ce contrat d une recession . <end>\n",
            "Training execution time (mins) 257.5513118386269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBTNu5JPpfFO"
      },
      "source": [
        "##### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "tH1XXhElhxfa",
        "outputId": "d13b7bce-eebc-41ef-a3cc-e6f7def22f76"
      },
      "source": [
        "evaluate_save_model(model, test_en[:1000], test_fr_out[:1000],translate_seq2seqattention,\n",
        "                    training_results, execution_time, learning_rate, epochs)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'BLEU-1': 0.219911621635852, 'BLEU-2': 0.15107701121156186, 'BLEU-3': 0.05173881761102471, 'BLEU-4': 0.012296466757608206}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye0ZvBIIpfkQ",
        "outputId": "f36f6ddf-880d-46c2-fca4-ca25a59b2753"
      },
      "source": [
        "# Make some translations using the model\n",
        "for i,data_idx in enumerate(data_samples):\n",
        "  print(\"English:\",data_en[data_idx])\n",
        "  print(\"French (actual):\",data_fr_out[data_idx])\n",
        "  source_text, output_text = translate_seq2seqattention(model, test_source_text=data_en[data_idx])\n",
        "  print(\"French (prediction):\",output_text)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: They found us .\n",
            "French (actual): Elles nous ont trouves . <end>\n",
            "French (prediction): la raison . <end>\n",
            "\n",
            "\n",
            "English: I don t compromise .\n",
            "French (actual): Je ne transige pas . <end>\n",
            "French (prediction): je ne fais pas . <end>\n",
            "\n",
            "\n",
            "English: I like to sit in the front of the bus .\n",
            "French (actual): J aime m asseoir a l avant du bus . <end>\n",
            "French (prediction): je comprends comme destruction dit . <end>\n",
            "\n",
            "\n",
            "English: It may rain in the evening .\n",
            "French (actual): Il se peut qu il pleuve dans la soiree . <end>\n",
            "French (prediction): d abord important . <end>\n",
            "\n",
            "\n",
            "English: Is it still raining ?\n",
            "French (actual): Pleut il encore ? <end>\n",
            "French (prediction): qui prend chose qui est assis bonne idee de nouveau . <end>\n",
            "\n",
            "\n",
            "English: I wish you luck .\n",
            "French (actual): Je te souhaite bonne chance . <end>\n",
            "French (prediction): je vous souhaite . <end>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPlXzAC-7eU6"
      },
      "source": [
        "#### **Transfomer**\n",
        "\n",
        "* The transformer will consist of an Encoder and Decoder\n",
        "* The Encoder consists of embedding, positional encoding, self attention, normalization, and feed forward layers\n",
        "* The Decoder consists of embedding, positional encoding, decoder self attention, encoder-decoder attention, normalization, and feed forward layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW_20LrSm744"
      },
      "source": [
        "Lets look at the various parts to the Transfomer Encoder Decoder architecture\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/transformer/transformer_020.svg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Dj4FSx7kRU"
      },
      "source": [
        "##### Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4W5BEoG7EJb"
      },
      "source": [
        "**Model Param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po0Ukby97GsW"
      },
      "source": [
        "############################\n",
        "# Model Params\n",
        "############################\n",
        "MODEL_SIZE = 128 # depth size of the model\n",
        "NUM_LAYERS = 4 # number of layers (Multi-Head Attention + FFN)\n",
        "NUM_HEADS = 8 # number of attention heads"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_UnJlTeOLIK"
      },
      "source": [
        "**Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r-UJW5uOMv4"
      },
      "source": [
        "def generate_positional_encoding(max_length, model_size):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / model_size) for j in range(model_size)]\n",
        "            if pos != 0\n",
        "            else np.zeros(model_size)\n",
        "            for pos in range(max_length)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kUh76D6nZ6i"
      },
      "source": [
        "Here we have an implementation of `MultiHeadSelfAttention` layer. But you can use the `keras.layers.MultiHeadSelfAttention` directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSFCOWR_3_r3"
      },
      "source": [
        "class MultiHeadSelfAttention(keras.layers.Layer):\n",
        "  def __init__(self, model_size, num_heads):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "    self.key_size = model_size // num_heads # Key size\n",
        "    self.num_heads = num_heads # number of attention heads\n",
        "\n",
        "    # Query weights\n",
        "    self.wq = keras.layers.Dense(units=model_size)\n",
        "    # Key weights\n",
        "    self.wk = keras.layers.Dense(units=model_size)\n",
        "    # Value weights\n",
        "    self.wv = keras.layers.Dense(units=model_size)\n",
        "    # Output weights\n",
        "    self.wo = keras.layers.Dense(units=model_size)\n",
        "\n",
        "  def call(self, query, value, mask=None):\n",
        "    \n",
        "    ############################################\n",
        "    # Step 1: Create query, key, value vectors\n",
        "    ############################################\n",
        "    query = self.wq(query) # query has shape (batch, query_len, model_size)\n",
        "    key = self.wk(value) # key has shape (batch, value_len, model_size)\n",
        "    value = self.wv(value) # value has shape (batch, value_len, model_size)\n",
        "    batch_size = query.shape[0]\n",
        "    # Each heads query, key, value needs to be size = key_size\n",
        "    # Reshape to (batch, query_len, num_heads, key_size)\n",
        "    query = tf.reshape(query, [batch_size, -1, self.num_heads, self.key_size])\n",
        "    key = tf.reshape(key, [batch_size, -1, self.num_heads, self.key_size])\n",
        "    value = tf.reshape(value, [batch_size, -1, self.num_heads, self.key_size])\n",
        "    # Tranpose to (batch, num_heads, query_len, key_size) for matmul\n",
        "    query = tf.transpose(query, [0, 2, 1, 3])\n",
        "    key = tf.transpose(key, [0, 2, 1, 3])\n",
        "    value = tf.transpose(value, [0, 2, 1, 3])\n",
        "    \n",
        "    ########################################\n",
        "    # Step 2+3: Compute Self-Attention Score\n",
        "    ########################################\n",
        "    # Score is dot product of query vector and key vector\n",
        "    # Score is then divided by the square root of key_size : This leads to having more stable gradients\n",
        "    score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
        "    \n",
        "    # Mask out scores if a mask is provided\n",
        "    if mask is not None:\n",
        "        score *= mask\n",
        "        # Apply a very large negative value so the softmax in the next step will zero it out\n",
        "        score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
        "    \n",
        "    ##################################################\n",
        "    # Step 4: Normalize Score to get Alignment vector\n",
        "    ##################################################\n",
        "    # Pass the score through a softmax operation\n",
        "    # Softmax normalizes the scores so they’re all positive and add up to 1\n",
        "    # Alignment vector shape = (batch, h, query_len, value_len)\n",
        "    alignment = tf.nn.softmax(score, axis=-1)\n",
        "    \n",
        "    ################################\n",
        "    # Step 5: Compute Context vector\n",
        "    ################################\n",
        "    # Computed by multiplying each value vector with alignment vector\n",
        "    # Context vector shape = (batch, h, query_len, key_size)\n",
        "    context = tf.matmul(alignment, value)\n",
        "    \n",
        "    ################################\n",
        "    # Step 6: Compute Final Output\n",
        "    ################################\n",
        "    # Align all the context vectors (from all heads)\n",
        "    context = tf.transpose(context, [0, 2, 1, 3])\n",
        "    context = tf.reshape(context, [batch_size, -1, self.key_size * self.num_heads])\n",
        "    # Multiply context vectors with weight matric wo\n",
        "    z = self.wo(context)\n",
        "    \n",
        "    # Final Z matrix, alignment of all heads\n",
        "    return z, alignment"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBmJLTR1mkw_"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjN2t11i7k5a"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, model_size, num_layers, num_heads):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.model_size = model_size # depth size of the model\n",
        "    self.num_layers = num_layers # number of layers (Multi-Head Self Attention + FFN)\n",
        "    self.num_heads = num_heads # number of attention heads\n",
        "\n",
        "    # Embedding Layer\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, self.model_size)\n",
        "    self.embedding_dropout = keras.layers.Dropout(0.1)\n",
        "\n",
        "    ###################\n",
        "    # Attention Blocks (repeated num_layer times)\n",
        "    ###################\n",
        "\n",
        "    # Multi-Head Self Attention + Normalization\n",
        "    self.attention = [MultiHeadSelfAttention(self.model_size, self.num_heads) for _ in range(self.num_layers)]\n",
        "    self.attention_dropout = [keras.layers.Dropout(0.1) for _ in range(self.num_layers)]\n",
        "    self.attention_norm = [keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(self.num_layers)]\n",
        "\n",
        "    # Feedforward + Normalization\n",
        "    self.dense_1 = [keras.layers.Dense(self.model_size * 4, activation='relu') for _ in range(self.num_layers)]\n",
        "    self.dense_2 = [keras.layers.Dense(self.model_size) for _ in range(self.num_layers)]\n",
        "    self.ffn_dropout = [keras.layers.Dropout(0.1) for _ in range(self.num_layers)]\n",
        "    self.ffn_norm = [keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(self.num_layers)]\n",
        "\n",
        "  def call(self, data, training=True, encoder_mask=None):\n",
        "    # Embedding output\n",
        "    embedding_output = self.embedding(data)\n",
        "    embedding_output *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "    # Add Positionsl Encoding to embedding output\n",
        "    embedding_output = embedding_output + generate_positional_encoding(data.shape[1],self.model_size)\n",
        "    embedding_output = self.embedding_dropout(embedding_output, training=training)\n",
        "\n",
        "    # Sub layer input\n",
        "    sub_layer_input = embedding_output\n",
        "    alignments = [] # alignment (attention) vectors for all layers\n",
        "\n",
        "    # Repeat for each layer\n",
        "    for i in range(self.num_layers):\n",
        "      # Pass in query, value, mask to Attention layer\n",
        "      sub_layer_output, alignment = self.attention[i](sub_layer_input, sub_layer_input, encoder_mask)\n",
        "      sub_layer_output = self.attention_dropout[i](sub_layer_output, training=training)\n",
        "      # Add residual connection \n",
        "      sub_layer_output = sub_layer_input + sub_layer_output\n",
        "      # Normalization\n",
        "      sub_layer_output = self.attention_norm[i](sub_layer_output)\n",
        "      alignments.append(alignment)\n",
        "\n",
        "      # Feedforward Network\n",
        "      ffn_input = sub_layer_output\n",
        "      ffn_output = self.dense_2[i](self.dense_1[i](ffn_input))\n",
        "      ffn_output = self.ffn_dropout[i](ffn_output, training=training)\n",
        "      # Add residual connection \n",
        "      ffn_output = ffn_input + ffn_output\n",
        "      # Normalization\n",
        "      ffn_output = self.ffn_norm[i](ffn_output)\n",
        "\n",
        "      # Pass FFN output to the next layer\n",
        "      sub_layer_input = ffn_output\n",
        "\n",
        "    # Final encoder output and all alignment vectors\n",
        "    return ffn_output, alignments"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrnsl8vGmoMb"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zQXzE-v6sMe"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, model_size, num_layers, num_heads):\n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    self.model_size = model_size # depth size of the model\n",
        "    self.num_layers = num_layers # number of layers (Multi-Head Self Attention + FFN)\n",
        "    self.num_heads = num_heads # number of attention heads\n",
        "\n",
        "    # Embedding Layer\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, model_size)\n",
        "    self.embedding_dropout = keras.layers.Dropout(0.1)\n",
        "\n",
        "    ###################\n",
        "    # Attention Blocks (repeated num_layer times)\n",
        "    ###################\n",
        "\n",
        "    # Bottom Multi-Head Self Attention + Normalization\n",
        "    self.attention_bottom = [MultiHeadSelfAttention(self.model_size, self.num_heads) for _ in range(self.num_layers)]\n",
        "    self.attention_bottom_dropout = [keras.layers.Dropout(0.1) for _ in range(self.num_layers)]\n",
        "    self.attention_bottom_norm = [keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(self.num_layers)]\n",
        "\n",
        "    # Middle Multi-Head Self Attention + Normalization\n",
        "    self.attention_middle = [MultiHeadSelfAttention(self.model_size, self.num_heads) for _ in range(self.num_layers)]\n",
        "    self.attention_middle_dropout = [keras.layers.Dropout(0.1) for _ in range(self.num_layers)]\n",
        "    self.attention_middle_norm = [keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(self.num_layers)]\n",
        "\n",
        "    # Feedforward + Normalization\n",
        "    self.dense_1 = [keras.layers.Dense(units=self.model_size * 4, activation='relu') for _ in range(self.num_layers)]\n",
        "    self.dense_2 = [keras.layers.Dense(units=self.model_size) for _ in range(self.num_layers)]\n",
        "    self.ffn_dropout = [keras.layers.Dropout(0.1) for _ in range(self.num_layers)]\n",
        "    self.ffn_norm = [keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(self.num_layers)]\n",
        "\n",
        "    # Final output/logits layer\n",
        "    self.dense = keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "  def call(self, data, encoder_output, training=True, encoder_mask=None):\n",
        "    # Embedding output\n",
        "    embedding_output = self.embedding(data)\n",
        "    embedding_output *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "    # Add Positionsl Encoding to embedding output\n",
        "    embedding_output = embedding_output + generate_positional_encoding(data.shape[1],self.model_size)\n",
        "    embedding_output = self.embedding_dropout(embedding_output)\n",
        "\n",
        "    bottom_sub_layer_input = embedding_output\n",
        "    bottom_alignments = [] # bottom alignment (attention) vectors for all layers\n",
        "    middle_alignments = [] # middle alignment (attention) vectors for all layers\n",
        "\n",
        "    # Repeat for each layer\n",
        "    for i in range(self.num_layers):\n",
        "      # Bottom Multi-Head Sub Layer\n",
        "      sequence_length = bottom_sub_layer_input.shape[1]\n",
        "\n",
        "      if training:\n",
        "        mask = tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)\n",
        "      else:\n",
        "        mask = None\n",
        "\n",
        "      # Pass in query, value, mask to Attention layer\n",
        "      bottom_sub_layer_output, bottom_alignment = self.attention_bottom[i](bottom_sub_layer_input, bottom_sub_layer_input, mask)\n",
        "      bottom_sub_layer_output = self.attention_bottom_dropout[i](bottom_sub_layer_output, training=training)\n",
        "      # Add residual connection \n",
        "      bottom_sub_layer_output = bottom_sub_layer_input + bottom_sub_layer_output\n",
        "      # Normalization\n",
        "      bottom_sub_layer_output = self.attention_bottom_norm[i](bottom_sub_layer_output)\n",
        "      bottom_alignments.append(bottom_alignment)\n",
        "\n",
        "      # Middle Multi-Head Sub Layer\n",
        "      middle_sub_layer_input = bottom_sub_layer_output\n",
        "      # Pass in query, value, mask to Attention layer\n",
        "      middle_sub_layer_output, middle_alignment = self.attention_middle[i](middle_sub_layer_input, encoder_output, encoder_mask)\n",
        "      middle_sub_layer_output = self.attention_middle_dropout[i](middle_sub_layer_output, training=training)\n",
        "      # Add residual connection \n",
        "      middle_sub_layer_output = middle_sub_layer_output + middle_sub_layer_input\n",
        "      # Normalization\n",
        "      middle_sub_layer_output = self.attention_middle_norm[i](middle_sub_layer_output)\n",
        "      middle_alignments.append(middle_alignment)\n",
        "\n",
        "      # Feedforward Network\n",
        "      ffn_input = middle_sub_layer_output\n",
        "      ffn_output = self.dense_2[i](self.dense_1[i](ffn_input))\n",
        "      ffn_output = self.ffn_dropout[i](ffn_output, training=training)\n",
        "      # Add residual connection \n",
        "      ffn_output = ffn_output + ffn_input\n",
        "      # Normalization\n",
        "      ffn_output = self.ffn_norm[i](ffn_output)\n",
        "\n",
        "      # Pass FFN output to the next layer\n",
        "      bottom_sub_layer_input = ffn_output\n",
        "\n",
        "    # Compute final output logits (batch_size, length, model_size)\n",
        "    logits = self.dense(ffn_output)\n",
        "\n",
        "    # Final decoder output and bottom and middle alignment vectors\n",
        "    return logits, bottom_alignments, middle_alignments"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUPcSGhmtV1"
      },
      "source": [
        "**Transfomer Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xl5lEQ26sU0"
      },
      "source": [
        "class Transformer(keras.Model):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, model_size, num_layers, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = Encoder(source_vocab_size, model_size, num_layers, num_heads)\n",
        "    # Decoder\n",
        "    self.decoder = Decoder(target_vocab_size, model_size, num_layers, num_heads)\n",
        "\n",
        "  def train_step(self, data):\n",
        "    # Unpack data\n",
        "    x1, x2, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_mask = 1 - tf.cast(tf.equal(x1, 0), dtype=tf.float32)\n",
        "        # Add two more dimensions to make it broadcastable when computing attention heads\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        # Encoder\n",
        "        encoder_output, _ = self.encoder(x1, encoder_mask=encoder_mask)\n",
        "        # Decoder\n",
        "        decoder_output, _, _ = self.decoder(x2, encoder_output, encoder_mask=encoder_mask)\n",
        "        # Compute loss\n",
        "        loss = self.loss(y, decoder_output)\n",
        "\n",
        "    variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict of performance\n",
        "    results = {m.name: m.result() for m in self.metrics}\n",
        "    results.update({\"loss\": loss})\n",
        "    return results"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lboW8z61mIh6"
      },
      "source": [
        "##### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVpJdze9PwyA"
      },
      "source": [
        "# Function to perform translation using the model\n",
        "def translate_transformer(model,test_source_text=None, max_ouput_size=25):\n",
        "  if test_source_text is None:\n",
        "    test_source_text = data_en[np.random.choice(len(data_en))]\n",
        "  test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "\n",
        "  # Encoder\n",
        "  en_output, _ = model.encoder(tf.constant(test_source_seq), training=False)\n",
        "  # Decoder\n",
        "  decoder_input = tf.constant([[fr_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "  output_words = []\n",
        "  output_seq = []\n",
        "  for i in range(max_ouput_size):\n",
        "    de_output, _,_ = model.decoder(decoder_input, en_output, training=False)\n",
        "    new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "    output_seq.append(new_word.numpy()[0][0])\n",
        "    output_words.append(fr_tokenizer.index_word[output_seq[-1]])\n",
        "\n",
        "    # Add the predicted word to create a new input sequence\n",
        "    decoder_input = tf.concat((decoder_input, new_word), axis=-1)\n",
        "\n",
        "    # Check for end of sentence\n",
        "    if output_words[-1] == '<end>':\n",
        "        break\n",
        "\n",
        "  output_text = ' '.join(output_words)\n",
        "\n",
        "  return test_source_text, output_text\n",
        "\n",
        "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Learning schedule for training the Transformer\n",
        "    Attributes:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        warmup_steps: number of warmup steps at the beginning\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, warmup_steps=4000):\n",
        "        super(WarmupThenDecaySchedule, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step_term = tf.math.rsqrt(step)\n",
        "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NknH0v57meG"
      },
      "source": [
        "##### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oViUZyOu7m3o",
        "outputId": "dcb2c39a-408f-4639-fd32-03784663d5f1"
      },
      "source": [
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "epochs = 20\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model = Transformer(vocabulary_size_en, vocabulary_size_fr, MODEL_SIZE, NUM_LAYERS, NUM_HEADS)\n",
        "\n",
        "# Optimizer\n",
        "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
        "optimizer = keras.optimizers.Adam(lr,beta_1=0.9,beta_2=0.98,epsilon=1e-9)\n",
        "# Loss\n",
        "crossentropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "def custom_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "  return loss\n",
        "\n",
        "# Callback\n",
        "class DisplayTranslation(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print(\"\\nTranslation:\")\n",
        "    source_text, output_text = translate_transformer(self.model)\n",
        "    print(source_text,\" => \", output_text)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=optimizer, loss=custom_loss)\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "training_results = model.fit(\n",
        "        train_data, # train_data.take(20) while testing\n",
        "        epochs=epochs,\n",
        "        callbacks=[DisplayTranslation()],\n",
        "        verbose=1)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2609/2609 [==============================] - 226s 83ms/step - loss: 0.6558\n",
            "\n",
            "Translation:\n",
            "You raise a good point .  =>  tu ai trouve un bon . <end>\n",
            "Epoch 2/20\n",
            "2609/2609 [==============================] - 216s 83ms/step - loss: 0.3077\n",
            "\n",
            "Translation:\n",
            "I expected a better explanation .  =>  j attendais une explication . <end>\n",
            "Epoch 3/20\n",
            "2609/2609 [==============================] - 218s 83ms/step - loss: 0.2344\n",
            "\n",
            "Translation:\n",
            "This is an interesting book to read .  =>  c est un livre interessant a lire . <end>\n",
            "Epoch 4/20\n",
            "2609/2609 [==============================] - 217s 83ms/step - loss: 0.2062\n",
            "\n",
            "Translation:\n",
            "You have to be here at tomorrow afternoon .  =>  il vous faut pour l apres midi . <end>\n",
            "Epoch 5/20\n",
            "2609/2609 [==============================] - 217s 83ms/step - loss: 0.1912\n",
            "\n",
            "Translation:\n",
            "I think we can go faster .  =>  je peux aller plus vite . <end>\n",
            "Epoch 6/20\n",
            "2609/2609 [==============================] - 218s 84ms/step - loss: 0.1810\n",
            "\n",
            "Translation:\n",
            "I have a passport .  =>  j ai de passeport . <end>\n",
            "Epoch 7/20\n",
            "2609/2609 [==============================] - 218s 83ms/step - loss: 0.1735\n",
            "\n",
            "Translation:\n",
            "You can t handle this alone .  =>  tu n en dois pas par place . <end>\n",
            "Epoch 8/20\n",
            "2609/2609 [==============================] - 218s 83ms/step - loss: 0.1680\n",
            "\n",
            "Translation:\n",
            "I don t want to discuss it anymore .  =>  je ne veux plus en discuter . <end>\n",
            "Epoch 9/20\n",
            "2609/2609 [==============================] - 218s 84ms/step - loss: 0.1634\n",
            "\n",
            "Translation:\n",
            "I feel like I know you .  =>  j ai la connaissance . <end>\n",
            "Epoch 10/20\n",
            "2609/2609 [==============================] - 218s 84ms/step - loss: 0.1595\n",
            "\n",
            "Translation:\n",
            "Why would I want this ?  =>  pourquoi ? <end>\n",
            "Epoch 11/20\n",
            "2609/2609 [==============================] - 218s 84ms/step - loss: 0.1560\n",
            "\n",
            "Translation:\n",
            "They were sunbathing on the beach .  =>  ils ont de la plage . <end>\n",
            "Epoch 12/20\n",
            "2609/2609 [==============================] - 218s 84ms/step - loss: 0.1527\n",
            "\n",
            "Translation:\n",
            "I told them what I thought of their plan .  =>  je pensais a leur plan . <end>\n",
            "Epoch 13/20\n",
            "2609/2609 [==============================] - 219s 84ms/step - loss: 0.1499\n",
            "\n",
            "Translation:\n",
            "He snuck out to meet up with a girl .  =>  il a perdu la rencontrer un . <end>\n",
            "Epoch 14/20\n",
            "2609/2609 [==============================] - 219s 84ms/step - loss: 0.1475\n",
            "\n",
            "Translation:\n",
            "I m not afraid of work .  =>  je n ai pas peur du travail . <end>\n",
            "Epoch 15/20\n",
            "2609/2609 [==============================] - 217s 83ms/step - loss: 0.1454\n",
            "\n",
            "Translation:\n",
            "During the rush hours in Tokyo traffic is heavy .  =>  pendant les heures de circulation est lourd . <end>\n",
            "Epoch 16/20\n",
            "2609/2609 [==============================] - 218s 83ms/step - loss: 0.1434\n",
            "\n",
            "Translation:\n",
            "I am ready to follow you .  =>  je vous prepare . <end>\n",
            "Epoch 17/20\n",
            "2609/2609 [==============================] - 217s 83ms/step - loss: 0.1414\n",
            "\n",
            "Translation:\n",
            "I want to apologize to all of you for what just happened .  =>  je te veux tous pour ce qui s est passe . <end>\n",
            "Epoch 18/20\n",
            "2609/2609 [==============================] - 219s 84ms/step - loss: 0.1396\n",
            "\n",
            "Translation:\n",
            "The boss is very upset .  =>  le grande . <end>\n",
            "Epoch 19/20\n",
            "2609/2609 [==============================] - 220s 84ms/step - loss: 0.1378\n",
            "\n",
            "Translation:\n",
            "I want you to do this right .  =>  je vous en fais . <end>\n",
            "Epoch 20/20\n",
            "2609/2609 [==============================] - 219s 84ms/step - loss: 0.1366\n",
            "\n",
            "Translation:\n",
            "She insulted him .  =>  elle l insulte . <end>\n",
            "Training execution time (mins) 73.01690214474996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "yWs6xlfrSR5a",
        "outputId": "3079252e-c139-4f4a-98c8-d87684abc44a"
      },
      "source": [
        "evaluate_save_model(model, test_en[:1000], test_fr_out[:1000],translate_transformer,\n",
        "                    training_results, execution_time, 0.0, epochs)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'BLEU-1': 0.3875468008231227, 'BLEU-2': 0.2976296405972586, 'BLEU-3': 0.23368150090528325, 'BLEU-4': 0.15462339899245026}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxnvPIraph1b"
      },
      "source": [
        "##### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYSr7hoUpiW1"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBC1vaiY7ssr"
      },
      "source": [
        "### **Compare all Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM_Bp7HN7tY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "17e6410c-6002-4410-b182-27219123fce9"
      },
      "source": [
        "models_store_path = \"models\"\n",
        "\n",
        "models_metrics_list = glob(models_store_path+\"/*_metrics.json\")\n",
        "\n",
        "all_models_metrics = []\n",
        "for mm_file in models_metrics_list:\n",
        "  with open(mm_file) as json_file:\n",
        "    model_metrics = json.load(json_file)\n",
        "    all_models_metrics.append(model_metrics)\n",
        "\n",
        "# Load metrics to dataframe\n",
        "view_metrics = pd.DataFrame(data=all_models_metrics)\n",
        "\n",
        "# Format columns\n",
        "view_metrics['trainable_parameters'] = view_metrics['trainable_parameters'].map('{:,.0f}'.format)\n",
        "view_metrics['execution_time'] = view_metrics['execution_time'].map('{:,.2f} mins'.format)\n",
        "view_metrics['loss'] = view_metrics['loss'].map('{:,.2f}'.format)\n",
        "view_metrics['BLEU-1'] = view_metrics['BLEU-1'].map('{:,.4f}'.format)\n",
        "view_metrics['BLEU-2'] = view_metrics['BLEU-2'].map('{:,.4f}'.format)\n",
        "view_metrics['BLEU-3'] = view_metrics['BLEU-3'].map('{:,.4f}'.format)\n",
        "view_metrics['BLEU-4'] = view_metrics['BLEU-4'].map('{:,.4f}'.format)\n",
        "\n",
        "# Filter columns\n",
        "view_metrics = view_metrics[[\"trainable_parameters\",\"execution_time\",\"loss\",\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\n",
        "                             \"learning_rate\",\"epochs\",\"name\"]]\n",
        "view_metrics = view_metrics.sort_values(by=['loss'],ascending=True)\n",
        "view_metrics.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trainable_parameters</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>loss</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>epochs</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9,753,104</td>\n",
              "      <td>73.02 mins</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.3875</td>\n",
              "      <td>0.2976</td>\n",
              "      <td>0.2337</td>\n",
              "      <td>0.1546</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20</td>\n",
              "      <td>transformer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19,577,360</td>\n",
              "      <td>91.58 mins</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.2214</td>\n",
              "      <td>0.1472</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.01</td>\n",
              "      <td>20</td>\n",
              "      <td>sequence2_sequence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20,364,816</td>\n",
              "      <td>257.55 mins</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.2199</td>\n",
              "      <td>0.1511</td>\n",
              "      <td>0.0517</td>\n",
              "      <td>0.0123</td>\n",
              "      <td>0.01</td>\n",
              "      <td>20</td>\n",
              "      <td>sequence2_sequence_attention</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  trainable_parameters execution_time  ... epochs                          name\n",
              "1            9,753,104     73.02 mins  ...     20                   transformer\n",
              "0           19,577,360     91.58 mins  ...     20            sequence2_sequence\n",
              "2           20,364,816    257.55 mins  ...     20  sequence2_sequence_attention\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77i1q-42752b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE-uqeIBMvUk"
      },
      "source": [
        "## **Mini BERT Language Model** <div id='bert'>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXoYPaA3MzCS"
      },
      "source": [
        "### **Overview**\n",
        "\n",
        "In this section, we're going to be working with some text related to machine & deep learning. The data is a very small set of defintions pulled form wikipedia. The primary goal will be to understand BERT(Bidirectional Encoder Representations from Transformers) architecture.\n",
        "\n",
        "**The Task:** Build a masked language model using a simplified version of the BERT. \n",
        "\n",
        "\n",
        "**Language Model**: \n",
        "\n",
        "A model that understands language and how words appear in context to one another. The model is trained using unsupervised approaches such as next word prediction in a sentence or next sentence prediction.\n",
        "\n",
        "\n",
        "**Review BERT**:\n",
        "\n",
        "* Masked Language Model\n",
        "* Made up of only the Encoder with stacked transformer blocks\n",
        "* Bidirectional language\n",
        "* Good for fill in the blanks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyvvT0r-M1qT"
      },
      "source": [
        "### **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2UBJAYUM4cX"
      },
      "source": [
        "#### **Download Dataset**\n",
        "\n",
        "Download the datasets to colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10OpEurrM63D",
        "outputId": "d803bac2-32d6-4a13-fbf2-dc24bd00b6b4"
      },
      "source": [
        "start_time = time.time()\n",
        "download_file(\"https://storage.googleapis.com/cs109b/nlp/deep_learning_terms.txt\", base_path=\"datasets\", extract=False)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Download execution time (mins)\",execution_time)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download execution time (mins) 0.0040332555770874025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfTQ8cvVM_cf"
      },
      "source": [
        "#### **Load Data**\n",
        "\n",
        "* Read-in data as lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh3Ab6Q_NAXU",
        "outputId": "ffa962fc-844d-4ea6-edc4-6fbcb83ed402"
      },
      "source": [
        "data_text_file = os.path.join(\"datasets\",\"deep_learning_terms.txt\")\n",
        "data_text = []\n",
        "with open(data_text_file) as file:\n",
        "  for line in file:\n",
        "    data_text.append(line)\n",
        "\n",
        "print(\"data_text len:\", len(data_text))\n",
        "\n",
        "# Get sentences\n",
        "sentences = []\n",
        "for text in data_text:\n",
        "  sentences.extend(sent_tokenize(text))\n",
        "\n",
        "# Add a few DEMO sentence\n",
        "sentences.append(\"pavlos taught positional encoding to shivas\")\n",
        "print(\"sentences len:\", len(sentences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_text len: 20\n",
            "sentences len: 183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8w6Az3oNCeO"
      },
      "source": [
        "#### **View Text**\n",
        "\n",
        "Let's take a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "gvqawCFaNGUP",
        "outputId": "3cc24da7-84b7-405c-ca70-cf412d8f38d3"
      },
      "source": [
        "display(sentences[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Deep learning also known as deep structured learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.',\n",
              " 'Learning can be supervised, semi-supervised or unsupervised.',\n",
              " 'Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.',\n",
              " 'Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems.',\n",
              " 'ANNs have various differences from biological brains.',\n",
              " 'Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue.',\n",
              " 'The adjective deep in deep learning refers to the use of multiple layers in the network.',\n",
              " 'Early work showed that a linear perceptron cannot be a universal classifier, and then that a network with a nonpolynomial activation function with one hidden layer of unbounded width can on the other hand so be.',\n",
              " 'Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions.',\n",
              " 'In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the structured part.']"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv9tdJaNNLKc"
      },
      "source": [
        "### **Build Data Pipelines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9giF7zxONMMr"
      },
      "source": [
        "#### **Text Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glHAE7TKNQ9Q",
        "outputId": "78ce50eb-e5d4-4b3d-9cff-3a74ad0dfb8c"
      },
      "source": [
        "MASK_TOKEN = \"[mask]\"\n",
        "max_len = max([len(x.split()) for x in sentences])\n",
        "print(\"max_len\",max_len)\n",
        "\n",
        "def standardize_text(input_text):\n",
        "  # Convert to lowercase\n",
        "  output_text = tf.strings.lower(input_text) \n",
        "  return tf.strings.regex_replace(\n",
        "        output_text, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )\n",
        "\n",
        "text_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
        "\n",
        "# Initialize Text Vectorizer\n",
        "text_vectorizer = TextVectorization(\n",
        "    standardize=standardize_text,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_len\n",
        ")\n",
        "\n",
        "# Generate Text Vector\n",
        "text_vectorizer.adapt(sentences)\n",
        "\n",
        "# Get Vocabulary\n",
        "vocabulary = text_vectorizer.get_vocabulary()\n",
        "vocabulary_size = len(vocabulary)\n",
        "# Add the mask token\n",
        "vocabulary = vocabulary[2 : vocabulary_size - 1] + [MASK_TOKEN]\n",
        "text_vectorizer.set_vocabulary(vocabulary)\n",
        "vocabulary = text_vectorizer.get_vocabulary()\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(\"Vocabulary Size:\",vocabulary_size)\n",
        "print(vocabulary[-1])\n",
        "\n",
        "# Generate word index\n",
        "word_index = dict(zip(vocabulary, range(vocabulary_size)))\n",
        "index_word = dict(zip(range(vocabulary_size), vocabulary))\n",
        "\n",
        "print(\"vocabulary:\",len(vocabulary),vocabulary)\n",
        "print(\"word_index:\",word_index)\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = word_index[MASK_TOKEN]\n",
        "print(\"Mask token Id:\",mask_token_id)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len 66\n",
            "Vocabulary Size: 1130\n",
            "[mask]\n",
            "vocabulary: 1130 ['', '[UNK]', 'the', 'of', 'a', 'to', 'and', 'is', 'in', 'learning', 'that', 'as', 'or', 'data', 'can', 'with', 'neural', 'networks', 'be', 'language', 'such', 'an', 'are', 'machine', 'image', 'from', 'for', 'network', 'this', 'model', 'on', 'layers', 'it', 'artificial', 'training', 'deep', 'by', 'also', 'recognition', 'layer', 'computer', 'used', 'natural', 'input', 'have', 'which', 'translation', 'not', 'neurons', 'where', 'process', 'human', 'between', 'words', 'vision', 'supervised', 'speech', 'processing', 'field', 'feature', 'systems', 'recurrent', 'output', 'nlg', 'more', 'its', 'intelligence', 'each', 'understanding', 'tasks', 'signal', 'representation', 'other', 'one', 'labeled', 'known', 'into', 'images', 'different', 'biological', 'algorithms', 'when', 'visual', 'unsupervised', 'they', 'than', 'segmentation', 'produce', 'models', 'lstm', 'information', 'but', 'use', 'through', 'system', 'set', 'sequences', 'semisupervised', 'rnns', 'only', 'multiple', 'methods', 'impulse', 'hidden', 'function', 'features', 'example', 'connected', 'called', 'based', 'algorithm', 'while', 'video', 'units', 'transformer', 'trained', 'time', 'text', 'similar', 'problem', 'probability', 'often', 'memory', 'many', 'include', 'has', 'examples', 'detection', 'connections', 'computers', 'classification', 'applications', 'allows', '3d', 'whereas', 'using', 'unlabeled', 'typically', 'transformers', 'then', 'them', 'supervision', 'study', 'statistical', 'specific', 'sequence', 'representations', 'phrases', 'perform', 'patterns', 'part', 'over', 'nlu', 'needs', 'naturallanguage', 'medical', 'may', 'make', 'long', 'like', 'inputs', 'graph', 'eg', 'documents', 'dnn', 'digital', 'composition', 'complex', 'cnns', 'class', 'applied', 'application', 'another', 'analysis', 'world', 'word', 'without', 'within', 'were', 'was', 'visible', 'usually', 'unlike', 'ul', 'threshold', 'there', 'theory', 'their', 'term', 'temporal', 'take', 'symbolic', 'structured', 'some', 'software', 'shortterm', 'series', 'same', 'referred', 'produces', 'processes', 'predictions', 'practical', 'pixels', 'order', 'object', 'nodes', 'nlp', 'neuron', 'needed', 'multilayer', 'most', 'modeling', 'mathematical', 'makes', 'machines', 'length', 'large', 'languages', 'label', 'inspired', 'infinite', 'if', 'humans', 'how', 'handwriting', 'given', 'generative', 'generate', 'general', 'gated', 'gate', 'forms', 'finite', 'feedforward', 'feedback', 'etc', 'entire', 'easier', 'dynamic', 'do', 'directed', 'decisions', 'dbn', 'convolutional', 'connection', 'concerned', 'compared', 'characteristics', 'certain', 'cell', 'brains', 'brain', 'both', 'been', 'at', 'anns', 'amount', 'all', 'ai', 'after', 'advantage', 'will', 'whole', 'well', 'weights', 'weight', 'viewed', 'various', 'variety', 'value', 'user', 'unsegmented', 'unseen', 'unrolled', 'unigram', 'under', 'unbounded', 'typical', 'two', 'turn', 'traditional', 'times', 'these', 'therefore', 'theoretical', 'tend', 'techniques', 'task', 'tagged', 'synapses', 'subfield', 'structure', 'storage', 'statistics', 'states', 'state', 'sound', 'solving', 'so', 'smaller', 'small', 'situations', 'single', 'since', 'shallow', 'sets', 'sequential', 'sentence', 'segments', 'seen', 'seeks', 'see', 'scientific', 'scene', 'rnn', 'results', 'result', 'restricted', 'respect', 'requires', 'require', 'reports', 'replaced', 'relatively', 'relative', 'related', 'regularization', 'region', 'recognize', 'receptive', 'real', 'pretrained', 'precisely', 'possible', 'performance', 'perceptrons', 'perceptron', 'pattern', 'particular', 'parallelization', 'pair', 'out', 'objects', 'numerical', 'number', 'nonlinear', 'next', 'n', 'much', 'mt', 'ml', 'means', 'mean', 'making', 'major', 'lstms', 'lowest', 'lower', 'linguistics', 'learns', 'learned', 'learn', 'leading', 'kernels', 'invariant', 'introduced', 'internal', 'interactive', 'instance', 'including', 'ideas', 'however', 'generation', 'generally', 'functions', 'fully', 'form', 'filters', 'filtering', 'fields', 'extraction', 'extract', 'expressed', 'exhibit', 'estimation', 'engineering', 'edges', 'during', 'domains', 'dog', 'dnns', 'discover', 'discipline', 'differences', 'dictionary', 'device', 'determining', 'define', 'deal', 'datasets', 'custom', 'cover', 'corpus', 'convolution', 'contours', 'context', 'content', 'consisting', 'connectivity', 'computed', 'computational', 'composed', 'components', 'complexity', 'common', 'classes', 'building', 'build', 'broad', 'belief', 'behavior', 'before', 'automatically', 'automate', 'autoencoders', 'audio', 'attempts', 'architectures', 'architecture', 'approach', 'applicable', 'any', 'ann', 'animal', 'analyzing', 'analyze', 'allow', 'acquisition', '“recurrent', 'yielded', 'writing', 'wreck', 'work', 'wikipedia', 'width', 'widely', 'wide', 'whether', 'whence', 'wellsuited', 'web', 'weather', 'weak', 'ways', 'way', 'wants', 'vs', 'views', 'videos', 'versions', 'vector', 'varying', 'variation', 'variables', 'variable', 'vanishing', 'values', 'vaguely', 'useful', 'usable', 'untagged', 'unsupervisedin', 'unknown', 'universality', 'universal', 'unit', 'unfeasible', 'undirected', 'understandability', 'understand', 'typology', 'types', 'type', 'trimming', 'tries', 'traversing', 'travel', 'transpilers', 'transmit', 'translators', 'translations', 'translate', 'transforms', 'transformations', 'transformation', 'transcribe', 'trainability', 'traffic', 'tracking', 'towards', 'thus', 'three', 'thought', 'those', 'things', 'theories', 'themselves', 'texture', 'textual', 'texttospeech', 'textbooks', 'terms', 'technology', 'technological', 'technique', 'taught', 'target', 'takes', 'tagging', 'symmetrical', 'surpassing', 'supervisory', 'summarization', 'sum', 'successfully', 'substitutions', 'substitution', 'subset', 'subnetworks', 'subnetwork', 'subdomains', 'strong', 'strictly', 'strength', 'stored', 'stimuli', 'step', 'static', 'starting', 'standardised', 'standard', 'stack', 'spectrum', 'specifically', 'special', 'sparsity', 'sparse', 'space', 'sounds', 'sometimes', 'something', 'solution', 'social', 'socalled', 'sl', 'skilled', 'size', 'simply', 'simplify', 'simpler', 'simple', 'similarly', 'significantly', 'signals', 'siann', 'showed', 'should', 'short', 'shivas', 'shift', 'sharedweight', 'share', 'servoing', 'serves', 'sent', 'sensor', 'sense', 'selforganization', 'selfconsistent', 'select', 'segment', 'score', 'scope', 'science', 'scenario', 'scanning', 'scanner', 'scan', 'scale', 'say', 'sample', 'sake', 'review', 'revealed', 'return', 'retrieval', 'retina', 'retaining', 'resulting', 'restoration', 'respond', 'resolve', 'resembles', 'research', 'replacing', 'replaces', 'render', 'remembers', 'relying', 'relationships', 'reinforcement', 'regulate', 'regularized', 'regions', 'refers', 'refer', 'reduced', 'reconstructions', 'reconstruction', 'reconstruct', 'recommender', 'receives', 'reasonable', 'realworld', 'readily', 'read', 'rbms', 'rbm', 'raw', 'rarely', 'rapidlygrowing', 'rapidly', 'randomly', 'quality', 'put', 'psychological', 'psycholinguists', 'provides', 'proved', 'protein', 'proposed', 'property', 'pronunciation', 'prone', 'programs', 'programming', 'programmed', 'program', 'profession', 'production', 'produced', 'processed', 'proceeds', 'procedure', 'problems[2]', 'problems', 'probabilities', 'probabilistically', 'probabilistic', 'prior', 'primitives', 'primarily', 'previous', 'preprocessing', 'prefer', 'predictive', 'predelections', 'potentially', 'potential', 'possibly', 'positional', 'pose', 'portion', 'polynomials', 'points', 'plastic', 'pixel', 'physics', 'physical', 'perspective', 'permitted', 'permits', 'performs', 'performing', 'perceives', 'pavlos', 'partofspeech', 'partitioning', 'particularly', 'partially', 'parsing', 'pairs', 'overlap', 'overfitting', 'organize', 'organizations', 'organization', 'organisms', 'optimized', 'optimize', 'optimization', 'optimal', 'optical', 'opposite', 'older', 'oil', 'observed', 'numerous', 'nuances', 'normalized', 'nonpolynomial', 'no', 'nns', 'nice', 'ngram', 'new', 'neuronal', 'network”', 'need', 'name', 'multivariate', 'multidimensional', 'motivated', 'motion', 'modern', 'modeled', 'mobile', 'mining', 'minimized', 'mind', 'mimicry', 'mimic', 'mild', 'might', 'mechanical', 'measured', 'meaningful', 'meaning', 'maximize', 'matrix', 'mathematically', 'material', 'matched', 'markov', 'marching', 'maps', 'mapping', 'manual', 'manipulation', 'machineaided', 'm', 'loss', 'loosely', 'loops', 'location', 'locate', 'living', 'little', 'linguistic', 'lines', 'linear', 'limiting', 'likelihood', 'levels', 'level', 'less', 'legal', 'led', 'leads', 'layered', 'layerbylayer', 'latter', 'latent', 'last', 'larger', 'lags', 'labels', 'labelled', 'labeling', 'knowledge', 'isolation', 'involves', 'involve', 'investigates', 'invariance', 'intrusion', 'intervention', 'intervals', 'interpolation', 'intermediate', 'interfaces', 'interest', 'interdisciplinary', 'interactions', 'intensity', 'intelligent', 'integrated', 'instances', 'inspection', 'insights', 'insensitivity', 'inputoutput', 'informed', 'infers', 'inferred', 'infeasible', 'inexpensive', 'inductive', 'individual', 'indiscriminately', 'indexing', 'independent', 'independence', 'increasing', 'increases', 'incorporates', 'improving', 'improvement', 'improve', 'important', 'implementation', 'imaging', 'imagery', 'idss', 'idioms', 'idea', 'humanreadable', 'huge', 'hope', 'highlevel', 'highdimensional', 'hierarchical', 'heterogeneous', 'hence', 'help', 'handling', 'handle', 'handengineered', 'hand', 'guidance', 'great', 'graphical', 'gradient', 'gpt', 'government', 'good', 'goals', 'goal', 'go', 'gets', 'geometry', 'generated', 'generalize', 'generalization', 'gates', 'gap', 'game', 'gain', 'further', 'functioning', 'fullyconnectedness', 'frequently', 'formulaic', 'former', 'formal', 'forget', 'forced', 'follows', 'focusing', 'focuses', 'fnn', 'flow', 'fish', 'first', 'finetuned', 'financial', 'fewer', 'fast', 'family', 'falls', 'factorization', 'fact', 'facilitates', 'extreme', 'extracted', 'extra', 'expression', 'express', 'exponentially', 'exploratory', 'explicitly', 'explicit', 'expert', 'experiment', 'experience', 'exhibits', 'examination', 'evidence', 'every', 'events', 'event', 'even', 'estimating', 'especially', 'error', 'erroneous', 'equivalent', 'environment', 'english', 'energybased', 'end', 'encountered', 'encoding', 'encoder', 'enabled', 'enable', 'emulate', 'emotionality', 'embossed', 'email', 'elicit', 'either', 'efficiency', 'effective', 'edge', 'early', 'duration', 'due', 'drug', 'domain', 'does', 'divergence', 'distribution', 'distributed', 'distinguish', 'distinction', 'displayed', 'display', 'disentangling', 'disambiguate', 'direct', 'difficult', 'deviate', 'development', 'developed', 'develop', 'determine', 'detectors', 'desired', 'designed', 'design', 'descriptions', 'described', 'describe', 'derived', 'depends', 'densities', 'demonstrated', 'delivers', 'delays', 'deeplearning', 'decreases', 'decompilers', 'deals', 'dbns', 'cyclic', 'customization', 'curves', 'current', 'cubes', 'crosses', 'create', 'crawl', 'counterparts', 'cost', 'cortical', 'cortex', 'correctly', 'convnet', 'conversations', 'conversation', 'conventional', 'convenient', 'controlled', 'control', 'contrastive', 'contrast', 'contextual', 'contents', 'contained', 'construction', 'constructed', 'constitute', 'consist', 'considered', 'considerations', 'considerably', 'considerable', 'consciousness', 'connectionist', 'connectedness', 'conjunction', 'confused', 'conditions', 'concept', 'computing', 'computeraided', 'computationally', 'compositional', 'component', 'comparable', 'compact', 'communication', 'commonly', 'combines', 'color', 'colloquially', 'collectively', 'collection', 'cognitive', 'code', 'cnn', 'clustering', 'closest', 'closely', 'classifying', 'classifier', 'chosen', 'choose', 'choice', 'chatbot', 'characteristic', 'character', 'change', 'chance', 'challenging', 'challenges', 'categorize', 'categories', 'cases', 'car', 'captures', 'capable', 'cannot', 'cameras', 'calculate', 'business', 'broader', 'breeds', 'breed', 'braincomputer', 'bounded', 'boundaries', 'boltzmann', 'board', 'blurbs', 'biologically', 'bioinformatics', 'bidirectional', 'biases', 'bias', 'better', 'bert', 'being', 'behind', 'beginning', 'become', 'because', 'beach', 'basic', 'bag', 'automated', 'assumption', 'associated', 'associate', 'assigns', 'assigning', 'assemble', 'arbitrary', 'approximate', 'appropriate', 'apply', 'anomaly', 'anomalies', 'animals', 'andor', 'analyzes', 'analytics', 'analogue', 'amounts', 'american', 'ambiguous', 'ambiguity', 'ambiguities', 'always', 'alternatively', 'alternative', 'along', 'alone', 'allowable', 'algorithmically', 'aid', 'agi', 'aggregated', 'aggregate', 'agents', 'agent', 'adjusts', 'adjective', 'adjacent', 'additional', 'acyclic', 'activation', 'actions', 'action', 'act', 'across', 'acronym', 'acquiring', 'acoustic', 'achieving', 'accurately', 'accuracy', 'above', 'about', 'abi', 'abbreviation', '2017', '[mask]']\n",
            "word_index: {'': 0, '[UNK]': 1, 'the': 2, 'of': 3, 'a': 4, 'to': 5, 'and': 6, 'is': 7, 'in': 8, 'learning': 9, 'that': 10, 'as': 11, 'or': 12, 'data': 13, 'can': 14, 'with': 15, 'neural': 16, 'networks': 17, 'be': 18, 'language': 19, 'such': 20, 'an': 21, 'are': 22, 'machine': 23, 'image': 24, 'from': 25, 'for': 26, 'network': 27, 'this': 28, 'model': 29, 'on': 30, 'layers': 31, 'it': 32, 'artificial': 33, 'training': 34, 'deep': 35, 'by': 36, 'also': 37, 'recognition': 38, 'layer': 39, 'computer': 40, 'used': 41, 'natural': 42, 'input': 43, 'have': 44, 'which': 45, 'translation': 46, 'not': 47, 'neurons': 48, 'where': 49, 'process': 50, 'human': 51, 'between': 52, 'words': 53, 'vision': 54, 'supervised': 55, 'speech': 56, 'processing': 57, 'field': 58, 'feature': 59, 'systems': 60, 'recurrent': 61, 'output': 62, 'nlg': 63, 'more': 64, 'its': 65, 'intelligence': 66, 'each': 67, 'understanding': 68, 'tasks': 69, 'signal': 70, 'representation': 71, 'other': 72, 'one': 73, 'labeled': 74, 'known': 75, 'into': 76, 'images': 77, 'different': 78, 'biological': 79, 'algorithms': 80, 'when': 81, 'visual': 82, 'unsupervised': 83, 'they': 84, 'than': 85, 'segmentation': 86, 'produce': 87, 'models': 88, 'lstm': 89, 'information': 90, 'but': 91, 'use': 92, 'through': 93, 'system': 94, 'set': 95, 'sequences': 96, 'semisupervised': 97, 'rnns': 98, 'only': 99, 'multiple': 100, 'methods': 101, 'impulse': 102, 'hidden': 103, 'function': 104, 'features': 105, 'example': 106, 'connected': 107, 'called': 108, 'based': 109, 'algorithm': 110, 'while': 111, 'video': 112, 'units': 113, 'transformer': 114, 'trained': 115, 'time': 116, 'text': 117, 'similar': 118, 'problem': 119, 'probability': 120, 'often': 121, 'memory': 122, 'many': 123, 'include': 124, 'has': 125, 'examples': 126, 'detection': 127, 'connections': 128, 'computers': 129, 'classification': 130, 'applications': 131, 'allows': 132, '3d': 133, 'whereas': 134, 'using': 135, 'unlabeled': 136, 'typically': 137, 'transformers': 138, 'then': 139, 'them': 140, 'supervision': 141, 'study': 142, 'statistical': 143, 'specific': 144, 'sequence': 145, 'representations': 146, 'phrases': 147, 'perform': 148, 'patterns': 149, 'part': 150, 'over': 151, 'nlu': 152, 'needs': 153, 'naturallanguage': 154, 'medical': 155, 'may': 156, 'make': 157, 'long': 158, 'like': 159, 'inputs': 160, 'graph': 161, 'eg': 162, 'documents': 163, 'dnn': 164, 'digital': 165, 'composition': 166, 'complex': 167, 'cnns': 168, 'class': 169, 'applied': 170, 'application': 171, 'another': 172, 'analysis': 173, 'world': 174, 'word': 175, 'without': 176, 'within': 177, 'were': 178, 'was': 179, 'visible': 180, 'usually': 181, 'unlike': 182, 'ul': 183, 'threshold': 184, 'there': 185, 'theory': 186, 'their': 187, 'term': 188, 'temporal': 189, 'take': 190, 'symbolic': 191, 'structured': 192, 'some': 193, 'software': 194, 'shortterm': 195, 'series': 196, 'same': 197, 'referred': 198, 'produces': 199, 'processes': 200, 'predictions': 201, 'practical': 202, 'pixels': 203, 'order': 204, 'object': 205, 'nodes': 206, 'nlp': 207, 'neuron': 208, 'needed': 209, 'multilayer': 210, 'most': 211, 'modeling': 212, 'mathematical': 213, 'makes': 214, 'machines': 215, 'length': 216, 'large': 217, 'languages': 218, 'label': 219, 'inspired': 220, 'infinite': 221, 'if': 222, 'humans': 223, 'how': 224, 'handwriting': 225, 'given': 226, 'generative': 227, 'generate': 228, 'general': 229, 'gated': 230, 'gate': 231, 'forms': 232, 'finite': 233, 'feedforward': 234, 'feedback': 235, 'etc': 236, 'entire': 237, 'easier': 238, 'dynamic': 239, 'do': 240, 'directed': 241, 'decisions': 242, 'dbn': 243, 'convolutional': 244, 'connection': 245, 'concerned': 246, 'compared': 247, 'characteristics': 248, 'certain': 249, 'cell': 250, 'brains': 251, 'brain': 252, 'both': 253, 'been': 254, 'at': 255, 'anns': 256, 'amount': 257, 'all': 258, 'ai': 259, 'after': 260, 'advantage': 261, 'will': 262, 'whole': 263, 'well': 264, 'weights': 265, 'weight': 266, 'viewed': 267, 'various': 268, 'variety': 269, 'value': 270, 'user': 271, 'unsegmented': 272, 'unseen': 273, 'unrolled': 274, 'unigram': 275, 'under': 276, 'unbounded': 277, 'typical': 278, 'two': 279, 'turn': 280, 'traditional': 281, 'times': 282, 'these': 283, 'therefore': 284, 'theoretical': 285, 'tend': 286, 'techniques': 287, 'task': 288, 'tagged': 289, 'synapses': 290, 'subfield': 291, 'structure': 292, 'storage': 293, 'statistics': 294, 'states': 295, 'state': 296, 'sound': 297, 'solving': 298, 'so': 299, 'smaller': 300, 'small': 301, 'situations': 302, 'single': 303, 'since': 304, 'shallow': 305, 'sets': 306, 'sequential': 307, 'sentence': 308, 'segments': 309, 'seen': 310, 'seeks': 311, 'see': 312, 'scientific': 313, 'scene': 314, 'rnn': 315, 'results': 316, 'result': 317, 'restricted': 318, 'respect': 319, 'requires': 320, 'require': 321, 'reports': 322, 'replaced': 323, 'relatively': 324, 'relative': 325, 'related': 326, 'regularization': 327, 'region': 328, 'recognize': 329, 'receptive': 330, 'real': 331, 'pretrained': 332, 'precisely': 333, 'possible': 334, 'performance': 335, 'perceptrons': 336, 'perceptron': 337, 'pattern': 338, 'particular': 339, 'parallelization': 340, 'pair': 341, 'out': 342, 'objects': 343, 'numerical': 344, 'number': 345, 'nonlinear': 346, 'next': 347, 'n': 348, 'much': 349, 'mt': 350, 'ml': 351, 'means': 352, 'mean': 353, 'making': 354, 'major': 355, 'lstms': 356, 'lowest': 357, 'lower': 358, 'linguistics': 359, 'learns': 360, 'learned': 361, 'learn': 362, 'leading': 363, 'kernels': 364, 'invariant': 365, 'introduced': 366, 'internal': 367, 'interactive': 368, 'instance': 369, 'including': 370, 'ideas': 371, 'however': 372, 'generation': 373, 'generally': 374, 'functions': 375, 'fully': 376, 'form': 377, 'filters': 378, 'filtering': 379, 'fields': 380, 'extraction': 381, 'extract': 382, 'expressed': 383, 'exhibit': 384, 'estimation': 385, 'engineering': 386, 'edges': 387, 'during': 388, 'domains': 389, 'dog': 390, 'dnns': 391, 'discover': 392, 'discipline': 393, 'differences': 394, 'dictionary': 395, 'device': 396, 'determining': 397, 'define': 398, 'deal': 399, 'datasets': 400, 'custom': 401, 'cover': 402, 'corpus': 403, 'convolution': 404, 'contours': 405, 'context': 406, 'content': 407, 'consisting': 408, 'connectivity': 409, 'computed': 410, 'computational': 411, 'composed': 412, 'components': 413, 'complexity': 414, 'common': 415, 'classes': 416, 'building': 417, 'build': 418, 'broad': 419, 'belief': 420, 'behavior': 421, 'before': 422, 'automatically': 423, 'automate': 424, 'autoencoders': 425, 'audio': 426, 'attempts': 427, 'architectures': 428, 'architecture': 429, 'approach': 430, 'applicable': 431, 'any': 432, 'ann': 433, 'animal': 434, 'analyzing': 435, 'analyze': 436, 'allow': 437, 'acquisition': 438, '“recurrent': 439, 'yielded': 440, 'writing': 441, 'wreck': 442, 'work': 443, 'wikipedia': 444, 'width': 445, 'widely': 446, 'wide': 447, 'whether': 448, 'whence': 449, 'wellsuited': 450, 'web': 451, 'weather': 452, 'weak': 453, 'ways': 454, 'way': 455, 'wants': 456, 'vs': 457, 'views': 458, 'videos': 459, 'versions': 460, 'vector': 461, 'varying': 462, 'variation': 463, 'variables': 464, 'variable': 465, 'vanishing': 466, 'values': 467, 'vaguely': 468, 'useful': 469, 'usable': 470, 'untagged': 471, 'unsupervisedin': 472, 'unknown': 473, 'universality': 474, 'universal': 475, 'unit': 476, 'unfeasible': 477, 'undirected': 478, 'understandability': 479, 'understand': 480, 'typology': 481, 'types': 482, 'type': 483, 'trimming': 484, 'tries': 485, 'traversing': 486, 'travel': 487, 'transpilers': 488, 'transmit': 489, 'translators': 490, 'translations': 491, 'translate': 492, 'transforms': 493, 'transformations': 494, 'transformation': 495, 'transcribe': 496, 'trainability': 497, 'traffic': 498, 'tracking': 499, 'towards': 500, 'thus': 501, 'three': 502, 'thought': 503, 'those': 504, 'things': 505, 'theories': 506, 'themselves': 507, 'texture': 508, 'textual': 509, 'texttospeech': 510, 'textbooks': 511, 'terms': 512, 'technology': 513, 'technological': 514, 'technique': 515, 'taught': 516, 'target': 517, 'takes': 518, 'tagging': 519, 'symmetrical': 520, 'surpassing': 521, 'supervisory': 522, 'summarization': 523, 'sum': 524, 'successfully': 525, 'substitutions': 526, 'substitution': 527, 'subset': 528, 'subnetworks': 529, 'subnetwork': 530, 'subdomains': 531, 'strong': 532, 'strictly': 533, 'strength': 534, 'stored': 535, 'stimuli': 536, 'step': 537, 'static': 538, 'starting': 539, 'standardised': 540, 'standard': 541, 'stack': 542, 'spectrum': 543, 'specifically': 544, 'special': 545, 'sparsity': 546, 'sparse': 547, 'space': 548, 'sounds': 549, 'sometimes': 550, 'something': 551, 'solution': 552, 'social': 553, 'socalled': 554, 'sl': 555, 'skilled': 556, 'size': 557, 'simply': 558, 'simplify': 559, 'simpler': 560, 'simple': 561, 'similarly': 562, 'significantly': 563, 'signals': 564, 'siann': 565, 'showed': 566, 'should': 567, 'short': 568, 'shivas': 569, 'shift': 570, 'sharedweight': 571, 'share': 572, 'servoing': 573, 'serves': 574, 'sent': 575, 'sensor': 576, 'sense': 577, 'selforganization': 578, 'selfconsistent': 579, 'select': 580, 'segment': 581, 'score': 582, 'scope': 583, 'science': 584, 'scenario': 585, 'scanning': 586, 'scanner': 587, 'scan': 588, 'scale': 589, 'say': 590, 'sample': 591, 'sake': 592, 'review': 593, 'revealed': 594, 'return': 595, 'retrieval': 596, 'retina': 597, 'retaining': 598, 'resulting': 599, 'restoration': 600, 'respond': 601, 'resolve': 602, 'resembles': 603, 'research': 604, 'replacing': 605, 'replaces': 606, 'render': 607, 'remembers': 608, 'relying': 609, 'relationships': 610, 'reinforcement': 611, 'regulate': 612, 'regularized': 613, 'regions': 614, 'refers': 615, 'refer': 616, 'reduced': 617, 'reconstructions': 618, 'reconstruction': 619, 'reconstruct': 620, 'recommender': 621, 'receives': 622, 'reasonable': 623, 'realworld': 624, 'readily': 625, 'read': 626, 'rbms': 627, 'rbm': 628, 'raw': 629, 'rarely': 630, 'rapidlygrowing': 631, 'rapidly': 632, 'randomly': 633, 'quality': 634, 'put': 635, 'psychological': 636, 'psycholinguists': 637, 'provides': 638, 'proved': 639, 'protein': 640, 'proposed': 641, 'property': 642, 'pronunciation': 643, 'prone': 644, 'programs': 645, 'programming': 646, 'programmed': 647, 'program': 648, 'profession': 649, 'production': 650, 'produced': 651, 'processed': 652, 'proceeds': 653, 'procedure': 654, 'problems[2]': 655, 'problems': 656, 'probabilities': 657, 'probabilistically': 658, 'probabilistic': 659, 'prior': 660, 'primitives': 661, 'primarily': 662, 'previous': 663, 'preprocessing': 664, 'prefer': 665, 'predictive': 666, 'predelections': 667, 'potentially': 668, 'potential': 669, 'possibly': 670, 'positional': 671, 'pose': 672, 'portion': 673, 'polynomials': 674, 'points': 675, 'plastic': 676, 'pixel': 677, 'physics': 678, 'physical': 679, 'perspective': 680, 'permitted': 681, 'permits': 682, 'performs': 683, 'performing': 684, 'perceives': 685, 'pavlos': 686, 'partofspeech': 687, 'partitioning': 688, 'particularly': 689, 'partially': 690, 'parsing': 691, 'pairs': 692, 'overlap': 693, 'overfitting': 694, 'organize': 695, 'organizations': 696, 'organization': 697, 'organisms': 698, 'optimized': 699, 'optimize': 700, 'optimization': 701, 'optimal': 702, 'optical': 703, 'opposite': 704, 'older': 705, 'oil': 706, 'observed': 707, 'numerous': 708, 'nuances': 709, 'normalized': 710, 'nonpolynomial': 711, 'no': 712, 'nns': 713, 'nice': 714, 'ngram': 715, 'new': 716, 'neuronal': 717, 'network”': 718, 'need': 719, 'name': 720, 'multivariate': 721, 'multidimensional': 722, 'motivated': 723, 'motion': 724, 'modern': 725, 'modeled': 726, 'mobile': 727, 'mining': 728, 'minimized': 729, 'mind': 730, 'mimicry': 731, 'mimic': 732, 'mild': 733, 'might': 734, 'mechanical': 735, 'measured': 736, 'meaningful': 737, 'meaning': 738, 'maximize': 739, 'matrix': 740, 'mathematically': 741, 'material': 742, 'matched': 743, 'markov': 744, 'marching': 745, 'maps': 746, 'mapping': 747, 'manual': 748, 'manipulation': 749, 'machineaided': 750, 'm': 751, 'loss': 752, 'loosely': 753, 'loops': 754, 'location': 755, 'locate': 756, 'living': 757, 'little': 758, 'linguistic': 759, 'lines': 760, 'linear': 761, 'limiting': 762, 'likelihood': 763, 'levels': 764, 'level': 765, 'less': 766, 'legal': 767, 'led': 768, 'leads': 769, 'layered': 770, 'layerbylayer': 771, 'latter': 772, 'latent': 773, 'last': 774, 'larger': 775, 'lags': 776, 'labels': 777, 'labelled': 778, 'labeling': 779, 'knowledge': 780, 'isolation': 781, 'involves': 782, 'involve': 783, 'investigates': 784, 'invariance': 785, 'intrusion': 786, 'intervention': 787, 'intervals': 788, 'interpolation': 789, 'intermediate': 790, 'interfaces': 791, 'interest': 792, 'interdisciplinary': 793, 'interactions': 794, 'intensity': 795, 'intelligent': 796, 'integrated': 797, 'instances': 798, 'inspection': 799, 'insights': 800, 'insensitivity': 801, 'inputoutput': 802, 'informed': 803, 'infers': 804, 'inferred': 805, 'infeasible': 806, 'inexpensive': 807, 'inductive': 808, 'individual': 809, 'indiscriminately': 810, 'indexing': 811, 'independent': 812, 'independence': 813, 'increasing': 814, 'increases': 815, 'incorporates': 816, 'improving': 817, 'improvement': 818, 'improve': 819, 'important': 820, 'implementation': 821, 'imaging': 822, 'imagery': 823, 'idss': 824, 'idioms': 825, 'idea': 826, 'humanreadable': 827, 'huge': 828, 'hope': 829, 'highlevel': 830, 'highdimensional': 831, 'hierarchical': 832, 'heterogeneous': 833, 'hence': 834, 'help': 835, 'handling': 836, 'handle': 837, 'handengineered': 838, 'hand': 839, 'guidance': 840, 'great': 841, 'graphical': 842, 'gradient': 843, 'gpt': 844, 'government': 845, 'good': 846, 'goals': 847, 'goal': 848, 'go': 849, 'gets': 850, 'geometry': 851, 'generated': 852, 'generalize': 853, 'generalization': 854, 'gates': 855, 'gap': 856, 'game': 857, 'gain': 858, 'further': 859, 'functioning': 860, 'fullyconnectedness': 861, 'frequently': 862, 'formulaic': 863, 'former': 864, 'formal': 865, 'forget': 866, 'forced': 867, 'follows': 868, 'focusing': 869, 'focuses': 870, 'fnn': 871, 'flow': 872, 'fish': 873, 'first': 874, 'finetuned': 875, 'financial': 876, 'fewer': 877, 'fast': 878, 'family': 879, 'falls': 880, 'factorization': 881, 'fact': 882, 'facilitates': 883, 'extreme': 884, 'extracted': 885, 'extra': 886, 'expression': 887, 'express': 888, 'exponentially': 889, 'exploratory': 890, 'explicitly': 891, 'explicit': 892, 'expert': 893, 'experiment': 894, 'experience': 895, 'exhibits': 896, 'examination': 897, 'evidence': 898, 'every': 899, 'events': 900, 'event': 901, 'even': 902, 'estimating': 903, 'especially': 904, 'error': 905, 'erroneous': 906, 'equivalent': 907, 'environment': 908, 'english': 909, 'energybased': 910, 'end': 911, 'encountered': 912, 'encoding': 913, 'encoder': 914, 'enabled': 915, 'enable': 916, 'emulate': 917, 'emotionality': 918, 'embossed': 919, 'email': 920, 'elicit': 921, 'either': 922, 'efficiency': 923, 'effective': 924, 'edge': 925, 'early': 926, 'duration': 927, 'due': 928, 'drug': 929, 'domain': 930, 'does': 931, 'divergence': 932, 'distribution': 933, 'distributed': 934, 'distinguish': 935, 'distinction': 936, 'displayed': 937, 'display': 938, 'disentangling': 939, 'disambiguate': 940, 'direct': 941, 'difficult': 942, 'deviate': 943, 'development': 944, 'developed': 945, 'develop': 946, 'determine': 947, 'detectors': 948, 'desired': 949, 'designed': 950, 'design': 951, 'descriptions': 952, 'described': 953, 'describe': 954, 'derived': 955, 'depends': 956, 'densities': 957, 'demonstrated': 958, 'delivers': 959, 'delays': 960, 'deeplearning': 961, 'decreases': 962, 'decompilers': 963, 'deals': 964, 'dbns': 965, 'cyclic': 966, 'customization': 967, 'curves': 968, 'current': 969, 'cubes': 970, 'crosses': 971, 'create': 972, 'crawl': 973, 'counterparts': 974, 'cost': 975, 'cortical': 976, 'cortex': 977, 'correctly': 978, 'convnet': 979, 'conversations': 980, 'conversation': 981, 'conventional': 982, 'convenient': 983, 'controlled': 984, 'control': 985, 'contrastive': 986, 'contrast': 987, 'contextual': 988, 'contents': 989, 'contained': 990, 'construction': 991, 'constructed': 992, 'constitute': 993, 'consist': 994, 'considered': 995, 'considerations': 996, 'considerably': 997, 'considerable': 998, 'consciousness': 999, 'connectionist': 1000, 'connectedness': 1001, 'conjunction': 1002, 'confused': 1003, 'conditions': 1004, 'concept': 1005, 'computing': 1006, 'computeraided': 1007, 'computationally': 1008, 'compositional': 1009, 'component': 1010, 'comparable': 1011, 'compact': 1012, 'communication': 1013, 'commonly': 1014, 'combines': 1015, 'color': 1016, 'colloquially': 1017, 'collectively': 1018, 'collection': 1019, 'cognitive': 1020, 'code': 1021, 'cnn': 1022, 'clustering': 1023, 'closest': 1024, 'closely': 1025, 'classifying': 1026, 'classifier': 1027, 'chosen': 1028, 'choose': 1029, 'choice': 1030, 'chatbot': 1031, 'characteristic': 1032, 'character': 1033, 'change': 1034, 'chance': 1035, 'challenging': 1036, 'challenges': 1037, 'categorize': 1038, 'categories': 1039, 'cases': 1040, 'car': 1041, 'captures': 1042, 'capable': 1043, 'cannot': 1044, 'cameras': 1045, 'calculate': 1046, 'business': 1047, 'broader': 1048, 'breeds': 1049, 'breed': 1050, 'braincomputer': 1051, 'bounded': 1052, 'boundaries': 1053, 'boltzmann': 1054, 'board': 1055, 'blurbs': 1056, 'biologically': 1057, 'bioinformatics': 1058, 'bidirectional': 1059, 'biases': 1060, 'bias': 1061, 'better': 1062, 'bert': 1063, 'being': 1064, 'behind': 1065, 'beginning': 1066, 'become': 1067, 'because': 1068, 'beach': 1069, 'basic': 1070, 'bag': 1071, 'automated': 1072, 'assumption': 1073, 'associated': 1074, 'associate': 1075, 'assigns': 1076, 'assigning': 1077, 'assemble': 1078, 'arbitrary': 1079, 'approximate': 1080, 'appropriate': 1081, 'apply': 1082, 'anomaly': 1083, 'anomalies': 1084, 'animals': 1085, 'andor': 1086, 'analyzes': 1087, 'analytics': 1088, 'analogue': 1089, 'amounts': 1090, 'american': 1091, 'ambiguous': 1092, 'ambiguity': 1093, 'ambiguities': 1094, 'always': 1095, 'alternatively': 1096, 'alternative': 1097, 'along': 1098, 'alone': 1099, 'allowable': 1100, 'algorithmically': 1101, 'aid': 1102, 'agi': 1103, 'aggregated': 1104, 'aggregate': 1105, 'agents': 1106, 'agent': 1107, 'adjusts': 1108, 'adjective': 1109, 'adjacent': 1110, 'additional': 1111, 'acyclic': 1112, 'activation': 1113, 'actions': 1114, 'action': 1115, 'act': 1116, 'across': 1117, 'acronym': 1118, 'acquiring': 1119, 'acoustic': 1120, 'achieving': 1121, 'accurately': 1122, 'accuracy': 1123, 'above': 1124, 'about': 1125, 'abi': 1126, 'abbreviation': 1127, '2017': 1128, '[mask]': 1129}\n",
            "Mask token Id: 1129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNNjXuOgNUxo",
        "outputId": "f8911ed4-e21c-41ec-c425-9a3ce7e09d67"
      },
      "source": [
        "# 1) Convert input text to tokens\n",
        "all_data_tokens = text_vectorizer(sentences)\n",
        "print(\"Input text:\\n\")\n",
        "print(sentences[0])\n",
        "print(sentences[1])\n",
        "print(sentences[3])\n",
        "print(\"all_data_tokens shape:\",all_data_tokens.shape)\n",
        "print(\"Tokenized text:\", all_data_tokens[:5,:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "\n",
            "Deep learning also known as deep structured learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n",
            "Learning can be supervised, semi-supervised or unsupervised.\n",
            "Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems.\n",
            "all_data_tokens shape: (183, 66)\n",
            "Tokenized text: tf.Tensor(\n",
            "[[  35    9   37   75   11   35  192    9    7  150    3    4 1048  879\n",
            "     3   23    9  101  109   30]\n",
            " [   9   14   18   55   97   12   83    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]\n",
            " [ 961  428   20   11   35   16   17   35  420   17   61   16   17    6\n",
            "   244   16   17   44  254  170]\n",
            " [  33   16   17  256  178  220   36   90   57    6  934 1013  206    8\n",
            "    79   60    0    0    0    0]\n",
            " [ 256   44  268  394   25   79  251    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]], shape=(5, 20), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1Luc53NYD5"
      },
      "source": [
        "#### **Generate Training Data**\n",
        "\n",
        "For the training we need inputs and lables but we only have input texts. In lecture we learnt that language models are trained in a smei supervised way where we generate inputs and labels from the input text. \n",
        "\n",
        "<br>\n",
        "\n",
        "To generate inputs and lables for training we will mask random words from the input text. Then our labels will be the masked text. The model will be trained to predict the masked word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki3-sDuSNb0T",
        "outputId": "599aef3b-cd60-445c-cb12-705520f7efd4"
      },
      "source": [
        "# Since we have a very small dataset, we will just replicate the data\n",
        "encoded_texts = all_data_tokens.numpy()\n",
        "print(encoded_texts.shape)\n",
        "encoded_texts = np.vstack([encoded_texts]*50)\n",
        "print(encoded_texts.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(183, 66)\n",
            "(9150, 66)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50k84FwINfGx",
        "outputId": "bebb2f32-a34a-4545-dc03-eb7f86f7ae9c"
      },
      "source": [
        "# 15% BERT masking\n",
        "inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "print(\"inp_mask:\",inp_mask.shape)\n",
        "#print(\"inp_mask:\",inp_mask[0,:25])\n",
        "\n",
        "# Exclude masking special tokens\n",
        "inp_mask[encoded_texts <= 2] = False\n",
        "\n",
        "# Set targets to -1 by default, it means ignore\n",
        "labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "\n",
        "# Set labels for masked tokens\n",
        "labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "# Prepare input\n",
        "encoded_texts_masked = np.copy(encoded_texts)\n",
        "\n",
        "# Set input to [MASK] which is the last token for the 90% of tokens\n",
        "# This means leaving 10% unchanged\n",
        "inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "encoded_texts_masked[\n",
        "    inp_mask_2mask\n",
        "] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "# Set 10% to a random token\n",
        "inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "    3, mask_token_id, inp_mask_2random.sum()\n",
        ")\n",
        "\n",
        "# Prepare sample_weights to pass to .fit() method\n",
        "sample_weights = np.ones(labels.shape)\n",
        "sample_weights[labels == -1] = 0\n",
        "\n",
        "# y_labels would be same as encoded_texts i.e input tokens\n",
        "y_labels = np.copy(encoded_texts)\n",
        "\n",
        "print(\"encoded_texts_masked:\",encoded_texts_masked.shape)\n",
        "print(encoded_texts_masked[:5,:10])\n",
        "print(\"y_labels:\",y_labels.shape)\n",
        "print(y_labels[:5,:10])\n",
        "print(\"sample_weights:\",sample_weights.shape)\n",
        "print(sample_weights[:5,:10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp_mask: (9150, 66)\n",
            "encoded_texts_masked: (9150, 66)\n",
            "[[  35    9   37   75   11   35  192    9    7  150]\n",
            " [   9   14 1129   55   97   12 1129    0    0    0]\n",
            " [ 961  428   20   11   35 1129   17   35  420   17]\n",
            " [1129   16   17  256 1129  220   36   90   57    6]\n",
            " [ 256   44 1129  394   25   79  251    0    0    0]]\n",
            "y_labels: (9150, 66)\n",
            "[[ 35   9  37  75  11  35 192   9   7 150]\n",
            " [  9  14  18  55  97  12  83   0   0   0]\n",
            " [961 428  20  11  35  16  17  35 420  17]\n",
            " [ 33  16  17 256 178 220  36  90  57   6]\n",
            " [256  44 268 394  25  79 251   0   0   0]]\n",
            "sample_weights: (9150, 66)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSqYIavhNjEo"
      },
      "source": [
        "#### **Create TF Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg-0hgLBNjop",
        "outputId": "a953778a-90d0-4e32-df7c-1b11869c6ae4"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "TRAIN_SHUFFLE_BUFFER_SIZE = encoded_texts.shape[0]\n",
        "\n",
        "# Create TF Dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((encoded_texts_masked, y_labels, sample_weights))\n",
        "\n",
        "#############\n",
        "# Train data\n",
        "#############\n",
        "train_data = train_data.shuffle(buffer_size=TRAIN_SHUFFLE_BUFFER_SIZE)\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "train_data = train_data.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"train_data\",train_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data <PrefetchDataset shapes: ((None, 66), (None, 66), (None, 66)), types: (tf.int64, tf.int64, tf.float64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jocnJ4pzNm2V"
      },
      "source": [
        "### **Mini BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOWFUkAFNn7T"
      },
      "source": [
        "#### **Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_z_izpyNqdA"
      },
      "source": [
        "def generate_positional_encoding(max_length, model_size):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / model_size) for j in range(model_size)]\n",
        "            if pos != 0\n",
        "            else np.zeros(model_size)\n",
        "            for pos in range(max_length)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WX80B8kNr3f"
      },
      "source": [
        "#### **Transformer Encoder Block: Break Out Room  Activity🎊🎉**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryzR5hFWryLn"
      },
      "source": [
        "In this activity you will build out a Transfomer Encoder block from scratch. We will use this as the building block for our mini BERT model when you finish.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/transformer_encoder_block.svg\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOmV4hc9NttR"
      },
      "source": [
        "def transfomer_encoder_block(query, key, value, embedding_dim, num_heads, ff_dim):\n",
        "\n",
        "  # 1) Add a keras.layers.MultiHeadAttention Layer\n",
        "  # Set num_heads\n",
        "  # Set key_dim = embedding_dim // num_heads\n",
        "  # Pass in the (query, key, value) to this layer to get the attention_output\n",
        "  attention_output = ...\n",
        "\n",
        "  # 2) Add Dropout(0.1) to the attention_output\n",
        "  attention_output = ...\n",
        "\n",
        "  # 3) Normalization + Residual Connection\n",
        "  # Add a keras.layers.LayerNormalization with epsilon=1e-6\n",
        "  # Pass in the query and attention_output to the LayerNormalization\n",
        "  attention_output = ...\n",
        "\n",
        "  # 4) Feedforward Layer\n",
        "  # Add a Dense layer with size ff_dim and activation=\"relu\"\n",
        "  # Pass in attention_output to get the ffn_output\n",
        "  # Add another Dense layer with size of embedding_dim and pass in the ffn_output\n",
        "  ffn_output = ...\n",
        "\n",
        "  # 5) Add Dropout(0.1) to the ffn_output\n",
        "  ffn_output = ...\n",
        "\n",
        "  # 6) Normalization + Residual Connection\n",
        "  # Add a keras.layers.LayerNormalization with epsilon=1e-6\n",
        "  # Pass in the attention_output and ffn_output to the LayerNormalization to get the final sequence_output\n",
        "  sequence_output = ...\n",
        "\n",
        "\n",
        "  return sequence_output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvo124IepUXA"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-fg7Co6pjKt"
      },
      "source": [
        "###### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq7mNyf8pWr2"
      },
      "source": [
        "```\n",
        "def transfomer_encoder_block(query, key, value, embedding_dim, num_heads, ff_dim):\n",
        "    # MultiHeadAttention Layer\n",
        "    attention_output = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                       key_dim=embedding_dim // num_heads)(query, key, value)\n",
        "    # Normalization + Residual Connection\n",
        "    attention_output = keras.layers.Dropout(0.1)(attention_output)\n",
        "    attention_output = keras.layers.LayerNormalization(epsilon=1e-6)(query + attention_output)\n",
        "\n",
        "    # Feedforward Layer\n",
        "    ffn_output = keras.layers.Dense(units=ff_dim, activation=\"relu\")(attention_output)\n",
        "    ffn_output = keras.layers.Dense(units=embedding_dim)(ffn_output)\n",
        "    ffn_output = keras.layers.Dropout(0.1)(ffn_output)\n",
        "\n",
        "    # Normalization + Residual Connection \n",
        "    sequence_output = keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8f3z6DONvWe"
      },
      "source": [
        "#### **Masked Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98LwWdAwNxFk"
      },
      "source": [
        "# Loss Tracker\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "class MaskedLanguageModel(keras.Model):\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "      if len(inputs) == 3:\n",
        "          features, labels, sample_weight = inputs\n",
        "      else:\n",
        "          features, labels = inputs\n",
        "          sample_weight = None\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(features, training=True)\n",
        "          loss = loss_fn(labels, predictions[0], sample_weight=sample_weight)\n",
        "\n",
        "      # Compute gradients\n",
        "      trainable_vars = self.trainable_variables\n",
        "      gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "      # Update weights\n",
        "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "      # Compute our own metrics\n",
        "      loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "      # Return a dict mapping metric names to current value\n",
        "      return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "      return [loss_tracker]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbmZXKb5N12K"
      },
      "source": [
        "def create_masked_language_bert_model(vocab_size,embedding_dim, sequence_length,num_layers, num_heads,ff_dim, enable_pe=True):\n",
        "  # Model input\n",
        "  model_input = keras.layers.Input(shape=(sequence_length), dtype=tf.int64)\n",
        "\n",
        "  embeddings = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(model_input)\n",
        "  if enable_pe:\n",
        "    embeddings = embeddings + generate_positional_encoding(sequence_length,embedding_dim)\n",
        "  \n",
        "  encoder_output = embeddings\n",
        "  for i in range(num_layers):\n",
        "      encoder_output = transfomer_encoder_block(encoder_output, encoder_output, encoder_output, embedding_dim,num_heads, ff_dim)\n",
        "\n",
        "  # Output Layer\n",
        "  output = keras.layers.Dense(units=vocab_size, activation=\"softmax\")(encoder_output)\n",
        "\n",
        "  # Create Model\n",
        "  model = MaskedLanguageModel(inputs=[model_input], outputs=[output,encoder_output], name=\"masked_bert_model\")\n",
        "\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZqN_mNFOAFw"
      },
      "source": [
        "### **Train with Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFNvMotbOAlH",
        "outputId": "91a2fcd2-fb76-4dfc-b145-c0cd787eea10"
      },
      "source": [
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "embedding_dim = 256\n",
        "sequence_length = max_len\n",
        "num_layers = 2\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model_w_pe = create_masked_language_bert_model(vocabulary_size,embedding_dim, sequence_length,num_layers,num_heads,ff_dim, enable_pe=True)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model_w_pe.summary())\n",
        "\n",
        "# Optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Compile\n",
        "model_w_pe.compile(optimizer=optimizer)\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "training_results = model_w_pe.fit(\n",
        "        train_data,\n",
        "        epochs=epochs,\n",
        "        verbose=1)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 66)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 66, 256)      289280      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 66, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention (MultiHead (None, 66, 256)      263168      tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 66, 256)      0           multi_head_attention[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 66, 256)      0           tf.__operators__.add[0][0]       \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, 66, 256)      512         tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 66, 256)      65792       layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 66, 256)      65792       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 66, 256)      0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_2 (TFOpLam (None, 66, 256)      0           layer_normalization[0][0]        \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, 66, 256)      512         tf.__operators__.add_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_1 (MultiHe (None, 66, 256)      263168      layer_normalization_1[0][0]      \n",
            "                                                                 layer_normalization_1[0][0]      \n",
            "                                                                 layer_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 66, 256)      0           multi_head_attention_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_3 (TFOpLam (None, 66, 256)      0           layer_normalization_1[0][0]      \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_2 (LayerNor (None, 66, 256)      512         tf.__operators__.add_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 66, 256)      65792       layer_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 66, 256)      65792       dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 66, 256)      0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_4 (TFOpLam (None, 66, 256)      0           layer_normalization_2[0][0]      \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, 66, 256)      512         tf.__operators__.add_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 66, 1130)     290410      layer_normalization_3[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 1,371,242\n",
            "Trainable params: 1,371,242\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - 127s 870ms/step - loss: 5.7937\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 125s 874ms/step - loss: 4.6416\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 126s 883ms/step - loss: 3.5153\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 125s 873ms/step - loss: 2.5483\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 125s 874ms/step - loss: 1.6850\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 125s 874ms/step - loss: 1.0168\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 125s 877ms/step - loss: 0.6326\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 125s 873ms/step - loss: 0.4149\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 125s 874ms/step - loss: 0.2834\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 126s 880ms/step - loss: 0.2040\n",
            "Training execution time (mins) 20.910524022579192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUYEitnDOMO3"
      },
      "source": [
        "### **Evaluate**\n",
        "\n",
        "Let us look at what exaclty is the language model predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugk3-k8SOMr3",
        "outputId": "685d007a-72d1-4918-b57a-b5e502f15d50"
      },
      "source": [
        "sample_tokens = text_vectorizer([\"convolutional [mask] networks have been applied to fields including computer vision\"])\n",
        "sample_tokens = sample_tokens.numpy()\n",
        "print(sample_tokens)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 244 1129   17   44  254  170    5  380  370   40   54    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-H7xZOuOQx0"
      },
      "source": [
        "# Make a prediction\n",
        "predictions = model_w_pe.predict(sample_tokens)\n",
        "\n",
        "# Get the first output from the model\n",
        "mask_predictions = predictions[0]\n",
        "encoder_output = predictions[1]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hntwaau3OS5J",
        "outputId": "1ae6799d-67d3-4df1-c4cd-1d1e3f682003"
      },
      "source": [
        "masked_index = np.where(sample_tokens == mask_token_id)\n",
        "masked_index = masked_index[1]\n",
        "mask_prediction = mask_predictions[0][masked_index]\n",
        "print(\"mask_prediction:\", mask_prediction)\n",
        "\n",
        "top_indices = mask_prediction[0].argsort()[-5 :][::-1]\n",
        "values = mask_prediction[0][top_indices]\n",
        "\n",
        "def decode(tokens):\n",
        "  return \" \".join([index_word[t] for t in tokens if t != 0])\n",
        "\n",
        "for i in range(len(top_indices)):\n",
        "  p = top_indices[i]\n",
        "  v = values[i]\n",
        "  tokens = np.copy(sample_tokens[0])\n",
        "  tokens[masked_index[0]] = p\n",
        "  print(\"Prediction:\",decode(tokens),\"- Probability:\",v, \"- Mask token:\",index_word[p])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mask_prediction: [[4.02807254e-09 3.36504975e-08 5.23967714e-09 ... 2.05522133e-06\n",
            "  3.68732032e-08 1.27696325e-08]]\n",
            "Prediction: convolutional networks networks have been applied to fields including computer vision - Probability: 0.8604023 - Mask token: networks\n",
            "Prediction: convolutional were networks have been applied to fields including computer vision - Probability: 0.074843526 - Mask token: were\n",
            "Prediction: convolutional ai networks have been applied to fields including computer vision - Probability: 0.03147794 - Mask token: ai\n",
            "Prediction: convolutional or networks have been applied to fields including computer vision - Probability: 0.003195785 - Mask token: or\n",
            "Prediction: convolutional neural networks have been applied to fields including computer vision - Probability: 0.0031564995 - Mask token: neural\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZ7hwmtOc0y"
      },
      "source": [
        "### **Train with NO Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6qjotpeOdbA",
        "outputId": "e7bd60eb-56d5-4b26-cb18-28cfc1e38349"
      },
      "source": [
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "embedding_dim = 256\n",
        "sequence_length = max_len\n",
        "num_layers = 2\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model_no_pe = create_masked_language_bert_model(vocabulary_size,embedding_dim, sequence_length,num_layers,num_heads,ff_dim, enable_pe=False)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Compile\n",
        "model_no_pe.compile(optimizer=optimizer)\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "training_results = model_no_pe.fit(\n",
        "        train_data,\n",
        "        epochs=epochs,\n",
        "        verbose=1)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 128s 876ms/step - loss: 5.8659\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 125s 872ms/step - loss: 4.2099\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 124s 870ms/step - loss: 3.1755\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 125s 874ms/step - loss: 2.6924\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 125s 874ms/step - loss: 2.4065\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 125s 876ms/step - loss: 2.1758\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 125s 877ms/step - loss: 1.9950\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 125s 872ms/step - loss: 1.8559\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 125s 871ms/step - loss: 1.7486\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 125s 872ms/step - loss: 1.6759\n",
            "Training execution time (mins) 20.861590536435447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx7qJ2H4Ole3"
      },
      "source": [
        "### **Evaluate Postional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePkzRtR7Ol52",
        "outputId": "de171410-058c-4738-afbe-d8bf6821af7d"
      },
      "source": [
        "sample_tokens1 = text_vectorizer([\"pavlos taught positional encoding to shivas\"])\n",
        "sample_tokens1 = sample_tokens1.numpy()\n",
        "print(sample_tokens1)\n",
        "\n",
        "sample_tokens2 = text_vectorizer([\"shivas taught positional encoding to pavlos\"])\n",
        "sample_tokens2 = sample_tokens2.numpy()\n",
        "print(sample_tokens2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[686 516 671 913   5 569   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "[[569 516 671 913   5 686   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_IYXaolOqOf",
        "outputId": "e46f4454-4588-48c1-f91b-54485f49d40a"
      },
      "source": [
        "# Make a prediction on model with pe\n",
        "predictions1 = model_w_pe.predict(sample_tokens1)\n",
        "\n",
        "# Get the second output from the model\n",
        "encoder_output1 = predictions1[1]\n",
        "print(\"encoder_output shape:\",encoder_output1.shape)\n",
        "print(encoder_output1[:,:6,:10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_output shape: (1, 66, 256)\n",
            "[[[-0.5246513  -1.4148      2.4360695   0.18109246 -0.28529295\n",
            "    1.0969348  -0.6597898   1.3101103   1.8054535   0.38613904]\n",
            "  [-0.29248154 -1.6944356   0.13257365  1.2679522  -0.21880502\n",
            "   -0.6975474  -0.0870638  -0.6007647  -0.18499145  0.98396164]\n",
            "  [ 2.1933389  -0.68234843  0.9784966  -1.0520777   1.033069\n",
            "   -0.8253678   4.516056   -2.8515844   3.0040007  -0.0372692 ]\n",
            "  [ 0.35252237 -1.2266605   1.2465748  -4.022191   -0.202049\n",
            "   -0.7520537   3.4767342  -2.084076    1.8309388  -0.7517202 ]\n",
            "  [ 0.08036129  1.8232131   1.4715368  -3.314671    0.42360392\n",
            "    0.37867984 -0.3242465  -1.6824111   0.00495543  0.45591003]\n",
            "  [-0.46004915  1.9646914   0.640684   -1.0743353   1.2014388\n",
            "    0.12545663 -0.40111935 -1.2040968   0.21612233  2.2937963 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhDdxqDRTXk0",
        "outputId": "7ed1eb25-998e-48ec-c47c-663cad9b9e8b"
      },
      "source": [
        "# Make a prediction on model with pe\n",
        "predictions2 = model_w_pe.predict(sample_tokens2)\n",
        "\n",
        "# Get the second output from the model\n",
        "encoder_output2 = predictions2[1]\n",
        "print(\"encoder_output shape:\",encoder_output2.shape)\n",
        "print(encoder_output2[:,:6,:10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_output shape: (1, 66, 256)\n",
            "[[[-0.56282425 -0.37398362  0.9143508  -0.99942887  0.17094693\n",
            "    1.44945    -0.42807174  3.1179116   1.7868702   1.5419998 ]\n",
            "  [-2.8787968  -1.5273198  -1.8377143  -0.39979696  0.89370733\n",
            "    1.6472256  -0.3572682   0.64689994  0.5910641   2.3080583 ]\n",
            "  [ 4.5261016  -1.7432227  -0.40208167 -3.412392    3.6799495\n",
            "   -1.1391723   2.5594602  -1.3695285   3.4517162   0.08981693]\n",
            "  [ 0.8687216  -4.3430266   0.8198559  -5.2271276   0.33220887\n",
            "   -3.233263    3.3498228  -2.5516894   2.8687356  -1.9730374 ]\n",
            "  [-3.3114238   0.23121396 -1.0878549  -4.64416     0.1165459\n",
            "   -0.670736   -1.2975407  -3.1562493   0.41120046 -0.94076943]\n",
            "  [-5.374191    0.30782017 -4.3353863  -1.8440819  -0.01870556\n",
            "   -1.7275656  -2.4623075  -1.8943458  -1.1614938  -0.62775993]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fIJXC9zTqCF",
        "outputId": "9184b702-99da-4712-8ad8-abaa68b502ca"
      },
      "source": [
        "# Make a prediction on model without pe\n",
        "predictions3 = model_no_pe.predict(sample_tokens1)\n",
        "\n",
        "# Get the second output from the model\n",
        "encoder_output3 = predictions3[1]\n",
        "print(\"encoder_output shape:\",encoder_output3.shape)\n",
        "print(encoder_output3[:,:6,:10])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_output shape: (1, 66, 256)\n",
            "[[[-0.03209928  0.68168175 -1.5825438  -2.5832224   0.4955348\n",
            "    0.48680615  0.16487107 -0.19007911  1.3275813  -0.30035347]\n",
            "  [-0.11713668  0.42059946 -2.520322   -1.1835684   0.195108\n",
            "    0.24005482 -1.4002932  -1.4046297  -0.23848444  0.3837928 ]\n",
            "  [ 0.5003081   0.2492835  -0.8094313  -1.5947437   2.4495676\n",
            "    0.6930185   0.9870634  -1.2392002  -1.4726596   1.7183204 ]\n",
            "  [-0.5274048  -2.1464496  -1.8480319  -1.4731182   0.1126432\n",
            "   -1.5234904  -1.5618027   0.36495173  0.04311258  1.7619013 ]\n",
            "  [-0.4698931   1.3999718  -2.93414    -2.4048123   0.18449478\n",
            "    0.85698533 -1.4570557  -0.67249787  0.9933041   0.57962334]\n",
            "  [ 0.59668267  0.25934488 -1.1342949  -2.2415469   0.7648588\n",
            "    0.5091696  -1.1315584   0.01400721 -0.528243   -0.3171308 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbzW0VY6T08T",
        "outputId": "6102916c-6766-41b4-8e3c-514238bc5b6a"
      },
      "source": [
        "# Make a prediction on model without pe\n",
        "predictions4 = model_no_pe.predict(sample_tokens2)\n",
        "\n",
        "# Get the second output from the model\n",
        "encoder_output4 = predictions4[1]\n",
        "print(\"encoder_output shape:\",encoder_output4.shape)\n",
        "print(encoder_output4[:,:6,:10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_output shape: (1, 66, 256)\n",
            "[[[ 0.59668255  0.25934428 -1.1342949  -2.2415476   0.76485896\n",
            "    0.50917    -1.1315589   0.01400714 -0.5282433  -0.3171306 ]\n",
            "  [-0.11713661  0.42059946 -2.520322   -1.1835688   0.19510822\n",
            "    0.24005441 -1.400293   -1.4046291  -0.23848428  0.38379204]\n",
            "  [ 0.50030816  0.24928322 -0.8094312  -1.5947442   2.4495683\n",
            "    0.6930185   0.9870634  -1.2392     -1.4726596   1.7183204 ]\n",
            "  [-0.5274049  -2.1464505  -1.8480324  -1.4731193   0.11264288\n",
            "   -1.5234902  -1.5618031   0.36495188  0.04311167  1.7619015 ]\n",
            "  [-0.46989337  1.3999724  -2.9341397  -2.4048123   0.18449481\n",
            "    0.8569854  -1.4570559  -0.6724977   0.9933045   0.57962257]\n",
            "  [-0.03209894  0.6816822  -1.5825441  -2.5832229   0.4955349\n",
            "    0.486806    0.16487104 -0.1900789   1.3275807  -0.3003537 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W0FUJMgVP3k"
      },
      "source": [
        "Check if the word `pavlos` & `shivas` have the same represenation in the two sentences:\n",
        "\n",
        "* pavlos taught positional encoding to shivas\n",
        "* shivas taught positional encoding to pavlos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-xvWywwURdi",
        "outputId": "9ad0ae0b-1353-4897-aff0-d84a21cbd94b"
      },
      "source": [
        "print(\"Model with Postional Encoding:\")\n",
        "print(\"pavlos:\",np.array_equal(encoder_output1[0,1], encoder_output2[0,6]))\n",
        "print(\"shivas:\",np.array_equal(encoder_output1[0,6], encoder_output2[0,1]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model with Postional Encoding:\n",
            "pavlos: False\n",
            "shivas: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fxu-EkyW35Q",
        "outputId": "f34e9b76-4a51-4a6d-eadf-035110bfa1cc"
      },
      "source": [
        "print(\"Model without Postional Encoding:\")\n",
        "print(\"pavlos:\",np.array_equal(encoder_output3[0,1], encoder_output4[0,6]))\n",
        "print(\"shivas:\",np.array_equal(encoder_output3[0,6], encoder_output4[0,1]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model without Postional Encoding:\n",
            "pavlos: False\n",
            "shivas: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R0BT3S0XIOf",
        "outputId": "591ed769-d8a6-416c-ac53-c975398f0024"
      },
      "source": [
        "# Generate postional encodings\n",
        "positional_encodings = generate_positional_encoding(sequence_length, embedding_dim)\n",
        "print(positional_encodings.shape)\n",
        "print(positional_encodings)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(66, 256)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 8.41470985e-01  5.40302306e-01  8.01961795e-01 ...  9.99999993e-01\n",
            "   1.07460783e-04  9.99999994e-01]\n",
            " [ 9.09297427e-01 -4.16146837e-01  9.58144376e-01 ...  9.99999973e-01\n",
            "   2.14921564e-04  9.99999977e-01]\n",
            " ...\n",
            " [ 1.67355700e-01  9.85896582e-01  8.74411641e-01 ...  9.99973536e-01\n",
            "   6.76997760e-03  9.99977083e-01]\n",
            " [ 9.20026038e-01  3.91857230e-01  1.33252272e-01 ...  9.99972690e-01\n",
            "   6.87743588e-03  9.99976350e-01]\n",
            " [ 8.26828679e-01 -5.62453851e-01 -7.15208402e-01 ...  9.99971829e-01\n",
            "   6.98489409e-03  9.99975605e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAv4dZ6wXiW4",
        "outputId": "27c7286d-1cd8-467e-fe9f-7e01c7d807e6"
      },
      "source": [
        "# Lets compute the dot product of postion 1 with every other postions\n",
        "dot_results = []\n",
        "for idx in range(positional_encodings.shape[0]):\n",
        "  dot_results.append(np.dot(positional_encodings[1], positional_encodings[idx]))\n",
        "\n",
        "print(dot_results)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 128.0, 124.43234098476238, 115.50458977748838, 105.37530500340779, 97.94432545559212, 94.64087821013749, 94.121753245165, 93.8502393543588, 92.1603436710688, 89.23354145966026, 86.45969701475562, 84.97779496123833, 84.71380122208555, 84.62470321154319, 83.75035476597505, 82.04664798383486, 80.29800792898595, 79.29890211031581, 79.11922964091144, 79.10703240790198, 78.55346942361555, 77.3515771121203, 76.04380078512345, 75.26274752602393, 75.12855087097213, 75.16680406977767, 74.79025407777927, 73.85903606549182, 72.78840446272093, 72.12573743447207, 72.02189567898664, 72.10231898291606, 71.84393109501454, 71.0803987241784, 70.14978541628656, 69.55469547587421, 69.47361432779485, 69.59533514834825, 69.42819412338986, 68.77776564229198, 67.93050868710183, 67.37044256968206, 67.30742171219089, 67.47434152196888, 67.38786488057734, 66.81870283497449, 66.01541960808856, 65.46408376081149, 65.41511922156263, 65.63574154519917, 65.63109818358849, 65.12448770529708, 64.33240220944225, 63.762485087642034, 63.7220284740318, 64.01052961673213, 64.10148294253764, 63.6493942606079, 62.836018436763126, 62.21157016575874, 62.1680229535507, 62.54583400890072, 62.76477710843426, 62.37416611808748, 61.503670377845246]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "Z3m33-MzX4SH",
        "outputId": "89d36b0a-1bfb-4efb-b77e-0ed07842eb56"
      },
      "source": [
        "# Plot the dot product results\n",
        "fig = plt.figure(figsize=(20,5))\n",
        "axs = fig.add_subplot(1,3,1)\n",
        "axs.plot(np.arange(0, positional_encodings.shape[0]), dot_results)\n",
        "axs.set_xlabel('Positions')\n",
        "axs.set_ylabel('Dot product of positions')\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Robr8-OfOwaF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqRparV76iQ"
      },
      "source": [
        "## **References** <div id='references'>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H26UyCa977Q3"
      },
      "source": [
        "#### **Lecture Notes**\n",
        "\n",
        "- [AC295: Advanced Practical Data Science - Attention and Transformers](https://harvard-iacs.github.io/2020F-AC295/lectures/lecture8/presentation/lecture8.pdf)\n",
        "- [CS224n: Natural Language Processing with Deep Learning - Self-Attention For Generative Models](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjfHOix18UeB"
      },
      "source": [
        "#### **Research Papers**\n",
        "\n",
        "- [Sequence To Sequence Learning](https://arxiv.org/abs/1409.3215)\n",
        "- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "- [Luong Attention](https://arxiv.org/abs/1508.04025)\n",
        "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_jAORIT8IRH"
      },
      "source": [
        "#### **Code**\n",
        "\n",
        "- [Neural Machine Translation (seq2seq) Tutorial](https://github.com/tensorflow/nmt)\n",
        "- [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n",
        "- [Text classification from scratch](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
        "- [Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
        "- [End-to-end Masked Language Modeling with BERT](https://keras.io/examples/nlp/masked_language_modeling/)\n",
        "- [Keras MultiHeadAttention Layer](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/multi_head_attention.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY2rGkDk8NWJ"
      },
      "source": [
        "#### **Articles**\n",
        "\n",
        "- [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "- [Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)\n",
        "- [Visualizing A Neural Machine Translation Model](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [Attention: Sequence 2 Sequence model with Attention Mechanism](https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a)"
      ]
    }
  ]
}