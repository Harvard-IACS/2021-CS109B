<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta
            name="description"
            content="Spring 2021 - Harvard University, Institute for Applied Computational Science. ??"
    />
    <meta name="author" content="Pavlos Protopapas"/>
    <meta
            name="keywords"
            content="??"
    />

    <!-- Bootstrap CSS -->
    <link
            rel="stylesheet"
            href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css"
            integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB"
            crossorigin="anonymous"
    />

    <link
            rel="stylesheet"
            href="https://use.fontawesome.com/releases/v5.2.0/css/all.css"
            integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ"
            crossorigin="anonymous"
    />

    <link
            href="https://fonts.googleapis.com/css?family=Roboto+Condensed|Roboto:300,400,700"
            rel="stylesheet"
    />

    <link
            href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css"
    />

    <link
            rel="stylesheet"
            href="../../../style/tipuesearch/tipuesearch.css"
    />

    <link
            rel="shortcut icon"
            type="image/x-icon"
            href="../../../style/images/favicon.ico"
    />
    <link rel="stylesheet" href="../../../style/css/iacs.css"/>

    <title>Harvard CS109B | ??</title>

    <style>
        .navbar {
            background-color: #c90016
        }
    </style>
</head>
<body>
<nav class="navbar navbar-dark navbar-expand-md">
    <div class="container">
        <a class="navbar-brand" href="../../..">
            <img
                    class="navbar-brand-logo"
                    src="../../../style/images/logo.png"
            />
            <h3 class="course-title">CS109B</h3>
        </a>
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarsDefault"
                aria-controls="navbarsDefault"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarsDefault">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="../../../pages/syllabus.html">Syllabus</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="../../../pages/calendar.html">Calendar</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="../../../pages/schedule.html">Schedule</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="../../../pages/materials.html">Materials</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="../../../pages/projects.html">Projects</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="../../../pages/faq.html">FAQ</a>
                </li>
                <form
                        class="form-inline my-2"
                        action="../../../search.html"
                        onsubmit="return validateForm(this.elements['q'].value);"
                >
                    <div class="input-group input-group-sm">
                        <input
                                class="form-control"
                                type="text"
                                name="q"
                                placeholder="Search Topic"
                        />
                        <div class="input-group-append">
                            <button class="btn btn-default" type="submit">
                                <i class="fas fa-search"></i>
                            </button>
                        </div>
                    </div>
                </form>
            </ul>
        </div>
        <!-- .collapse navbar-collapse -->
    </div>
    <!-- .container -->
</nav>
<main id="content" class="container">
    <div>
        <div class="float-left">
            <p>
                Key Word(s): <a href="../../../pages/materials.html#??"
            >??</a
            ></p>
        </div>
        <div class="float-right">
            <a href="../../../sections/sec03/cs109b_sec3.ipynb">
                Download Notebook <i class="fas fa-download"></i>
            </a>
        </div>
    </div>
    <br/>
    <hr/>
    <style type="text/css">/*!
*
* IPython notebook
*
*/
    /* CSS font colors for translated ANSI escape sequences */
    /* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
    .ansi-black-fg {
        color: #3E424D;
    }

    .ansi-black-bg {
        background-color: #3E424D;
    }

    .ansi-black-intense-fg {
        color: #282C36;
    }

    .ansi-black-intense-bg {
        background-color: #282C36;
    }

    .ansi-red-fg {
        color: #E75C58;
    }

    .ansi-red-bg {
        background-color: #E75C58;
    }

    .ansi-red-intense-fg {
        color: #B22B31;
    }

    .ansi-red-intense-bg {
        background-color: #B22B31;
    }

    .ansi-green-fg {
        color: #00A250;
    }

    .ansi-green-bg {
        background-color: #00A250;
    }

    .ansi-green-intense-fg {
        color: #007427;
    }

    .ansi-green-intense-bg {
        background-color: #007427;
    }

    .ansi-yellow-fg {
        color: #DDB62B;
    }

    .ansi-yellow-bg {
        background-color: #DDB62B;
    }

    .ansi-yellow-intense-fg {
        color: #B27D12;
    }

    .ansi-yellow-intense-bg {
        background-color: #B27D12;
    }

    .ansi-blue-fg {
        color: #208FFB;
    }

    .ansi-blue-bg {
        background-color: #208FFB;
    }

    .ansi-blue-intense-fg {
        color: #0065CA;
    }

    .ansi-blue-intense-bg {
        background-color: #0065CA;
    }

    .ansi-magenta-fg {
        color: #D160C4;
    }

    .ansi-magenta-bg {
        background-color: #D160C4;
    }

    .ansi-magenta-intense-fg {
        color: #A03196;
    }

    .ansi-magenta-intense-bg {
        background-color: #A03196;
    }

    .ansi-cyan-fg {
        color: #60C6C8;
    }

    .ansi-cyan-bg {
        background-color: #60C6C8;
    }

    .ansi-cyan-intense-fg {
        color: #258F8F;
    }

    .ansi-cyan-intense-bg {
        background-color: #258F8F;
    }

    .ansi-white-fg {
        color: #C5C1B4;
    }

    .ansi-white-bg {
        background-color: #C5C1B4;
    }

    .ansi-white-intense-fg {
        color: #A1A6B2;
    }

    .ansi-white-intense-bg {
        background-color: #A1A6B2;
    }

    .ansi-default-inverse-fg {
        color: #FFFFFF;
    }

    .ansi-default-inverse-bg {
        background-color: #000000;
    }

    .ansi-bold {
        font-weight: bold;
    }

    .ansi-underline {
        text-decoration: underline;
    }

    /* The following styles are deprecated an will be removed in a future version */
    .ansibold {
        font-weight: bold;
    }

    .ansi-inverse {
        outline: 0.5px dotted;
    }

    /* use dark versions for foreground, to improve visibility */
    .ansiblack {
        color: black;
    }

    .ansired {
        color: darkred;
    }

    .ansigreen {
        color: darkgreen;
    }

    .ansiyellow {
        color: #c4a000;
    }

    .ansiblue {
        color: darkblue;
    }

    .ansipurple {
        color: darkviolet;
    }

    .ansicyan {
        color: steelblue;
    }

    .ansigray {
        color: gray;
    }

    /* and light for background, for the same reason */
    .ansibgblack {
        background-color: black;
    }

    .ansibgred {
        background-color: red;
    }

    .ansibggreen {
        background-color: green;
    }

    .ansibgyellow {
        background-color: yellow;
    }

    .ansibgblue {
        background-color: blue;
    }

    .ansibgpurple {
        background-color: magenta;
    }

    .ansibgcyan {
        background-color: cyan;
    }

    .ansibggray {
        background-color: gray;
    }

    div.cell {
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: vertical;
        -moz-box-align: stretch;
        display: box;
        box-orient: vertical;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: column;
        align-items: stretch;
        border-radius: 2px;
        box-sizing: border-box;
        -moz-box-sizing: border-box;
        -webkit-box-sizing: border-box;
        border-width: 1px;
        border-style: solid;
        border-color: transparent;
        width: 100%;
        padding: 5px;
        /* This acts as a spacer between cells, that is outside the border */
        margin: 0px;
        outline: none;
        position: relative;
        overflow: visible;
    }

    div.cell:before {
        position: absolute;
        display: block;
        top: -1px;
        left: -1px;
        width: 5px;
        height: calc(100% + 2px);
        content: '';
        background: transparent;
    }

    div.cell.jupyter-soft-selected {
        border-left-color: #E3F2FD;
        border-left-width: 1px;
        padding-left: 5px;
        border-right-color: #E3F2FD;
        border-right-width: 1px;
        background: #E3F2FD;
    }

    @media print {
        div.cell.jupyter-soft-selected {
            border-color: transparent;
        }
    }

    div.cell.selected,
    div.cell.selected.jupyter-soft-selected {
        border-color: #ababab;
    }

    div.cell.selected:before,
    div.cell.selected.jupyter-soft-selected:before {
        position: absolute;
        display: block;
        top: -1px;
        left: -1px;
        width: 5px;
        height: calc(100% + 2px);
        content: '';
        background: #42A5F5;
    }

    @media print {
        div.cell.selected,
        div.cell.selected.jupyter-soft-selected {
            border-color: transparent;
        }
    }

    .edit_mode div.cell.selected {
        border-color: #66BB6A;
    }

    .edit_mode div.cell.selected:before {
        position: absolute;
        display: block;
        top: -1px;
        left: -1px;
        width: 5px;
        height: calc(100% + 2px);
        content: '';
        background: #66BB6A;
    }

    @media print {
        .edit_mode div.cell.selected {
            border-color: transparent;
        }
    }

    .prompt {
        /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
        min-width: 14ex;
        /* This padding is tuned to match the padding on the CodeMirror editor. */
        padding: 0.4em;
        margin: 0px;
        font-family: monospace;
        text-align: right;
        /* This has to match that of the the CodeMirror class line-height below */
        line-height: 1.21429em;
        /* Don't highlight prompt number selection */
        -webkit-touch-callout: none;
        -webkit-user-select: none;
        -khtml-user-select: none;
        -moz-user-select: none;
        -ms-user-select: none;
        user-select: none;
        /* Use default cursor */
        cursor: default;
    }

    @media (max-width: 540px) {
        .prompt {
            text-align: left;
        }
    }

    div.inner_cell {
        min-width: 0;
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: vertical;
        -moz-box-align: stretch;
        display: box;
        box-orient: vertical;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: column;
        align-items: stretch;
        /* Old browsers */
        -webkit-box-flex: 1;
        -moz-box-flex: 1;
        box-flex: 1;
        /* Modern browsers */
        flex: 1;
    }

    /* input_area and input_prompt must match in top border and margin for alignment */
    div.input_area {
        border: 1px solid #cfcfcf;
        border-radius: 2px;
        background: #f7f7f7;
        line-height: 1.21429em;
    }

    /* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
    div.prompt:empty {
        padding-top: 0;
        padding-bottom: 0;
    }

    div.unrecognized_cell {
        padding: 5px 5px 5px 0px;
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: horizontal;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: horizontal;
        -moz-box-align: stretch;
        display: box;
        box-orient: horizontal;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: row;
        align-items: stretch;
    }

    div.unrecognized_cell .inner_cell {
        border-radius: 2px;
        padding: 5px;
        font-weight: bold;
        color: red;
        border: 1px solid #cfcfcf;
        background: #eaeaea;
    }

    div.unrecognized_cell .inner_cell a {
        color: inherit;
        text-decoration: none;
    }

    div.unrecognized_cell .inner_cell a:hover {
        color: inherit;
        text-decoration: none;
    }

    @media (max-width: 540px) {
        div.unrecognized_cell > div.prompt {
            display: none;
        }
    }

    div.code_cell {
        /* avoid page breaking on code cells when printing */
    }

    @media print {
        div.code_cell {
            page-break-inside: avoid;
        }
    }

    /* any special styling for code cells that are currently running goes here */
    div.input {
        page-break-inside: avoid;
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: horizontal;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: horizontal;
        -moz-box-align: stretch;
        display: box;
        box-orient: horizontal;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: row;
        align-items: stretch;
    }

    @media (max-width: 540px) {
        div.input {
            /* Old browsers */
            display: -webkit-box;
            -webkit-box-orient: vertical;
            -webkit-box-align: stretch;
            display: -moz-box;
            -moz-box-orient: vertical;
            -moz-box-align: stretch;
            display: box;
            box-orient: vertical;
            box-align: stretch;
            /* Modern browsers */
            display: flex;
            flex-direction: column;
            align-items: stretch;
        }
    }

    /* input_area and input_prompt must match in top border and margin for alignment */
    div.input_prompt {
        color: #303F9F;
        border-top: 1px solid transparent;
    }

    div.input_area > div.highlight {
        margin: 0.4em;
        border: none;
        padding: 0px;
        background-color: transparent;
    }

    div.input_area > div.highlight > pre {
        margin: 0px;
        border: none;
        padding: 0px;
        background-color: transparent;
    }

    /* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
    .CodeMirror {
        line-height: 1.21429em;
        /* Changed from 1em to our global default */
        font-size: 14px;
        height: auto;
        /* Changed to auto to autogrow */
        background: none;
        /* Changed from white to allow our bg to show through */
    }

    .CodeMirror-scroll {
        /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
        /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
        overflow-y: hidden;
        overflow-x: auto;
    }

    .CodeMirror-lines {
        /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
        /* we have set a different line-height and want this to scale with that. */
        /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
        padding: 0.4em 0;
    }

    .CodeMirror-linenumber {
        padding: 0 8px 0 4px;
    }

    .CodeMirror-gutters {
        border-bottom-left-radius: 2px;
        border-top-left-radius: 2px;
    }

    .CodeMirror pre {
        /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
        padding: 0 0.4em;
        border: 0;
        border-radius: 0;
    }

    .CodeMirror-cursor {
        border-left: 1.4px solid black;
    }

    @media screen and (min-width: 2138px) and (max-width: 4319px) {
        .CodeMirror-cursor {
            border-left: 2px solid black;
        }
    }

    @media screen and (min-width: 4320px) {
        .CodeMirror-cursor {
            border-left: 4px solid black;
        }
    }

    /*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
    .highlight-base {
        color: #000;
    }

    .highlight-variable {
        color: #000;
    }

    .highlight-variable-2 {
        color: #1a1a1a;
    }

    .highlight-variable-3 {
        color: #333333;
    }

    .highlight-string {
        color: #BA2121;
    }

    .highlight-comment {
        color: #408080;
        font-style: italic;
    }

    .highlight-number {
        color: #080;
    }

    .highlight-atom {
        color: #88F;
    }

    .highlight-keyword {
        color: #008000;
        font-weight: bold;
    }

    .highlight-builtin {
        color: #008000;
    }

    .highlight-error {
        color: #f00;
    }

    .highlight-operator {
        color: #AA22FF;
        font-weight: bold;
    }

    .highlight-meta {
        color: #AA22FF;
    }

    /* previously not defined, copying from default codemirror */
    .highlight-def {
        color: #00f;
    }

    .highlight-string-2 {
        color: #f50;
    }

    .highlight-qualifier {
        color: #555;
    }

    .highlight-bracket {
        color: #997;
    }

    .highlight-tag {
        color: #170;
    }

    .highlight-attribute {
        color: #00c;
    }

    .highlight-header {
        color: blue;
    }

    .highlight-quote {
        color: #090;
    }

    .highlight-link {
        color: #00c;
    }

    /* apply the same style to codemirror */
    .cm-s-ipython span.cm-keyword {
        color: #008000;
        font-weight: bold;
    }

    .cm-s-ipython span.cm-atom {
        color: #88F;
    }

    .cm-s-ipython span.cm-number {
        color: #080;
    }

    .cm-s-ipython span.cm-def {
        color: #00f;
    }

    .cm-s-ipython span.cm-variable {
        color: #000;
    }

    .cm-s-ipython span.cm-operator {
        color: #AA22FF;
        font-weight: bold;
    }

    .cm-s-ipython span.cm-variable-2 {
        color: #1a1a1a;
    }

    .cm-s-ipython span.cm-variable-3 {
        color: #333333;
    }

    .cm-s-ipython span.cm-comment {
        color: #408080;
        font-style: italic;
    }

    .cm-s-ipython span.cm-string {
        color: #BA2121;
    }

    .cm-s-ipython span.cm-string-2 {
        color: #f50;
    }

    .cm-s-ipython span.cm-meta {
        color: #AA22FF;
    }

    .cm-s-ipython span.cm-qualifier {
        color: #555;
    }

    .cm-s-ipython span.cm-builtin {
        color: #008000;
    }

    .cm-s-ipython span.cm-bracket {
        color: #997;
    }

    .cm-s-ipython span.cm-tag {
        color: #170;
    }

    .cm-s-ipython span.cm-attribute {
        color: #00c;
    }

    .cm-s-ipython span.cm-header {
        color: blue;
    }

    .cm-s-ipython span.cm-quote {
        color: #090;
    }

    .cm-s-ipython span.cm-link {
        color: #00c;
    }

    .cm-s-ipython span.cm-error {
        color: #f00;
    }

    .cm-s-ipython span.cm-tab {
        background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
        background-position: right;
        background-repeat: no-repeat;
    }

    div.output_wrapper {
        /* this position must be relative to enable descendents to be absolute within it */
        position: relative;
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: vertical;
        -moz-box-align: stretch;
        display: box;
        box-orient: vertical;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: column;
        align-items: stretch;
        z-index: 1;
    }

    /* class for the output area when it should be height-limited */
    div.output_scroll {
        /* ideally, this would be max-height, but FF barfs all over that */
        height: 24em;
        /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
        width: 100%;
        overflow: auto;
        border-radius: 2px;
        -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
        box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
        display: block;
    }

    /* output div while it is collapsed */
    div.output_collapsed {
        margin: 0px;
        padding: 0px;
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: vertical;
        -moz-box-align: stretch;
        display: box;
        box-orient: vertical;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: column;
        align-items: stretch;
    }

    div.out_prompt_overlay {
        height: 100%;
        padding: 0px 0.4em;
        position: absolute;
        border-radius: 2px;
    }

    div.out_prompt_overlay:hover {
        /* use inner shadow to get border that is computed the same on WebKit/FF */
        -webkit-box-shadow: inset 0 0 1px #000;
        box-shadow: inset 0 0 1px #000;
        background: rgba(240, 240, 240, 0.5);
    }

    div.output_prompt {
        color: #D84315;
    }

    /* This class is the outer container of all output sections. */
    div.output_area {
        padding: 0px;
        page-break-inside: avoid;
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: horizontal;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: horizontal;
        -moz-box-align: stretch;
        display: box;
        box-orient: horizontal;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: row;
        align-items: stretch;
    }

    div.output_area .MathJax_Display {
        text-align: left !important;
    }

    div.output_area
    div.output_area
    div.output_area img,
    div.output_area svg {
        max-width: 100%;
        height: auto;
    }

    div.output_area img.unconfined,
    div.output_area svg.unconfined {
        max-width: none;
    }

    div.output_area .mglyph > img {
        max-width: none;
    }

    /* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
    .output {
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: vertical;
        -moz-box-align: stretch;
        display: box;
        box-orient: vertical;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: column;
        align-items: stretch;
    }

    @media (max-width: 540px) {
        div.output_area {
            /* Old browsers */
            display: -webkit-box;
            -webkit-box-orient: vertical;
            -webkit-box-align: stretch;
            display: -moz-box;
            -moz-box-orient: vertical;
            -moz-box-align: stretch;
            display: box;
            box-orient: vertical;
            box-align: stretch;
            /* Modern browsers */
            display: flex;
            flex-direction: column;
            align-items: stretch;
        }
    }

    div.output_area pre {
        margin: 0;
        padding: 1px 0 1px 0;
        border: 0;
        vertical-align: baseline;
        color: black;
        background-color: transparent;
        border-radius: 0;
    }

    /* This class is for the output subarea inside the output_area and after
   the prompt div. */
    div.output_subarea {
        overflow-x: auto;
        padding: 0.4em;
        /* Old browsers */
        -webkit-box-flex: 1;
        -moz-box-flex: 1;
        box-flex: 1;
        /* Modern browsers */
        flex: 1;
        max-width: calc(100% - 14ex);
    }

    div.output_scroll div.output_subarea {
        overflow-x: visible;
    }

    /* The rest of the output_* classes are for special styling of the different
   output types */
    /* all text output has this class: */
    div.output_text {
        text-align: left;
        color: #000;
        /* This has to match that of the the CodeMirror class line-height below */
        line-height: 1.21429em;
    }

    /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
    div.output_stderr {
        background: #fdd;
        /* very light red background for stderr */
    }

    div.output_latex {
        text-align: left;
    }

    /* Empty output_javascript divs should have no height */
    div.output_javascript:empty {
        padding: 0;
    }

    .js-error {
        color: darkred;
    }

    /* raw_input styles */
    div.raw_input_container {
        line-height: 1.21429em;
        padding-top: 5px;
    }

    pre.raw_input_prompt {
        /* nothing needed here. */
    }

    input.raw_input {
        font-family: monospace;
        font-size: inherit;
        color: inherit;
        width: auto;
        /* make sure input baseline aligns with prompt */
        vertical-align: baseline;
        /* padding + margin = 0.5em between prompt and cursor */
        padding: 0em 0.25em;
        margin: 0em 0.25em;
    }

    input.raw_input:focus {
        box-shadow: none;
    }

    p.p-space {
        margin-bottom: 10px;
    }

    div.output_unrecognized {
        padding: 5px;
        font-weight: bold;
        color: red;
    }

    div.output_unrecognized a {
        color: inherit;
        text-decoration: none;
    }

    div.output_unrecognized a:hover {
        color: inherit;
        text-decoration: none;
    }

    .rendered_html {
        color: #000;
        /* any extras will just be numbers: */
    }


    .rendered_html :link {
        text-decoration: underline;
    }

    .rendered_html :visited {
        text-decoration: underline;
    }


    .rendered_html h1:first-child {
        margin-top: 0.538em;
    }

    .rendered_html h2:first-child {
        margin-top: 0.636em;
    }

    .rendered_html h3:first-child {
        margin-top: 0.777em;
    }

    .rendered_html h4:first-child {
        margin-top: 1em;
    }

    .rendered_html h5:first-child {
        margin-top: 1em;
    }

    .rendered_html h6:first-child {
        margin-top: 1em;
    }

    .rendered_html ul:not(.list-inline),
    .rendered_html ol:not(.list-inline) {
        padding-left: 2em;
    }


    .rendered_html * + ul {
        margin-top: 1em;
    }

    .rendered_html * + ol {
        margin-top: 1em;
    }


    .rendered_html pre,
    .rendered_html tr,
    .rendered_html th,
    .rendered_html tbody tr:nth-child(odd) {
        background: #f5f5f5;
    }

    .rendered_html tbody tr:hover {
        background: rgba(66, 165, 245, 0.2);
    }

    .rendered_html * + table {
        margin-top: 1em;
    }

    .rendered_html * + p {
        margin-top: 1em;
    }

    .rendered_html * + img {
        margin-top: 1em;
    }

    .rendered_html img,
    .rendered_html img.unconfined,
    .rendered_html * + .alert {
        margin-top: 1em;
    }

    [dir="rtl"]
    div.text_cell {
        /* Old browsers */
        display: -webkit-box;
        -webkit-box-orient: horizontal;
        -webkit-box-align: stretch;
        display: -moz-box;
        -moz-box-orient: horizontal;
        -moz-box-align: stretch;
        display: box;
        box-orient: horizontal;
        box-align: stretch;
        /* Modern browsers */
        display: flex;
        flex-direction: row;
        align-items: stretch;
    }

    @media (max-width: 540px) {
        div.text_cell > div.prompt {
            display: none;
        }
    }

    div.text_cell_render {
        /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
        outline: none;
        resize: none;
        width: inherit;
        border-style: none;
        padding: 0.5em 0.5em 0.5em 0.4em;
        color: #000;
        box-sizing: border-box;
        -moz-box-sizing: border-box;
        -webkit-box-sizing: border-box;
    }

    a.anchor-link:link {
        text-decoration: none;
        padding: 0px 20px;
        visibility: hidden;
    }

    h1:hover .anchor-link,
    h2:hover .anchor-link,
    h3:hover .anchor-link,
    h4:hover .anchor-link,
    h5:hover .anchor-link,
    h6:hover .anchor-link {
        visibility: visible;
    }

    .text_cell.rendered .input_area {
        display: none;
    }

    .text_cell.rendered
    .text_cell.rendered .rendered_html tr,
    .text_cell.rendered .rendered_html th,
    .text_cell.rendered
    .text_cell.unrendered .text_cell_render {
        display: none;
    }

    .text_cell .dropzone .input_area {
        border: 2px dashed #bababa;
        margin: -1px;
    }

    .cm-header-1,
    .cm-header-2,
    .cm-header-3,
    .cm-header-4,
    .cm-header-5,
    .cm-header-6 {
        font-weight: bold;
        font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    }

    .cm-header-1 {
        font-size: 185.7%;
    }

    .cm-header-2 {
        font-size: 157.1%;
    }

    .cm-header-3 {
        font-size: 128.6%;
    }

    .cm-header-4 {
        font-size: 110%;
    }

    .cm-header-5 {
        font-size: 100%;
        font-style: italic;
    }

    .cm-header-6 {
        font-size: 100%;
        font-style: italic;
    }
    </style>
    <style type="text/css">.highlight .hll {
        background-color: #ffffcc
    }

    .highlight {
        background: #f8f8f8;
    }

    .highlight .c {
        color: #408080;
        font-style: italic
    }

    /* Comment */
    .highlight .err {
        border: 1px solid #FF0000
    }

    /* Error */
    .highlight .k {
        color: #008000;
        font-weight: bold
    }

    /* Keyword */
    .highlight .o {
        color: #666666
    }

    /* Operator */
    .highlight .ch {
        color: #408080;
        font-style: italic
    }

    /* Comment.Hashbang */
    .highlight .cm {
        color: #408080;
        font-style: italic
    }

    /* Comment.Multiline */
    .highlight .cp {
        color: #BC7A00
    }

    /* Comment.Preproc */
    .highlight .cpf {
        color: #408080;
        font-style: italic
    }

    /* Comment.PreprocFile */
    .highlight .c1 {
        color: #408080;
        font-style: italic
    }

    /* Comment.Single */
    .highlight .cs {
        color: #408080;
        font-style: italic
    }

    /* Comment.Special */
    .highlight .gd {
        color: #A00000
    }

    /* Generic.Deleted */
    .highlight .ge {
        font-style: italic
    }

    /* Generic.Emph */
    .highlight .gr {
        color: #FF0000
    }

    /* Generic.Error */
    .highlight .gh {
        color: #000080;
        font-weight: bold
    }

    /* Generic.Heading */
    .highlight .gi {
        color: #00A000
    }

    /* Generic.Inserted */
    .highlight .go {
        color: #888888
    }

    /* Generic.Output */
    .highlight .gp {
        color: #000080;
        font-weight: bold
    }

    /* Generic.Prompt */
    .highlight .gs {
        font-weight: bold
    }

    /* Generic.Strong */
    .highlight .gu {
        color: #800080;
        font-weight: bold
    }

    /* Generic.Subheading */
    .highlight .gt {
        color: #0044DD
    }

    /* Generic.Traceback */
    .highlight .kc {
        color: #008000;
        font-weight: bold
    }

    /* Keyword.Constant */
    .highlight .kd {
        color: #008000;
        font-weight: bold
    }

    /* Keyword.Declaration */
    .highlight .kn {
        color: #008000;
        font-weight: bold
    }

    /* Keyword.Namespace */
    .highlight .kp {
        color: #008000
    }

    /* Keyword.Pseudo */
    .highlight .kr {
        color: #008000;
        font-weight: bold
    }

    /* Keyword.Reserved */
    .highlight .kt {
        color: #B00040
    }

    /* Keyword.Type */
    .highlight .m {
        color: #666666
    }

    /* Literal.Number */
    .highlight .s {
        color: #BA2121
    }

    /* Literal.String */
    .highlight .na {
        color: #7D9029
    }

    /* Name.Attribute */
    .highlight .nb {
        color: #008000
    }

    /* Name.Builtin */
    .highlight .nc {
        color: #0000FF;
        font-weight: bold
    }

    /* Name.Class */
    .highlight .no {
        color: #880000
    }

    /* Name.Constant */
    .highlight .nd {
        color: #AA22FF
    }

    /* Name.Decorator */
    .highlight .ni {
        color: #999999;
        font-weight: bold
    }

    /* Name.Entity */
    .highlight .ne {
        color: #D2413A;
        font-weight: bold
    }

    /* Name.Exception */
    .highlight .nf {
        color: #0000FF
    }

    /* Name.Function */
    .highlight .nl {
        color: #A0A000
    }

    /* Name.Label */
    .highlight .nn {
        color: #0000FF;
        font-weight: bold
    }

    /* Name.Namespace */
    .highlight .nt {
        color: #008000;
        font-weight: bold
    }

    /* Name.Tag */
    .highlight .nv {
        color: #19177C
    }

    /* Name.Variable */
    .highlight .ow {
        color: #AA22FF;
        font-weight: bold
    }

    /* Operator.Word */
    .highlight .w {
        color: #bbbbbb
    }

    /* Text.Whitespace */
    .highlight .mb {
        color: #666666
    }

    /* Literal.Number.Bin */
    .highlight .mf {
        color: #666666
    }

    /* Literal.Number.Float */
    .highlight .mh {
        color: #666666
    }

    /* Literal.Number.Hex */
    .highlight .mi {
        color: #666666
    }

    /* Literal.Number.Integer */
    .highlight .mo {
        color: #666666
    }

    /* Literal.Number.Oct */
    .highlight .sa {
        color: #BA2121
    }

    /* Literal.String.Affix */
    .highlight .sb {
        color: #BA2121
    }

    /* Literal.String.Backtick */
    .highlight .sc {
        color: #BA2121
    }

    /* Literal.String.Char */
    .highlight .dl {
        color: #BA2121
    }

    /* Literal.String.Delimiter */
    .highlight .sd {
        color: #BA2121;
        font-style: italic
    }

    /* Literal.String.Doc */
    .highlight .s2 {
        color: #BA2121
    }

    /* Literal.String.Double */
    .highlight .se {
        color: #BB6622;
        font-weight: bold
    }

    /* Literal.String.Escape */
    .highlight .sh {
        color: #BA2121
    }

    /* Literal.String.Heredoc */
    .highlight .si {
        color: #BB6688;
        font-weight: bold
    }

    /* Literal.String.Interpol */
    .highlight .sx {
        color: #008000
    }

    /* Literal.String.Other */
    .highlight .sr {
        color: #BB6688
    }

    /* Literal.String.Regex */
    .highlight .s1 {
        color: #BA2121
    }

    /* Literal.String.Single */
    .highlight .ss {
        color: #19177C
    }

    /* Literal.String.Symbol */
    .highlight .bp {
        color: #008000
    }

    /* Name.Builtin.Pseudo */
    .highlight .fm {
        color: #0000FF
    }

    /* Name.Function.Magic */
    .highlight .vc {
        color: #19177C
    }

    /* Name.Variable.Class */
    .highlight .vg {
        color: #19177C
    }

    /* Name.Variable.Global */
    .highlight .vi {
        color: #19177C
    }

    /* Name.Variable.Instance */
    .highlight .vm {
        color: #19177C
    }

    /* Name.Variable.Magic */
    .highlight .il {
        color: #666666
    }

    /* Literal.Number.Integer.Long */</style>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <h1><img src="https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png"
                         style="float: left; padding-right: 10px; width: 45px"/> Data Science 2: Advanced Topics in Data
                    Science</h1>
                <h2 id="Section-3:-Recurrent-Neural-Networks">Section 3: Recurrent Neural Networks<a class="anchor-link"
                                                                                                     href="#Section-3:-Recurrent-Neural-Networks">¶</a>
                </h2>
                <p><strong>Harvard University</strong><br/>
                    <strong>Spring 2021</strong><br/>
                    <strong>Instructors</strong>: Mark Glickman, Pavlos Protopapas, and Chris Tanner <br/>
                    <strong>Authors</strong>: Chris Gumb and Eleni Kaxiras</p>
                <hr style="height:2pt"/>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [1]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1">## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span
                                class="n">HTML</span>
<span class="n">styles</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span
                                class="n">get</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css"</span><span
                                class="p">)</span><span class="o">.</span><span class="n">text</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">styles</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[1]:</div>
                    <div class="output_html rendered_html output_subarea output_execute_result">
                        <style>
                            blockquote {
                                background: #AEDE94;
                            }

                            h1 {
                                padding-top: 25px;
                                padding-bottom: 25px;
                                text-align: left;
                                padding-left: 10px;
                                background-color: #DDDDDD;
                                color: black;
                            }

                            h2 {
                                padding-top: 10px;
                                padding-bottom: 10px;
                                text-align: left;
                                padding-left: 5px;
                                background-color: #EEEEEE;
                                color: black;
                            }

                            div.exercise {
                                background-color: #ffcccc;
                                border-color: #E9967A;
                                border-left: 5px solid #800080;
                                padding: 0.5em;
                            }

                            div.discussion {
                                background-color: #ccffcc;
                                border-color: #88E97A;
                                border-left: 5px solid #0A8000;
                                padding: 0.5em;
                            }

                            div.theme {
                                background-color: #DDDDDD;
                                border-color: #E9967A;
                                border-left: 5px solid #800080;
                                padding: 0.5em;
                                font-size: 18pt;
                            }

                            div.gc {
                                background-color: #AEDE94;
                                border-color: #E9967A;
                                border-left: 5px solid #800080;
                                padding: 0.5em;
                                font-size: 12pt;
                            }

                            p.q1 {
                                padding-top: 5px;
                                padding-bottom: 5px;
                                text-align: left;
                                padding-left: 5px;
                                background-color: #EEEEEE;
                                color: black;
                            }

                            header {
                                padding-top: 35px;
                                padding-bottom: 35px;
                                text-align: left;
                                padding-left: 10px;
                                background-color: #DDDDDD;
                                color: black;
                            }
                        </style>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <h2 id="Learning-Objectives">Learning Objectives<a class="anchor-link" href="#Learning-Objectives">¶</a>
                </h2>
                <p>By the end of this lab, you should understand:</p>
                <ul>
                    <li>how to perform basic preprocessing on text data</li>
                    <li>the layers used in <code>keras</code> to construct RNNs and its variants (GRU, LSTM)</li>
                    <li>how the model's task (i.e., many-to-1, many-to-many) affects architecture choices</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><a id="contents"></a></p>
                <h2 id="Notebook-Contents">Notebook Contents<a class="anchor-link" href="#Notebook-Contents">¶</a></h2>
                <ul>
                    <li><a href="#imdb"><strong>IMDB Review Dataset</strong></a></li>
                    <li><a href="#prep"><strong>Preprocessing Text Data</strong></a>
                        <ul>
                            <li><a href="#token">Tokenization</a></li>
                            <li><a href="#pad">Padding</a></li>
                            <li><a href="#encode">Numerical Encoding</a></li>
                        </ul>
                    </li>
                    <li><a href="#FFNN"><strong>Movie Review Sentiment Analysis</strong></a>
                        <ul>
                            <li><a href="#FFNN">Naive FFNN</a></li>
                            <li><a href="#embed">Embedding Layer</a></li>
                            <li><a href="#cnn">1D CNN</a></li>
                            <li><a href="#rnn">Vanilla RNN</a></li>
                            <li><a href="#vanish">Vanishing/Exploding Gradients</a></li>
                            <li><a href="#gru">GRU</a></li>
                            <li><a href="#lstm">LSTM</a></li>
                            <li><a href="#bidir">BiDirectional Layer</a></li>
                            <li><a href="#deep">Deep RNNs</a></li>
                            <li><a href="#timedis">TimeDistributed Layer</a></li>
                            <li><a href="#repeatvec">RepeatVector Layer</a></li>
                            <li><a href="#cnnrnn">CNN + RNN</a></li>
                        </ul>
                    </li>
                    <li><a href="#metal"><strong>Heavy Metal Lyric Generator</strong></a>
                        <ul>
                            <li><a href="#pairs">Creating Input/Target Pairs</a></li>
                            <li><a href="#lambdacall">LambdaCallback</a></li>
                        </ul>
                    </li>
                    <li><a href="#math"><strong>Arithmetic /w RNN</strong></a></li>
                </ul>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [3]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span
                            class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span
                                class="n">imdb</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span
                                class="n">Sequential</span><span class="p">,</span> <span class="n">Model</span><span
                                class="p">,</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span
                                class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Bidirectional</span><span
                                class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span><span
                                class="p">,</span> <span class="n">GRU</span><span class="p">,</span> <span class="n">LSTM</span><span
                                class="p">,</span> <span class="n">SimpleRNN</span><span class="p">,</span>\
                                    <span class="n">Input</span><span class="p">,</span> <span
                                class="n">TimeDistributed</span><span class="p">,</span> <span
                                class="n">Dropout</span><span class="p">,</span> <span class="n">RepeatVector</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span
                                class="n">Conv1D</span><span class="p">,</span> <span class="n">Conv2D</span><span
                                class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span
                                class="n">MaxPool1D</span><span class="p">,</span> <span class="n">MaxPool2D</span><span
                                class="p">,</span> <span class="n">Lambda</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span
                                class="n">EarlyStopping</span><span class="p">,</span> <span
                                class="n">LambdaCallback</span><span class="p">,</span> <span
                                class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.initializers</span> <span class="kn">import</span> <span
                                class="n">Constant</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing</span> <span class="kn">import</span> <span
                                class="n">sequence</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span
                                class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span
                                class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span><span class="o">,</span> <span class="nn">sys</span>
<span class="c1"># fix random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span
                                class="p">(</span><span class="mi">109</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <h2 id="Case-Study:-IMDB-Review-Classifier">Case Study: IMDB Review Classifier<a class="anchor-link"
                                                                                                 href="#Case-Study:-IMDB-Review-Classifier">¶</a>
                </h2>
                <p><img src="fig/manyto1.png" width="300px"/></p>
                <p>Let's frame our discussion of RNNS around the example a text classifier. Specifically, We'll build
                    and evaluate various models that all attempt to descriminate between positive and negative reviews
                    through the Internet Movie Database (IMDB). The dataset is again made available to us through the
                    tensorflow datasets API.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [4]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span
                            class="nn">tensorflow_datasets</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [5]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="p">(</span><span
                            class="n">train</span><span class="p">,</span> <span class="n">test</span><span
                            class="p">),</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tensorflow_datasets</span><span
                            class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'imdb_reviews'</span><span
                            class="p">,</span> <span class="n">split</span><span class="o">=</span><span
                            class="p">[</span><span class="s1">'train'</span><span class="p">,</span> <span class="s1">'test'</span><span
                            class="p">],</span> <span class="n">with_info</span><span class="o">=</span><span
                            class="kc">True</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>The helpful <code>info</code> object provides details about the dataset.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [6]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">info</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[6]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>tfds.core.DatasetInfo(
    name='imdb_reviews',
    full_name='imdb_reviews/plain_text/1.0.0',
    description="""
    Large Movie Review Dataset.
    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.
    """,
    config_description="""
    Plain text
    """,
    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',
    data_path='/home/10914655/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',
    download_size=80.23 MiB,
    dataset_size=129.83 MiB,
    features=FeaturesDict({
        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
        'text': Text(shape=(), dtype=tf.string),
    }),
    supervised_keys=('text', 'label'),
    splits={
        'test': <splitinfo num_examples="25000," num_shards="1">,
        'train': <splitinfo num_examples="25000," num_shards="1">,
        'unsupervised': <splitinfo num_examples="50000," num_shards="1">,
    },
    citation="""@InProceedings{maas-EtAl:2011:ACL-HLT2011,
      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
      title     = {Learning Word Vectors for Sentiment Analysis},
      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
      month     = {June},
      year      = {2011},
      address   = {Portland, Oregon, USA},
      publisher = {Association for Computational Linguistics},
      pages     = {142--150},
      url       = {http://www.aclweb.org/anthology/P11-1015}
    }""",
)</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We see that the dataset consists of text reviews and binary good/bad labels. Here are two
                    examples:</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [7]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span
                            class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span
                            class="s1">'bad'</span><span class="p">,</span> <span class="mi">1</span><span
                            class="p">:</span> <span class="s1">'good'</span><span class="p">}</span>
<span class="n">seen</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'bad'</span><span
                                class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span
                                class="s1">'good'</span><span class="p">:</span> <span class="kc">False</span><span
                                class="p">}</span>
<span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">train</span><span
                                class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span
                                class="s1">'label'</span><span class="p">]</span><span class="o">.</span><span
                                class="n">numpy</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">seen</span><span class="p">[</span><span
                                class="n">labels</span><span class="p">[</span><span class="n">label</span><span
                                class="p">]]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span
                                class="s2">"text:</span><span class="se">\n</span><span class="si">{</span><span
                                class="n">review</span><span class="p">[</span><span class="s1">'text'</span><span
                                class="p">]</span><span class="o">.</span><span class="n">numpy</span><span
                                class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span
                                class="si">}</span><span class="se">\n</span><span class="s2">"</span><span
                                class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span
                                class="s2">"label: </span><span class="si">{</span><span class="n">labels</span><span
                                class="p">[</span><span class="n">label</span><span class="p">]</span><span
                                class="si">}</span><span class="se">\n</span><span class="s2">"</span><span
                                class="p">)</span>
        <span class="n">seen</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span
                                class="n">label</span><span class="p">]]</span> <span class="o">=</span> <span
                                class="kc">True</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">val</span> <span
                                class="o">==</span> <span class="kc">True</span> <span class="k">for</span> <span
                                class="n">val</span> <span class="ow">in</span> <span class="n">seen</span><span
                                class="o">.</span><span class="n">values</span><span class="p">()):</span>
        <span class="k">break</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>text:
This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.

label: bad

text:
This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.

label: good

</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Great! But unfortunately, computers can read! 📖--🤖❓</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <h2 id="Preprocessing-Text-Data-">Preprocessing Text Data
                    <div id="prep"></div>
                    <a class="anchor-link" href="#Preprocessing-Text-Data-">¶</a></h2>
                <p>Computers have no built-in knowledge of language and cannot understand text data in any rich way that
                    humans do -- at least not without some help! The first crucial step in natural language processing
                    is to clean and preprocess your data so that your algorithms and models can make use of it.</p>
                <p>We'll look at a few preprocess steps:</p>
                <pre><code>- tokenization
- padding
- numerical encoding

</code></pre>
                <p>Depending on your NLP task, you may want to take additional preprocessing steps which we will not
                    cover here. These can include:</p>
                <ul>
                    <li>converting all characters to lowercase</li>
                    <li>treating each punctuation mark as a token (e.g., , . ! ? are each separate tokens)</li>
                    <li>removing punctuation altogether</li>
                    <li>separating each sentence with a unique symbol (e.g., <s> and </s>)</li>
                    <li>removing words that are incredibly common (e.g., function words, (in)definite articles). These
                        are referred to as 'stopwords').
                    </li>
                    <li>Lemmatizing (replacing words with their 'dictionary entry form')</li>
                    <li>Stemming (removing grammatical morphemes)</li>
                </ul>
                <p>Useful NLP Python libraries such as <a href="https://www.nltk.org/">NLTK</a> and <a
                        href="https://spacy.io/">spaCy</a> provide built in methods for many of these preprocessing
                    steps.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="token"><b>Tokenization</b></div>
                </br></p>
                <p><strong>Tokens</strong> are the atomic units of meaning which our model will be working with. What
                    should these units be? These could be characters, words, or even sentences. For our movie review
                    classifier we will be working at the word level.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>For this example we will process just a subset of the original dataset.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [8]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">SAMPLE_SIZE</span> <span
                            class="o">=</span> <span class="mi">10</span>
<span class="n">subset</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span
                                class="n">train</span><span class="o">.</span><span class="n">take</span><span
                                class="p">(</span><span class="n">SAMPLE_SIZE</span><span class="p">))</span>
<span class="n">subset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[8]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>{'label': <tf.tensor: shape="()," dtype="int64," numpy="0">,
 'text': <tf.tensor: shape="()," dtype="string," numpy='b"This' was an absolutely terrible movie. don&#x27;t be lured in
                     by christopher walken or michael ironside. both are great actors, but this must simply be their
                     worst role in history. even their great acting could not redeem this movie&#x27;s ridiculous
                     storyline. this movie is an early nineties us propaganda piece. the most pathetic scenes were those
                     when the columbian rebels were making their cases for revolutions. maria conchita alonso appeared
                     phony, and her pseudo-love affair with walken was nothing but a pathetic emotional plug in a movie
                     that was devoid of any real meaning. i am disappointed that there are movies like this, ruining
                     actor&#x27;s like christopher walken&#x27;s good name. i could barely sit through it.&quot;>}</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>The TFDS format allows for the construction of efficient preprocessing pipelines. But for our own
                    preprocessing example we will be primarily working with Python <code>list</code> objects. This gives
                    us a chance to practice the Python <strong>list comprehension</strong> which is a powerful tool to
                    have at your disposal. It will serve you well when processing arbitrary text which may not already
                    be in a nice TFDS format (such as in the HW 😉).</p>
                <p>We'll convert our data subset into X and y lists.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [9]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span
                            class="o">=</span> <span class="p">[</span><span class="n">x</span><span
                            class="p">[</span><span class="s1">'text'</span><span class="p">]</span><span
                            class="o">.</span><span class="n">numpy</span><span class="p">()</span><span
                            class="o">.</span><span class="n">decode</span><span class="p">()</span> <span
                            class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">subset</span><span
                            class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span
                                class="p">[</span><span class="s1">'label'</span><span class="p">]</span><span
                                class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span
                                class="n">x</span> <span class="ow">in</span> <span class="n">subset</span><span
                                class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [10]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span
                            class="p">(</span><span class="sa">f</span><span class="s1">'X has </span><span
                            class="si">{</span><span class="nb">len</span><span class="p">(</span><span
                            class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s1"> reviews'</span><span
                            class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'y has </span><span
                                class="si">{</span><span class="nb">len</span><span class="p">(</span><span
                                class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s1"> labels'</span><span
                                class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>X has 10 reviews
y has 10 labels
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [11]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">N_CHARS</span> <span class="o">=</span> <span
                            class="mi">20</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'First </span><span
                                class="si">{</span><span class="n">N_CHARS</span><span class="si">}</span><span
                                class="s1"> characters of all reviews:</span><span class="se">\n</span><span class="si">{</span><span
                                class="p">[</span><span class="n">x</span><span class="p">[:</span><span
                                class="mi">20</span><span class="p">]</span><span class="o">+</span><span class="s2">"..."</span> <span
                                class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span
                                class="n">X</span><span class="p">]</span><span class="si">}</span><span
                                class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'All labels:</span><span
                                class="se">\n</span><span class="si">{</span><span class="n">y</span><span
                                class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>First 20 characters of all reviews:
['This was an absolute...', 'I have been known to...', 'Mann photographs the...', 'This is the kind of ...', 'As others have menti...', 'This is a film which...', 'Okay, you have:<br
            ...&#x27;, &#x27;the film is based on...&#x27;, &#x27;i really love the se...&#x27;, &quot;sure, this one
            isn&#x27;t...&quot;] all labels: [0, 0, 0, 1, 1, 1, 0, 0, 0, 0] &lt; pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Each observation in <code>X</code> is a review. A review is a <code>str</code> object which we can
                    think of as a sequence of characters. This is indeed how Python treats strings as made clear by how
                    we are printing 'slices' of each review in the code cell above.<br/></p>
                <p>We'll see a bit later that you can in fact sucessfully train a neural network on text data at the
                    character level.</p>
                <p>But for the moment we will work at the word level, treating the word level. This means our
                    observations should be organized as <strong>sequences of words</strong> rather than sequences of
                    characters.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [12]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># list comprehensions again to the rescue!</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span
                                class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span
                                class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span
                                class="p">]</span>
<span class="c1"># The same thing can be accomplished with:</span>
<span class="c1"># list(map(str.split, X))</span>
<span class="c1"># but that is much harder to parse! O_o</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Now let's look at the first 10 <strong>tokens</strong> in the first 2 reviews.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [13]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span><span
                            class="p">[</span><span class="mi">0</span><span class="p">][:</span><span
                            class="mi">10</span><span class="p">],</span> <span class="n">X</span><span
                            class="p">[</span><span class="mi">1</span><span class="p">][:</span><span
                            class="mi">10</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[13]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>(['This',
  'was',
  'an',
  'absolutely',
  'terrible',
  'movie.',
  "Don't",
  'be',
  'lured',
  'in'],
 ['I',
  'have',
  'been',
  'known',
  'to',
  'fall',
  'asleep',
  'during',
  'films,',
  'but'])</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="pad"><b>Padding</b></div>
                </br></p>
                <p>Let's take a look at the lengths of the reviews in our subset.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [14]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="p">[</span><span
                            class="nb">len</span><span class="p">(</span><span class="n">x</span><span
                            class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span
                            class="ow">in</span> <span class="n">X</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[14]:</div>
                    <div class="output_text output_subarea output_execute_result">
                        <pre>[116, 112, 132, 88, 81, 289, 557, 111, 223, 127]</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>If we were training our RNN one sentence at a time, it would be okay to have sentences of varying
                    lengths. However, as with any neural network, it can be sometimes be advantageous to train inputs in
                    batches. When doing so with RNNs, our input tensors need to be of the same length/dimensions.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Here are two examples of tokenized reviews padded to have a length of 5.</p>
                <pre><code>['I', 'loved', 'it', '<pad>', '<pad>']
['It', 'stinks', '<pad>', '<pad>', '<pad>']</code></pre>
                <p>Now let's pad our own examples. Note that 'padding' in this context also means truncating sequences
                    that are longer than our specified max length.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [15]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">MAX_LEN</span> <span class="o">=</span> <span
                            class="mi">500</span>
<span class="n">PAD</span> <span class="o">=</span> <span class="s1">'<pad>'</span>
<span class="c1"># truncate</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span
                                class="p">[:</span><span class="n">MAX_LEN</span><span class="p">]</span> <span
                                class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span
                                class="n">X</span><span class="p">]</span>
<span class="c1"># pad</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span
                                class="p">:</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span
                                class="p">)</span> <span class="o"><</span> <span class="n">MAX_LEN</span><span
                                class="p">:</span>
        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span
                                class="n">PAD</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [16]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="p">[</span><span
                            class="nb">len</span><span class="p">(</span><span class="n">x</span><span
                            class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span
                            class="ow">in</span> <span class="n">X</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[16]:</div>
                    <div class="output_text output_subarea output_execute_result">
                        <pre>[500, 500, 500, 500, 500, 500, 500, 500, 500, 500]</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Now all reviews are of a uniform length!</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="encode"><b>Numerical Encoding</b></div>
                </br></p>
                <p>If each review in our dataset is an observation, then the features of each observation are the
                    tokens, in this case, words. But these words are still strings. Our machine learning methods require
                    us to be able to multiple our features by weights. If we want to use these words as inputs for a
                    neural network we'll have to convert them into some numerical representation.</p>
                <p>One solution is to create a one-to-one mapping between unique words and integers.</p>
                <p>If the five sentences below were our entire corpus, our conversion would look this:</p>
                <ol>
                    <li>i have books - [1, 4, 2]</li>
                    <li>interesting books are useful [11,2,9,8]</li>
                    <li>i have computers [1,4,3]</li>
                    <li>computers are interesting and useful [3,5,11,10,8]</li>
                    <li>books and computers are both valuable. [2,10,3,9,13,12]</li>
                    <li>bye bye [7,7]</li>
                </ol>
                <p>I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11,
                    valuable-12, both-13</p>
                <p>To accomplish this we'll first need to know what all the unique words are in our dataset.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [17]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">all_tokens</span> <span
                            class="o">=</span> <span class="p">[</span><span class="n">word</span> <span
                            class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span
                            class="n">X</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span
                            class="n">review</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [18]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># sanity check</span>
<span class="nb">len</span><span class="p">(</span><span class="n">all_tokens</span><span class="p">),</span> <span
                                class="nb">sum</span><span class="p">([</span><span class="nb">len</span><span
                                class="p">(</span><span class="n">x</span><span class="p">)</span> <span
                                class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span
                                class="n">X</span><span class="p">])</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[18]:</div>
                    <div class="output_text output_subarea output_execute_result">
                        <pre>(5000, 5000)</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Casting our <code>list</code> of words into a <code>set</code> is a great way to get all the <em>unique</em>
                    words in the data.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [19]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">vocab</span> <span
                            class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span
                            class="p">(</span><span class="n">all_tokens</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Unique Words:'</span><span
                                class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Unique Words: 892
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Now we need to create a mapping from words to integers. For this will a <strong>dictionary
                    comprehension</strong>.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [20]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">word2idx</span> <span
                            class="o">=</span> <span class="p">{</span><span class="n">word</span><span
                            class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span
                            class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span
                            class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span
                            class="n">vocab</span><span class="p">)}</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [21]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">word2idx</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[21]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>{'"Absolute': 0,
 '"Bohlen"-Fan': 1,
 '"Brideshead': 2,
 '"Candy"?).': 3,
 '"City': 4,
 '"Dieter': 5,
 '"Dieter"': 6,
 '"Dragonfly"': 7,
 '"I\'ve': 8,
 '"Lady."<br&#x27;: 9, &#x27;&quot;london&#x27;: 10, &#x27;&quot;make&#x27;: 11, &#x27;&quot;miss&quot;&#x27;: 12, &#x27;&quot;mr&#x27;: 13, &#x27;&quot;mrs.&quot;&#x27;: 14, &#x27;&quot;actors&quot;&#x27;: 15, &#x27;&quot;dewy-eyed.&quot;&lt;br&#x27;: 16, &#x27;&quot;hey,&#x27;: 17, &#x27;&quot;meanwhile,&quot;)&#x27;: 18, &#x27;&quot;men&quot;&#x27;: 19, &quot;&#x27;where&quot;: 20, &quot;&#x27;em&quot;: 21, &quot;&#x27;round&quot;: 22, &#x27;(backbone&#x27;: 23, &#x27;(barbarella).&#x27;: 24, &#x27;(brit&#x27;: 25, &#x27;(i&#x27;: 26, &#x27;(jeremy&#x27;: 27, &#x27;(remember&#x27;: 28, &#x27;(you&#x27;: 29, &#x27;(as&#x27;: 30, &#x27;(at&#x27;: 31, &#x27;(not&#x27;: 32, &#x27;(the&#x27;: 33, &quot;(there&#x27;s&quot;: 34, &#x27;(they&#x27;: 35, &#x27;(we&#x27;: 36, &#x27;(what&#x27;: 37, &#x27;(when,&#x27;: 38, &#x27;(yes&#x27;: 39, &#x27;-&#x27;: 40, &#x27;.&#x27;: 41, &#x27; /><br&#x27;: 42, &#x27; />Ah,': 43,
 '/>And': 44,
 '/>But': 45,
 '/>Canadian': 46,
 '/>David': 47,
 '/>First': 48,
 '/>Henceforth,': 49,
 '/>Joanna': 50,
 '/>Journalist': 51,
 '/>Nothing': 52,
 '/>OK,': 53,
 '/>Penelope': 54,
 '/>Peter': 55,
 '/>Second': 56,
 '/>So': 57,
 '/>Susan': 58,
 '/>Thank': 59,
 '/>Third': 60,
 '/>To': 61,
 '/>When': 62,
 '/>Wrong!': 63,
 '/>and': 64,
 '1-dimensional': 65,
 '14': 66,
 '1950s': 67,
 '20': 68,
 '<pad>': 69,
 '<br&#x27;: 70, &#x27;a&#x27;: 71, &#x27;after&#x27;: 72, &#x27;alberta&#x27;: 73, &#x27;alison&#x27;: 74, &#x27;alonso&#x27;: 75, &#x27;american&#x27;: 76, &#x27;and&#x27;: 77, &#x27;as&#x27;: 78, &#x27;b.b.e.&#x27;: 79, &#x27;beginners&quot;,&#x27;: 80, &#x27;boarding-school,&#x27;: 81, &#x27;bohlen&quot;&#x27;: 82, &#x27;both&#x27;: 83, &#x27;brennan&#x27;: 84, &#x27;bulimia&#x27;: 85, &#x27;but&#x27;: 86, &#x27;cage&#x27;: 87, &#x27;canadian&#x27;: 88, &#x27;cher&#x27;: 89, &#x27;christmas!)&#x27;: 90, &#x27;christopher&#x27;: 91, &#x27;circus.&lt;br&#x27;: 92, &#x27;city&#x27;: 93, &#x27;city,&#x27;: 94, &#x27;col.&#x27;: 95, &#x27;colin&#x27;: 96, &#x27;colonel&#x27;: 97, &#x27;columbian&#x27;: 98, &#x27;conchita&#x27;: 99, &#x27;constantly&#x27;: 100, &#x27;coppola&#x27;: 101, &#x27;cricket&#x27;: 102, &quot;cricket&#x27;s&quot;: 103, &#x27;davies)&#x27;: 104, &#x27;dawson&#x27;: 105, &#x27;deadwood,&#x27;: 106, &#x27;dean&#x27;: 107, &#x27;depardieu,&#x27;: 108, &quot;don&#x27;t&quot;: 109, &#x27;dragonfly&#x27;: 110, &#x27;emily&#x27;: 111, &#x27;england)&#x27;: 112, &#x27;england.)&#x27;: 113, &#x27;european&#x27;: 114, &#x27;even&#x27;: 115, &#x27;film&#x27;: 116, &#x27;first&#x27;: 117, &#x27;gerard&#x27;: 118, &#x27;german&#x27;: 119, &#x27;giancarlo&#x27;: 120, &#x27;giannini&#x27;: 121, &quot;girls&#x27;&quot;: 122, &#x27;hampshire&#x27;: 123, &#x27;he&#x27;: 124, &#x27;headmistress&#x27;: 125, &#x27;her&#x27;: 126, &#x27;herringbone-tweed,&#x27;: 127, &#x27;hollywood&#x27;: 128, &#x27;home&#x27;: 129, &#x27;however&#x27;: 130, &#x27;i&#x27;: 131, &quot;i&#x27;ll&quot;: 132, &quot;i&#x27;m&quot;: 133, &#x27;ii&#x27;: 134, &#x27;if&#x27;: 135, &#x27;ironside.&#x27;: 136, &#x27;it&#x27;: 137, &quot;it&#x27;s&quot;: 138, &#x27;japanese&#x27;: 139, &#x27;jimmy&#x27;: 140, &#x27;john&#x27;: 141, &#x27;justice&quot;.&#x27;: 142, &#x27;katt&#x27;: 143, &#x27;keith&#x27;: 144, &#x27;klondike&#x27;: 145, &#x27;l.l.&#x27;: 146, &#x27;l.l.&lt;br&#x27;: 147, &#x27;lady&#x27;: 148, &#x27;law&#x27;: 149, &#x27;leading&#x27;: 150, &quot;lies&#x27;.&quot;: 151, &#x27;lohman&#x27;: 152, &quot;lohman&#x27;s.&quot;: 153, &#x27;lohman,&#x27;: 154, &#x27;london,&#x27;: 155, &#x27;lord&#x27;: 156, &#x27;love&#x27;: 157, &#x27;lumley&#x27;: 158, &#x27;madness&#x27;: 159, &#x27;mann&#x27;: 160, &#x27;manor,&#x27;: 161, &#x27;manor.&lt;br&#x27;: 162, &#x27;mare&#x27;: 163, &#x27;maria&#x27;: 164, &#x27;mccallum&#x27;: 165, &#x27;mcinnes&#x27;: 166, &quot;mcinnes&#x27;s&quot;: 167, &#x27;meanwhile&#x27;: 168, &#x27;michael&#x27;: 169, &#x27;miss&#x27;: 170, &#x27;mortimer&#x27;: 171, &#x27;mountains&#x27;: 172, &#x27;mountie&#x27;: 173, &#x27;mr.&#x27;: 174, &#x27;nancherrow&#x27;: 175, &#x27;new&#x27;: 176, &#x27;nicolas&#x27;: 177, &#x27;north&#x27;: 178, &quot;o&#x27;toole&quot;: 179, &#x27;ok-movie.&#x27;: 180, &#x27;okay,&#x27;: 181, &quot;ol&#x27;&quot;: 182, &#x27;our&#x27;: 183, &#x27;phillip&#x27;: 184, &#x27;pilcher&#x27;: 185, &#x27;polonia&#x27;: 186, &#x27;reefer&#x27;: 187, &#x27;revisited,&quot;&#x27;: 188, &#x27;rocky&#x27;: 189, &#x27;roman&#x27;: 190, &quot;she&#x27;s&quot;: 191, &#x27;shea&#x27;: 192, &#x27;so&#x27;: 193, &#x27;spades&quot;&#x27;: 194, &#x27;speaking&#x27;: 195, &#x27;stately&#x27;: 196, &#x27;stewart&#x27;: 197, &#x27;still,&#x27;: 198, &#x27;stockwell&#x27;: 199, &#x27;sunday&#x27;: 200, &#x27;sure,&#x27;: 201, &#x27;susan&#x27;: 202, &#x27;teacups,&#x27;: 203, &#x27;technically&#x27;: 204, &quot;that&#x27;s&quot;: 205, &#x27;the&#x27;: 206, &#x27;there&#x27;: 207, &#x27;they&#x27;: 208, &quot;they&#x27;re&quot;: 209, &#x27;things&#x27;: 210, &#x27;this&#x27;: 211, &#x27;together,&#x27;: 212, &#x27;truth&#x27;: 213, &#x27;us&#x27;: 214, &#x27;venerable&#x27;: 215, &#x27;walken&#x27;: 216, &quot;walken&#x27;s&quot;: 217, &#x27;walter&#x27;: 218, &#x27;war&#x27;: 219, &#x27;well,&#x27;: 220, &#x27;west.&lt;br&#x27;: 221, &#x27;when&#x27;: 222, &#x27;wild&#x27;: 223, &#x27;winningham&#x27;: 224, &#x27;wonderful&#x27;: 225, &#x27;world&#x27;: 226, &#x27;york&#x27;: 227, &#x27;yukon&#x27;: 228, &#x27;a&#x27;: 229, &#x27;ability&#x27;: 230, &#x27;ably&#x27;: 231, &#x27;about.&#x27;: 232, &#x27;absolutely&#x27;: 233, &#x27;acclaimed;&#x27;: 234, &#x27;accord&#x27;: 235, &#x27;accuracy.&#x27;: 236, &#x27;accurate&#x27;: 237, &#x27;achievement,&#x27;: 238, &#x27;act&#x27;: 239, &#x27;acting&#x27;: 240, &#x27;action&#x27;: 241, &#x27;actor&#x27;: 242, &quot;actor&#x27;s&quot;: 243, &#x27;actors&#x27;: 244, &#x27;actors,&#x27;: 245, &quot;actress&#x27;s&quot;: 246, &#x27;actresses&#x27;: 247, &#x27;actually&#x27;: 248, &#x27;admit,&#x27;: 249, &#x27;advice&#x27;: 250, &#x27;advice:&#x27;: 251, &#x27;affair&#x27;: 252, &#x27;after&#x27;: 253, &#x27;afternoon&#x27;: 254, &#x27;age&#x27;: 255, &#x27;age!).&#x27;: 256, &#x27;ago&#x27;: 257, &#x27;ahead&#x27;: 258, &#x27;air&#x27;: 259, &#x27;alive&#x27;: 260, &#x27;all&#x27;: 261, &#x27;all,&#x27;: 262, &#x27;all.&#x27;: 263, &#x27;all.&lt;br&#x27;: 264, &#x27;along.&#x27;: 265, &#x27;alright,&#x27;: 266, &#x27;always&#x27;: 267, &#x27;always)&#x27;: 268, &#x27;am&#x27;: 269, &#x27;amazingly&#x27;: 270, &#x27;amusing&#x27;: 271, &#x27;an&#x27;: 272, &#x27;and&#x27;: 273, &#x27;anguish&#x27;: 274, &#x27;animal&#x27;: 275, &#x27;another!)&lt;br&#x27;: 276, &#x27;another.&#x27;: 277, &#x27;any&#x27;: 278, &#x27;anybody&#x27;: 279, &#x27;anything&#x27;: 280, &#x27;apology&#x27;: 281, &#x27;appear&#x27;: 282, &#x27;appeared&#x27;: 283, &#x27;are&#x27;: 284, &#x27;arm-chair&#x27;: 285, &#x27;around&#x27;: 286, &#x27;around,&#x27;: 287, &#x27;as&#x27;: 288, &#x27;asleep&#x27;: 289, &#x27;aspiring&#x27;: 290, &#x27;astonishing&#x27;: 291, &#x27;at&#x27;: 292, &#x27;autobiography&#x27;: 293, &#x27;award&#x27;: 294, &#x27;baby&#x27;: 295, &#x27;backbone!&lt;br&#x27;: 296, &#x27;backlighting.&#x27;: 297, &#x27;badly-acted,&#x27;: 298, &#x27;barely&#x27;: 299, &#x27;barometers&#x27;: 300, &#x27;based&#x27;: 301, &#x27;battling&#x27;: 302, &#x27;be&#x27;: 303, &#x27;beautiful&#x27;: 304, &#x27;because&#x27;: 305, &#x27;become&#x27;: 306, &#x27;becomes&#x27;: 307, &#x27;been&#x27;: 308, &#x27;before&#x27;: 309, &#x27;behaviour&#x27;: 310, &#x27;being&#x27;: 311, &#x27;best&#x27;: 312, &#x27;best,&#x27;: 313, &#x27;best.&#x27;: 314, &#x27;better&#x27;: 315, &#x27;big&#x27;: 316, &#x27;bit&#x27;: 317, &#x27;blockbuster,&#x27;: 318, &#x27;body&#x27;: 319, &#x27;border&#x27;: 320, &#x27;boring.&#x27;: 321, &#x27;boy&#x27;: 322, &#x27;boy.&#x27;: 323, &#x27;brilliant&#x27;: 324, &#x27;bulimia&#x27;: 325, &#x27;business&#x27;: 326, &#x27;but&#x27;: 327, &#x27;but,&#x27;: 328, &#x27;by&#x27;: 329, &#x27;by,&#x27;: 330, &#x27;called&#x27;: 331, &#x27;cameos&#x27;: 332, &#x27;camps&#x27;: 333, &#x27;can&#x27;: 334, &#x27;can&quot;!&#x27;: 335, &quot;can&#x27;t&quot;: 336, &#x27;cant&#x27;: 337, &#x27;captures&#x27;: 338, &#x27;cases&#x27;: 339, &#x27;cast&#x27;: 340, &#x27;catatonic.&#x27;: 341, &#x27;causes&#x27;: 342, &#x27;causing&#x27;: 343, &#x27;character&#x27;: 344, &#x27;choice&#x27;: 345, &#x27;cinema&#x27;: 346, &#x27;colonel&#x27;: 347, &#x27;combination&#x27;: 348, &#x27;come&#x27;: 349, &#x27;comes&#x27;: 350, &#x27;comfortable&#x27;: 351, &#x27;company&#x27;: 352, &#x27;complete&#x27;: 353, &#x27;compulsive&#x27;: 354, &#x27;concern&#x27;: 355, &#x27;consent&#x27;: 356, &#x27;considerate&#x27;: 357, &#x27;constant.&#x27;: 358, &#x27;contacts?)&lt;br&#x27;: 359, &#x27;content&#x27;: 360, &#x27;continue&#x27;: 361, &#x27;control&#x27;: 362, &#x27;convey&#x27;: 363, &#x27;could&#x27;: 364, &quot;couldn&#x27;t&quot;: 365, &#x27;couple&#x27;: 366, &#x27;courage&#x27;: 367, &#x27;course)&#x27;: 368, &#x27;cover&#x27;: 369, &#x27;cries&#x27;: 370, &#x27;criticize.&#x27;: 371, &#x27;cross,&#x27;: 372, &#x27;crush&#x27;: 373, &#x27;crying,&#x27;: 374, &#x27;dangerous&#x27;: 375, &#x27;dash&#x27;: 376, &#x27;days&#x27;: 377, &#x27;dead&#x27;: 378, &#x27;decide&#x27;: 379, &#x27;deep,&#x27;: 380, &#x27;degree&#x27;: 381, &#x27;depends&#x27;: 382, &#x27;depths&#x27;: 383, &#x27;descend&#x27;: 384, &#x27;deserves&#x27;: 385, &#x27;despair,&#x27;: 386, &#x27;despair.&#x27;: 387, &#x27;desperation&#x27;: 388, &#x27;destroy&#x27;: 389, &#x27;development&#x27;: 390, &#x27;devoid&#x27;: 391, &#x27;did&#x27;: 392, &quot;didn&#x27;t&quot;: 393, &#x27;directed&#x27;: 394, &#x27;director&#x27;: 395, &#x27;disappointed&#x27;: 396, &#x27;disgust.&#x27;: 397, &#x27;disorder.&#x27;: 398, &#x27;do.&#x27;: 399, &#x27;does&#x27;: 400, &quot;don&#x27;t&quot;: 401, &#x27;done,&#x27;: 402, &#x27;done.&lt;br&#x27;: 403, &#x27;due&#x27;: 404, &#x27;dumb&#x27;: 405, &#x27;during&#x27;: 406, &#x27;each&#x27;: 407, &#x27;early&#x27;: 408, &#x27;eaten&#x27;: 409, &#x27;eating&#x27;: 410, &#x27;edge.&#x27;: 411, &#x27;effected&#x27;: 412, &#x27;either&#x27;: 413, &#x27;elect&#x27;: 414, &#x27;else.&#x27;: 415, &#x27;emblazered&#x27;: 416, &#x27;emotional&#x27;: 417, &#x27;emotions&#x27;: 418, &#x27;end,&#x27;: 419, &#x27;enforce&#x27;: 420, &#x27;enjoyable&#x27;: 421, &#x27;enough&#x27;: 422, &#x27;enough.&#x27;: 423, &#x27;ensweatered&#x27;: 424, &#x27;equally&#x27;: 425, &#x27;even&#x27;: 426, &#x27;every&#x27;: 427, &#x27;everything&#x27;: 428, &#x27;exactly&#x27;: 429, &#x27;excellent.&#x27;: 430, &#x27;experiment&#x27;: 431, &#x27;explanation&#x27;: 432, &#x27;exploits&#x27;: 433, &#x27;extreme&#x27;: 434, &#x27;eyes&#x27;: 435, &#x27;fall&#x27;: 436, &#x27;family&#x27;: 437, &#x27;fashion,&#x27;: 438, &#x27;fast&#x27;: 439, &#x27;fathers&#x27;: 440, &#x27;fell&#x27;: 441, &#x27;female&#x27;: 442, &#x27;few&#x27;: 443, &#x27;filled&#x27;: 444, &#x27;film&#x27;: 445, &#x27;film,&#x27;: 446, &#x27;filmmaker&#x27;: 447, &#x27;films&#x27;: 448, &#x27;films,&#x27;: 449, &#x27;films.&#x27;: 450, &#x27;finally:&lt;br&#x27;: 451, &#x27;find&#x27;: 452, &#x27;finds&#x27;: 453, &#x27;finely&#x27;: 454, &#x27;fired.&#x27;: 455, &#x27;first&#x27;: 456, &#x27;folks;&#x27;: 457, &#x27;for&#x27;: 458, &#x27;found&#x27;: 459, &#x27;fourth&#x27;: 460, &#x27;frenzy&#x27;: 461, &#x27;from&#x27;: 462, &#x27;fruits&#x27;: 463, &#x27;full&#x27;: 464, &#x27;gal!)&lt;br&#x27;: 465, &#x27;gave&#x27;: 466, &#x27;gently&#x27;: 467, &#x27;genuine&#x27;: 468, &#x27;get&#x27;: 469, &#x27;gets&#x27;: 470, &#x27;girl&#x27;: 471, &#x27;girl,&#x27;: 472, &#x27;give&#x27;: 473, &#x27;glamourous&#x27;: 474, &#x27;glitzy,&#x27;: 475, &#x27;go&#x27;: 476, &#x27;going&#x27;: 477, &#x27;gold&#x27;: 478, &#x27;good&#x27;: 479, &#x27;goodness&#x27;: 480, &#x27;gorgeous.&#x27;: 481, &#x27;gradations&#x27;: 482, &#x27;graduation.&#x27;: 483, &#x27;great&#x27;: 484, &#x27;guess&#x27;: 485, &#x27;gunfighters&#x27;: 486, &#x27;guy&#x27;: 487, &#x27;had&#x27;: 488, &#x27;happen&#x27;: 489, &#x27;happen,&#x27;: 490, &#x27;happened&#x27;: 491, &#x27;happening&#x27;: 492, &#x27;has&#x27;: 493, &#x27;hated&#x27;: 494, &#x27;have&#x27;: 495, &#x27;have:&lt;br&#x27;: 496, &#x27;having&#x27;: 497, &#x27;head&#x27;: 498, &#x27;her&#x27;: 499, &#x27;here&#x27;: 500, &#x27;highly&#x27;: 501, &#x27;him&#x27;: 502, &#x27;his&#x27;: 503, &#x27;history.&#x27;: 504, &#x27;homage&#x27;: 505, &#x27;home&#x27;: 506, &#x27;hours.&#x27;: 507, &#x27;how&#x27;: 508, &#x27;howl&#x27;: 509, &#x27;humor&#x27;: 510, &#x27;hush-hush&#x27;: 511, &#x27;hypocrisy&#x27;: 512, &#x27;hysteria&#x27;: 513, &#x27;i&#x27;: 514, &#x27;if&#x27;: 515, &#x27;immense&#x27;: 516, &#x27;in&#x27;: 517, &#x27;in,&#x27;: 518, &#x27;including,&#x27;: 519, &#x27;individual&#x27;: 520, &#x27;inside.&#x27;: 521, &#x27;intensity.&#x27;: 522, &#x27;interested&#x27;: 523, &#x27;into&#x27;: 524, &#x27;is&#x27;: 525, &quot;isn&#x27;t&quot;: 526, &#x27;it&#x27;: 527, &quot;it&#x27;s&quot;: 528, &#x27;it.&#x27;: 529, &#x27;its&#x27;: 530, &#x27;job&#x27;: 531, &#x27;just&#x27;: 532, &#x27;killed&#x27;: 533, &#x27;kind&#x27;: 534, &#x27;knowledge&#x27;: 535, &#x27;known&#x27;: 536, &#x27;last,&#x27;: 537, &#x27;lastly,&#x27;: 538, &#x27;later&#x27;: 539, &#x27;law&#x27;: 540, &#x27;least&#x27;: 541, &#x27;let&#x27;: 542, &#x27;libido.&#x27;: 543, &#x27;life&#x27;: 544, &#x27;like&#x27;: 545, &#x27;live&#x27;: 546, &#x27;local&#x27;: 547, &#x27;loss,&#x27;: 548, &#x27;lost&#x27;: 549, &#x27;lot.&#x27;: 550, &#x27;lots&#x27;: 551, &#x27;love&#x27;: 552, &#x27;love.&#x27;: 553, &#x27;loved&#x27;: 554, &#x27;lucky&#x27;: 555, &#x27;ludicrous&#x27;: 556, &#x27;lured&#x27;: 557, &#x27;made&#x27;: 558, &#x27;majority&#x27;: 559, &#x27;make&#x27;: 560, &#x27;making&#x27;: 561, &#x27;man&#x27;: 562, &#x27;marshal&#x27;: 563, &#x27;marshal!)&#x27;: 564, &#x27;marvels&#x27;: 565, &#x27;matter&#x27;: 566, &#x27;may&#x27;: 567, &#x27;me&#x27;: 568, &#x27;meaning.&#x27;: 569, &#x27;meant&#x27;: 570, &#x27;measured&#x27;: 571, &#x27;mellow&#x27;: 572, &#x27;men&#x27;: 573, &#x27;mentioned,&#x27;: 574, &#x27;microscopically&#x27;: 575, &#x27;mind&#x27;: 576, &#x27;mine)&#x27;: 577, &#x27;missed&#x27;: 578, &#x27;mistaken&#x27;: 579, &#x27;moist.&#x27;: 580, &#x27;more.&#x27;: 581, &#x27;most&#x27;: 582, &#x27;mostly&#x27;: 583, &#x27;mother,&#x27;: 584, &#x27;movie&#x27;: 585, &quot;movie&#x27;s&quot;: 586, &#x27;movie.&#x27;: 587, &#x27;movies&#x27;: 588, &#x27;much&#x27;: 589, &#x27;musical&#x27;: 590, &#x27;musician,&#x27;: 591, &#x27;must&#x27;: 592, &#x27;name&#x27;: 593, &#x27;name.&#x27;: 594, &#x27;nature&#x27;: 595, &#x27;never&#x27;: 596, &#x27;nightmare&#x27;: 597, &#x27;nineties&#x27;: 598, &#x27;no&#x27;: 599, &#x27;nor&#x27;: 600, &#x27;nostalgic&#x27;: 601, &#x27;not&#x27;: 602, &#x27;not.&#x27;: 603, &#x27;nothing&#x27;: 604, &#x27;novel.&lt;br&#x27;: 605, &quot;novelist&#x27;s&quot;: 606, &#x27;novels&quot;:&#x27;: 607, &#x27;novels:&#x27;: 608, &#x27;now&#x27;: 609, &#x27;nude&#x27;: 610, &#x27;occasion&#x27;: 611, &#x27;of&#x27;: 612, &#x27;off&#x27;: 613, &#x27;off.&#x27;: 614, &#x27;offensive&#x27;: 615, &#x27;office&#x27;: 616, &#x27;old&#x27;: 617, &#x27;oldest&#x27;: 618, &#x27;on&#x27;: 619, &#x27;on-screen&#x27;: 620, &#x27;once.&#x27;: 621, &#x27;one&#x27;: 622, &quot;one&#x27;s&quot;: 623, &#x27;one-dimensional&#x27;: 624, &#x27;only&#x27;: 625, &#x27;or&#x27;: 626, &#x27;or,&#x27;: 627, &#x27;oscillators&#x27;: 628, &#x27;other&#x27;: 629, &#x27;others&#x27;: 630, &#x27;out&#x27;: 631, &#x27;outdoors&#x27;: 632, &#x27;outside&#x27;: 633, &#x27;outstanding,&#x27;: 634, &#x27;own&#x27;: 635, &#x27;paddle&#x27;: 636, &#x27;paid&#x27;: 637, &#x27;pair&#x27;: 638, &#x27;parents&#x27;: 639, &#x27;part&#x27;: 640, &#x27;past&#x27;: 641, &#x27;pathetic&#x27;: 642, &#x27;people&#x27;: 643, &#x27;people.&#x27;: 644, &#x27;perfect&#x27;: 645, &#x27;performances&#x27;: 646, &#x27;perhaps&#x27;: 647, &#x27;phony,&#x27;: 648, &#x27;photographs&#x27;: 649, &#x27;picture.&#x27;: 650, &#x27;piece.&#x27;: 651, &#x27;play&#x27;: 652, &#x27;played&#x27;: 653, &#x27;plays&#x27;: 654, &#x27;plot&#x27;: 655, &#x27;plot,&#x27;: 656, &#x27;plug&#x27;: 657, &#x27;poignant&#x27;: 658, &#x27;pointlessly&#x27;: 659, &#x27;popular&#x27;: 660, &#x27;portrayal&#x27;: 661, &#x27;position.&#x27;: 662, &#x27;praise&#x27;: 663, &#x27;precise,&#x27;: 664, &#x27;prepared&#x27;: 665, &#x27;pressure&#x27;: 666, &#x27;pressure,&#x27;: 667, &#x27;prime&#x27;: 668, &#x27;prison&#x27;: 669, &#x27;producers&#x27;: 670, &#x27;propaganda&#x27;: 671, &#x27;proud&#x27;: 672, &#x27;pseudo-love&#x27;: 673, &#x27;pursue&#x27;: 674, &#x27;pursued,&#x27;: 675, &#x27;pursuers&#x27;: 676, &#x27;pursuing&#x27;: 677, &#x27;puts&#x27;: 678, &#x27;quite&#x27;: 679, &#x27;range&#x27;: 680, &#x27;rapids&#x27;: 681, &#x27;re-makings&#x27;: 682, &#x27;reaches&#x27;: 683, &#x27;read&#x27;: 684, &#x27;reading&#x27;: 685, &#x27;real&#x27;: 686, &#x27;really&#x27;: 687, &#x27;reason&#x27;: 688, &#x27;rebels&#x27;: 689, &#x27;received.&#x27;: 690, &#x27;recommend&#x27;: 691, &#x27;redeem&#x27;: 692, &#x27;remotely&#x27;: 693, &#x27;resembling&#x27;: 694, &#x27;resonance&#x27;: 695, &#x27;rest&#x27;: 696, &#x27;revolutions.&#x27;: 697, &#x27;ridiculous&#x27;: 698, &#x27;right,&#x27;: 699, &#x27;right?&lt;br&#x27;: 700, &#x27;rightly&#x27;: 701, &#x27;rising&#x27;: 702, &#x27;role&#x27;: 703, &#x27;roles&#x27;: 704, &#x27;romance&#x27;: 705, &#x27;row&#x27;: 706, &#x27;rubbish.&#x27;: 707, &#x27;ruining&#x27;: 708, &#x27;running&#x27;: 709, &#x27;rush.&#x27;: 710, &#x27;said&#x27;: 711, &#x27;same&#x27;: 712, &#x27;say&#x27;: 713, &#x27;says&#x27;: 714, &#x27;scale&#x27;: 715, &#x27;scene&#x27;: 716, &#x27;scenes&#x27;: 717, &#x27;schools,&#x27;: 718, &#x27;sci-fi&#x27;: 719, &#x27;script.&#x27;: 720, &#x27;see&#x27;: 721, &#x27;seeing&#x27;: 722, &#x27;seem&#x27;: 723, &#x27;seemed&#x27;: 724, &#x27;seen&#x27;: 725, &#x27;sense&#x27;: 726, &#x27;sensitive&#x27;: 727, &#x27;serving&#x27;: 728, &#x27;set&#x27;: 729, &#x27;sette&#x27;: 730, &#x27;setting&#x27;: 731, &#x27;sexiest&#x27;: 732, &#x27;sexual&#x27;: 733, &#x27;sexy&#x27;: 734, &#x27;shake&#x27;: 735, &#x27;shall&#x27;: 736, &#x27;sharp&#x27;: 737, &#x27;she&#x27;: 738, &quot;she&#x27;s&quot;: 739, &#x27;shooting&#x27;: 740, &#x27;should&#x27;: 741, &#x27;show&#x27;: 742, &#x27;show.&#x27;: 743, &#x27;shown&#x27;: 744, &#x27;shows&#x27;: 745, &#x27;side&#x27;: 746, &#x27;side.&#x27;: 747, &#x27;sign:&#x27;: 748, &#x27;simply&#x27;: 749, &#x27;sis&#x27;: 750, &#x27;sit&#x27;: 751, &#x27;sixties&#x27;: 752, &#x27;sixties.&#x27;: 753, &#x27;slow&#x27;: 754, &#x27;snowy&#x27;: 755, &#x27;so&#x27;: 756, &#x27;so.&#x27;: 757, &#x27;some&#x27;: 758, &#x27;somehow,&#x27;: 759, &#x27;soporific&#x27;: 760, &#x27;sort.&lt;br&#x27;: 761, &#x27;soul&#x27;: 762, &#x27;speaking&#x27;: 763, &#x27;spectacular&#x27;: 764, &#x27;spelling,&#x27;: 765, &#x27;spirit&#x27;: 766, &#x27;spotlight&#x27;: 767, &#x27;squeeze.&#x27;: 768, &#x27;standard)&#x27;: 769, &#x27;stars&#x27;: 770, &#x27;still&#x27;: 771, &#x27;story&#x27;: 772, &#x27;story,&#x27;: 773, &#x27;storyline.&#x27;: 774, &#x27;streets&#x27;: 775, &#x27;subject.&#x27;: 776, &#x27;such&#x27;: 777, &#x27;suffering&#x27;: 778, &#x27;sum&#x27;: 779, &#x27;sunlight!&quot;&lt;br&#x27;: 780, &#x27;superb&#x27;: 781, &#x27;supporting&#x27;: 782, &#x27;sympathise&#x27;: 783, &#x27;sympathy&#x27;: 784, &#x27;symptoms.&#x27;: 785, &#x27;target&#x27;: 786, &#x27;tea&#x27;: 787, &#x27;teenage&#x27;: 788, &#x27;tell&#x27;: 789, &#x27;telling&#x27;: 790, &#x27;tells&#x27;: 791, &#x27;temp-jobs&#x27;: 792, &#x27;terrible&#x27;: 793, &#x27;than&#x27;: 794, &#x27;that&#x27;: 795, &quot;that&#x27;s&quot;: 796, &#x27;that.&#x27;: 797, &#x27;thats&#x27;: 798, &#x27;the&#x27;: 799, &#x27;their&#x27;: 800, &#x27;them&#x27;: 801, &#x27;them.&#x27;: 802, &#x27;themselves&#x27;: 803, &#x27;themselves,&#x27;: 804, &#x27;then&#x27;: 805, &#x27;there&#x27;: 806, &#x27;these&#x27;: 807, &#x27;they&#x27;: 808, &#x27;things&#x27;: 809, &#x27;this&#x27;: 810, &#x27;this,&#x27;: 811, &#x27;those&#x27;: 812, &#x27;thought&#x27;: 813, &#x27;three&#x27;: 814, &#x27;three.&#x27;: 815, &#x27;through&#x27;: 816, &#x27;time&#x27;: 817, &#x27;tired,&#x27;: 818, &#x27;to&#x27;: 819, &#x27;today,&#x27;: 820, &#x27;took&#x27;: 821, &#x27;tormented&#x27;: 822, &#x27;town?&#x27;: 823, &#x27;toy-boy&#x27;: 824, &#x27;toy-boy,&#x27;: 825, &#x27;trauma,&#x27;: 826, &#x27;true&#x27;: 827, &#x27;tuned&#x27;: 828, &#x27;turned&#x27;: 829, &#x27;two&#x27;: 830, &#x27;type&#x27;: 831, &#x27;ultimate&#x27;: 832, &#x27;unbearable&#x27;: 833, &#x27;up&#x27;: 834, &#x27;up:&#x27;: 835, &#x27;us&#x27;: 836, &#x27;us.&lt;br&#x27;: 837, &#x27;use&#x27;: 838, &#x27;uses&#x27;: 839, &#x27;usually&#x27;: 840, &#x27;version&#x27;: 841, &#x27;very&#x27;: 842, &#x27;vibrating&#x27;: 843, &#x27;viewers&#x27;: 844, &#x27;waiting&#x27;: 845, &#x27;waiting!&#x27;: 846, &#x27;want&#x27;: 847, &#x27;warm&#x27;: 848, &#x27;warn&#x27;: 849, &#x27;warning&#x27;: 850, &#x27;was&#x27;: 851, &#x27;was,&#x27;: 852, &quot;wasn&#x27;t&quot;: 853, &#x27;watched&#x27;: 854, &#x27;waters,&#x27;: 855, &#x27;we&#x27;: 856, &#x27;well.&#x27;: 857, &#x27;were&#x27;: 858, &#x27;what&#x27;: 859, &#x27;when&#x27;: 860, &#x27;which&#x27;: 861, &#x27;who&#x27;: 862, &#x27;whole&#x27;: 863, &#x27;whom&#x27;: 864, &#x27;why.&#x27;: 865, &#x27;will&#x27;: 866, &#x27;wish&#x27;: 867, &#x27;with&#x27;: 868, &#x27;within&#x27;: 869, &#x27;without&#x27;: 870, &#x27;witness&#x27;: 871, &#x27;witty&#x27;: 872, &#x27;woman&#x27;: 873, &#x27;women&#x27;: 874, &#x27;wonder&#x27;: 875, &#x27;work&#x27;: 876, &#x27;working&#x27;: 877, &#x27;world&#x27;: 878, &#x27;world.&quot;&#x27;: 879, &#x27;worst&#x27;: 880, &#x27;would&#x27;: 881, &#x27;write&#x27;: 882, &#x27;written&#x27;: 883, &#x27;wrote&#x27;: 884, &#x27;year&#x27;: 885, &#x27;years&#x27;: 886, &#x27;you&#x27;: 887, &#x27;young&#x27;: 888, &#x27;younger&#x27;: 889, &#x27;your&#x27;: 890, &#x27;yours.&lt;br&#x27;: 891}&lt; pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We repeat the process, this time mapping integers to words.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [22]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">idx2word</span> <span
                            class="o">=</span> <span class="p">{</span><span class="n">idx</span><span
                            class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">idx</span><span
                            class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span
                            class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [23]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">idx2word</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[23]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>{0: '"Absolute',
 1: '"Bohlen"-Fan',
 2: '"Brideshead',
 3: '"Candy"?).',
 4: '"City',
 5: '"Dieter',
 6: '"Dieter"',
 7: '"Dragonfly"',
 8: '"I\'ve',
 9: '"Lady."<br&#x27;, 10: &#x27;&quot;london&#x27;, 11: &#x27;&quot;make&#x27;, 12: &#x27;&quot;miss&quot;&#x27;, 13: &#x27;&quot;mr&#x27;, 14: &#x27;&quot;mrs.&quot;&#x27;, 15: &#x27;&quot;actors&quot;&#x27;, 16: &#x27;&quot;dewy-eyed.&quot;&lt;br&#x27;, 17: &#x27;&quot;hey,&#x27;, 18: &#x27;&quot;meanwhile,&quot;)&#x27;, 19: &#x27;&quot;men&quot;&#x27;, 20: &quot;&#x27;where&quot;, 21: &quot;&#x27;em&quot;, 22: &quot;&#x27;round&quot;, 23: &#x27;(backbone&#x27;, 24: &#x27;(barbarella).&#x27;, 25: &#x27;(brit&#x27;, 26: &#x27;(i&#x27;, 27: &#x27;(jeremy&#x27;, 28: &#x27;(remember&#x27;, 29: &#x27;(you&#x27;, 30: &#x27;(as&#x27;, 31: &#x27;(at&#x27;, 32: &#x27;(not&#x27;, 33: &#x27;(the&#x27;, 34: &quot;(there&#x27;s&quot;, 35: &#x27;(they&#x27;, 36: &#x27;(we&#x27;, 37: &#x27;(what&#x27;, 38: &#x27;(when,&#x27;, 39: &#x27;(yes&#x27;, 40: &#x27;-&#x27;, 41: &#x27;.&#x27;, 42: &#x27; /><br&#x27;, 43: &#x27; />Ah,',
 44: '/>And',
 45: '/>But',
 46: '/>Canadian',
 47: '/>David',
 48: '/>First',
 49: '/>Henceforth,',
 50: '/>Joanna',
 51: '/>Journalist',
 52: '/>Nothing',
 53: '/>OK,',
 54: '/>Penelope',
 55: '/>Peter',
 56: '/>Second',
 57: '/>So',
 58: '/>Susan',
 59: '/>Thank',
 60: '/>Third',
 61: '/>To',
 62: '/>When',
 63: '/>Wrong!',
 64: '/>and',
 65: '1-dimensional',
 66: '14',
 67: '1950s',
 68: '20',
 69: '<pad>',
 70: '<br&#x27;, 71: &#x27;a&#x27;, 72: &#x27;after&#x27;, 73: &#x27;alberta&#x27;, 74: &#x27;alison&#x27;, 75: &#x27;alonso&#x27;, 76: &#x27;american&#x27;, 77: &#x27;and&#x27;, 78: &#x27;as&#x27;, 79: &#x27;b.b.e.&#x27;, 80: &#x27;beginners&quot;,&#x27;, 81: &#x27;boarding-school,&#x27;, 82: &#x27;bohlen&quot;&#x27;, 83: &#x27;both&#x27;, 84: &#x27;brennan&#x27;, 85: &#x27;bulimia&#x27;, 86: &#x27;but&#x27;, 87: &#x27;cage&#x27;, 88: &#x27;canadian&#x27;, 89: &#x27;cher&#x27;, 90: &#x27;christmas!)&#x27;, 91: &#x27;christopher&#x27;, 92: &#x27;circus.&lt;br&#x27;, 93: &#x27;city&#x27;, 94: &#x27;city,&#x27;, 95: &#x27;col.&#x27;, 96: &#x27;colin&#x27;, 97: &#x27;colonel&#x27;, 98: &#x27;columbian&#x27;, 99: &#x27;conchita&#x27;, 100: &#x27;constantly&#x27;, 101: &#x27;coppola&#x27;, 102: &#x27;cricket&#x27;, 103: &quot;cricket&#x27;s&quot;, 104: &#x27;davies)&#x27;, 105: &#x27;dawson&#x27;, 106: &#x27;deadwood,&#x27;, 107: &#x27;dean&#x27;, 108: &#x27;depardieu,&#x27;, 109: &quot;don&#x27;t&quot;, 110: &#x27;dragonfly&#x27;, 111: &#x27;emily&#x27;, 112: &#x27;england)&#x27;, 113: &#x27;england.)&#x27;, 114: &#x27;european&#x27;, 115: &#x27;even&#x27;, 116: &#x27;film&#x27;, 117: &#x27;first&#x27;, 118: &#x27;gerard&#x27;, 119: &#x27;german&#x27;, 120: &#x27;giancarlo&#x27;, 121: &#x27;giannini&#x27;, 122: &quot;girls&#x27;&quot;, 123: &#x27;hampshire&#x27;, 124: &#x27;he&#x27;, 125: &#x27;headmistress&#x27;, 126: &#x27;her&#x27;, 127: &#x27;herringbone-tweed,&#x27;, 128: &#x27;hollywood&#x27;, 129: &#x27;home&#x27;, 130: &#x27;however&#x27;, 131: &#x27;i&#x27;, 132: &quot;i&#x27;ll&quot;, 133: &quot;i&#x27;m&quot;, 134: &#x27;ii&#x27;, 135: &#x27;if&#x27;, 136: &#x27;ironside.&#x27;, 137: &#x27;it&#x27;, 138: &quot;it&#x27;s&quot;, 139: &#x27;japanese&#x27;, 140: &#x27;jimmy&#x27;, 141: &#x27;john&#x27;, 142: &#x27;justice&quot;.&#x27;, 143: &#x27;katt&#x27;, 144: &#x27;keith&#x27;, 145: &#x27;klondike&#x27;, 146: &#x27;l.l.&#x27;, 147: &#x27;l.l.&lt;br&#x27;, 148: &#x27;lady&#x27;, 149: &#x27;law&#x27;, 150: &#x27;leading&#x27;, 151: &quot;lies&#x27;.&quot;, 152: &#x27;lohman&#x27;, 153: &quot;lohman&#x27;s.&quot;, 154: &#x27;lohman,&#x27;, 155: &#x27;london,&#x27;, 156: &#x27;lord&#x27;, 157: &#x27;love&#x27;, 158: &#x27;lumley&#x27;, 159: &#x27;madness&#x27;, 160: &#x27;mann&#x27;, 161: &#x27;manor,&#x27;, 162: &#x27;manor.&lt;br&#x27;, 163: &#x27;mare&#x27;, 164: &#x27;maria&#x27;, 165: &#x27;mccallum&#x27;, 166: &#x27;mcinnes&#x27;, 167: &quot;mcinnes&#x27;s&quot;, 168: &#x27;meanwhile&#x27;, 169: &#x27;michael&#x27;, 170: &#x27;miss&#x27;, 171: &#x27;mortimer&#x27;, 172: &#x27;mountains&#x27;, 173: &#x27;mountie&#x27;, 174: &#x27;mr.&#x27;, 175: &#x27;nancherrow&#x27;, 176: &#x27;new&#x27;, 177: &#x27;nicolas&#x27;, 178: &#x27;north&#x27;, 179: &quot;o&#x27;toole&quot;, 180: &#x27;ok-movie.&#x27;, 181: &#x27;okay,&#x27;, 182: &quot;ol&#x27;&quot;, 183: &#x27;our&#x27;, 184: &#x27;phillip&#x27;, 185: &#x27;pilcher&#x27;, 186: &#x27;polonia&#x27;, 187: &#x27;reefer&#x27;, 188: &#x27;revisited,&quot;&#x27;, 189: &#x27;rocky&#x27;, 190: &#x27;roman&#x27;, 191: &quot;she&#x27;s&quot;, 192: &#x27;shea&#x27;, 193: &#x27;so&#x27;, 194: &#x27;spades&quot;&#x27;, 195: &#x27;speaking&#x27;, 196: &#x27;stately&#x27;, 197: &#x27;stewart&#x27;, 198: &#x27;still,&#x27;, 199: &#x27;stockwell&#x27;, 200: &#x27;sunday&#x27;, 201: &#x27;sure,&#x27;, 202: &#x27;susan&#x27;, 203: &#x27;teacups,&#x27;, 204: &#x27;technically&#x27;, 205: &quot;that&#x27;s&quot;, 206: &#x27;the&#x27;, 207: &#x27;there&#x27;, 208: &#x27;they&#x27;, 209: &quot;they&#x27;re&quot;, 210: &#x27;things&#x27;, 211: &#x27;this&#x27;, 212: &#x27;together,&#x27;, 213: &#x27;truth&#x27;, 214: &#x27;us&#x27;, 215: &#x27;venerable&#x27;, 216: &#x27;walken&#x27;, 217: &quot;walken&#x27;s&quot;, 218: &#x27;walter&#x27;, 219: &#x27;war&#x27;, 220: &#x27;well,&#x27;, 221: &#x27;west.&lt;br&#x27;, 222: &#x27;when&#x27;, 223: &#x27;wild&#x27;, 224: &#x27;winningham&#x27;, 225: &#x27;wonderful&#x27;, 226: &#x27;world&#x27;, 227: &#x27;york&#x27;, 228: &#x27;yukon&#x27;, 229: &#x27;a&#x27;, 230: &#x27;ability&#x27;, 231: &#x27;ably&#x27;, 232: &#x27;about.&#x27;, 233: &#x27;absolutely&#x27;, 234: &#x27;acclaimed;&#x27;, 235: &#x27;accord&#x27;, 236: &#x27;accuracy.&#x27;, 237: &#x27;accurate&#x27;, 238: &#x27;achievement,&#x27;, 239: &#x27;act&#x27;, 240: &#x27;acting&#x27;, 241: &#x27;action&#x27;, 242: &#x27;actor&#x27;, 243: &quot;actor&#x27;s&quot;, 244: &#x27;actors&#x27;, 245: &#x27;actors,&#x27;, 246: &quot;actress&#x27;s&quot;, 247: &#x27;actresses&#x27;, 248: &#x27;actually&#x27;, 249: &#x27;admit,&#x27;, 250: &#x27;advice&#x27;, 251: &#x27;advice:&#x27;, 252: &#x27;affair&#x27;, 253: &#x27;after&#x27;, 254: &#x27;afternoon&#x27;, 255: &#x27;age&#x27;, 256: &#x27;age!).&#x27;, 257: &#x27;ago&#x27;, 258: &#x27;ahead&#x27;, 259: &#x27;air&#x27;, 260: &#x27;alive&#x27;, 261: &#x27;all&#x27;, 262: &#x27;all,&#x27;, 263: &#x27;all.&#x27;, 264: &#x27;all.&lt;br&#x27;, 265: &#x27;along.&#x27;, 266: &#x27;alright,&#x27;, 267: &#x27;always&#x27;, 268: &#x27;always)&#x27;, 269: &#x27;am&#x27;, 270: &#x27;amazingly&#x27;, 271: &#x27;amusing&#x27;, 272: &#x27;an&#x27;, 273: &#x27;and&#x27;, 274: &#x27;anguish&#x27;, 275: &#x27;animal&#x27;, 276: &#x27;another!)&lt;br&#x27;, 277: &#x27;another.&#x27;, 278: &#x27;any&#x27;, 279: &#x27;anybody&#x27;, 280: &#x27;anything&#x27;, 281: &#x27;apology&#x27;, 282: &#x27;appear&#x27;, 283: &#x27;appeared&#x27;, 284: &#x27;are&#x27;, 285: &#x27;arm-chair&#x27;, 286: &#x27;around&#x27;, 287: &#x27;around,&#x27;, 288: &#x27;as&#x27;, 289: &#x27;asleep&#x27;, 290: &#x27;aspiring&#x27;, 291: &#x27;astonishing&#x27;, 292: &#x27;at&#x27;, 293: &#x27;autobiography&#x27;, 294: &#x27;award&#x27;, 295: &#x27;baby&#x27;, 296: &#x27;backbone!&lt;br&#x27;, 297: &#x27;backlighting.&#x27;, 298: &#x27;badly-acted,&#x27;, 299: &#x27;barely&#x27;, 300: &#x27;barometers&#x27;, 301: &#x27;based&#x27;, 302: &#x27;battling&#x27;, 303: &#x27;be&#x27;, 304: &#x27;beautiful&#x27;, 305: &#x27;because&#x27;, 306: &#x27;become&#x27;, 307: &#x27;becomes&#x27;, 308: &#x27;been&#x27;, 309: &#x27;before&#x27;, 310: &#x27;behaviour&#x27;, 311: &#x27;being&#x27;, 312: &#x27;best&#x27;, 313: &#x27;best,&#x27;, 314: &#x27;best.&#x27;, 315: &#x27;better&#x27;, 316: &#x27;big&#x27;, 317: &#x27;bit&#x27;, 318: &#x27;blockbuster,&#x27;, 319: &#x27;body&#x27;, 320: &#x27;border&#x27;, 321: &#x27;boring.&#x27;, 322: &#x27;boy&#x27;, 323: &#x27;boy.&#x27;, 324: &#x27;brilliant&#x27;, 325: &#x27;bulimia&#x27;, 326: &#x27;business&#x27;, 327: &#x27;but&#x27;, 328: &#x27;but,&#x27;, 329: &#x27;by&#x27;, 330: &#x27;by,&#x27;, 331: &#x27;called&#x27;, 332: &#x27;cameos&#x27;, 333: &#x27;camps&#x27;, 334: &#x27;can&#x27;, 335: &#x27;can&quot;!&#x27;, 336: &quot;can&#x27;t&quot;, 337: &#x27;cant&#x27;, 338: &#x27;captures&#x27;, 339: &#x27;cases&#x27;, 340: &#x27;cast&#x27;, 341: &#x27;catatonic.&#x27;, 342: &#x27;causes&#x27;, 343: &#x27;causing&#x27;, 344: &#x27;character&#x27;, 345: &#x27;choice&#x27;, 346: &#x27;cinema&#x27;, 347: &#x27;colonel&#x27;, 348: &#x27;combination&#x27;, 349: &#x27;come&#x27;, 350: &#x27;comes&#x27;, 351: &#x27;comfortable&#x27;, 352: &#x27;company&#x27;, 353: &#x27;complete&#x27;, 354: &#x27;compulsive&#x27;, 355: &#x27;concern&#x27;, 356: &#x27;consent&#x27;, 357: &#x27;considerate&#x27;, 358: &#x27;constant.&#x27;, 359: &#x27;contacts?)&lt;br&#x27;, 360: &#x27;content&#x27;, 361: &#x27;continue&#x27;, 362: &#x27;control&#x27;, 363: &#x27;convey&#x27;, 364: &#x27;could&#x27;, 365: &quot;couldn&#x27;t&quot;, 366: &#x27;couple&#x27;, 367: &#x27;courage&#x27;, 368: &#x27;course)&#x27;, 369: &#x27;cover&#x27;, 370: &#x27;cries&#x27;, 371: &#x27;criticize.&#x27;, 372: &#x27;cross,&#x27;, 373: &#x27;crush&#x27;, 374: &#x27;crying,&#x27;, 375: &#x27;dangerous&#x27;, 376: &#x27;dash&#x27;, 377: &#x27;days&#x27;, 378: &#x27;dead&#x27;, 379: &#x27;decide&#x27;, 380: &#x27;deep,&#x27;, 381: &#x27;degree&#x27;, 382: &#x27;depends&#x27;, 383: &#x27;depths&#x27;, 384: &#x27;descend&#x27;, 385: &#x27;deserves&#x27;, 386: &#x27;despair,&#x27;, 387: &#x27;despair.&#x27;, 388: &#x27;desperation&#x27;, 389: &#x27;destroy&#x27;, 390: &#x27;development&#x27;, 391: &#x27;devoid&#x27;, 392: &#x27;did&#x27;, 393: &quot;didn&#x27;t&quot;, 394: &#x27;directed&#x27;, 395: &#x27;director&#x27;, 396: &#x27;disappointed&#x27;, 397: &#x27;disgust.&#x27;, 398: &#x27;disorder.&#x27;, 399: &#x27;do.&#x27;, 400: &#x27;does&#x27;, 401: &quot;don&#x27;t&quot;, 402: &#x27;done,&#x27;, 403: &#x27;done.&lt;br&#x27;, 404: &#x27;due&#x27;, 405: &#x27;dumb&#x27;, 406: &#x27;during&#x27;, 407: &#x27;each&#x27;, 408: &#x27;early&#x27;, 409: &#x27;eaten&#x27;, 410: &#x27;eating&#x27;, 411: &#x27;edge.&#x27;, 412: &#x27;effected&#x27;, 413: &#x27;either&#x27;, 414: &#x27;elect&#x27;, 415: &#x27;else.&#x27;, 416: &#x27;emblazered&#x27;, 417: &#x27;emotional&#x27;, 418: &#x27;emotions&#x27;, 419: &#x27;end,&#x27;, 420: &#x27;enforce&#x27;, 421: &#x27;enjoyable&#x27;, 422: &#x27;enough&#x27;, 423: &#x27;enough.&#x27;, 424: &#x27;ensweatered&#x27;, 425: &#x27;equally&#x27;, 426: &#x27;even&#x27;, 427: &#x27;every&#x27;, 428: &#x27;everything&#x27;, 429: &#x27;exactly&#x27;, 430: &#x27;excellent.&#x27;, 431: &#x27;experiment&#x27;, 432: &#x27;explanation&#x27;, 433: &#x27;exploits&#x27;, 434: &#x27;extreme&#x27;, 435: &#x27;eyes&#x27;, 436: &#x27;fall&#x27;, 437: &#x27;family&#x27;, 438: &#x27;fashion,&#x27;, 439: &#x27;fast&#x27;, 440: &#x27;fathers&#x27;, 441: &#x27;fell&#x27;, 442: &#x27;female&#x27;, 443: &#x27;few&#x27;, 444: &#x27;filled&#x27;, 445: &#x27;film&#x27;, 446: &#x27;film,&#x27;, 447: &#x27;filmmaker&#x27;, 448: &#x27;films&#x27;, 449: &#x27;films,&#x27;, 450: &#x27;films.&#x27;, 451: &#x27;finally:&lt;br&#x27;, 452: &#x27;find&#x27;, 453: &#x27;finds&#x27;, 454: &#x27;finely&#x27;, 455: &#x27;fired.&#x27;, 456: &#x27;first&#x27;, 457: &#x27;folks;&#x27;, 458: &#x27;for&#x27;, 459: &#x27;found&#x27;, 460: &#x27;fourth&#x27;, 461: &#x27;frenzy&#x27;, 462: &#x27;from&#x27;, 463: &#x27;fruits&#x27;, 464: &#x27;full&#x27;, 465: &#x27;gal!)&lt;br&#x27;, 466: &#x27;gave&#x27;, 467: &#x27;gently&#x27;, 468: &#x27;genuine&#x27;, 469: &#x27;get&#x27;, 470: &#x27;gets&#x27;, 471: &#x27;girl&#x27;, 472: &#x27;girl,&#x27;, 473: &#x27;give&#x27;, 474: &#x27;glamourous&#x27;, 475: &#x27;glitzy,&#x27;, 476: &#x27;go&#x27;, 477: &#x27;going&#x27;, 478: &#x27;gold&#x27;, 479: &#x27;good&#x27;, 480: &#x27;goodness&#x27;, 481: &#x27;gorgeous.&#x27;, 482: &#x27;gradations&#x27;, 483: &#x27;graduation.&#x27;, 484: &#x27;great&#x27;, 485: &#x27;guess&#x27;, 486: &#x27;gunfighters&#x27;, 487: &#x27;guy&#x27;, 488: &#x27;had&#x27;, 489: &#x27;happen&#x27;, 490: &#x27;happen,&#x27;, 491: &#x27;happened&#x27;, 492: &#x27;happening&#x27;, 493: &#x27;has&#x27;, 494: &#x27;hated&#x27;, 495: &#x27;have&#x27;, 496: &#x27;have:&lt;br&#x27;, 497: &#x27;having&#x27;, 498: &#x27;head&#x27;, 499: &#x27;her&#x27;, 500: &#x27;here&#x27;, 501: &#x27;highly&#x27;, 502: &#x27;him&#x27;, 503: &#x27;his&#x27;, 504: &#x27;history.&#x27;, 505: &#x27;homage&#x27;, 506: &#x27;home&#x27;, 507: &#x27;hours.&#x27;, 508: &#x27;how&#x27;, 509: &#x27;howl&#x27;, 510: &#x27;humor&#x27;, 511: &#x27;hush-hush&#x27;, 512: &#x27;hypocrisy&#x27;, 513: &#x27;hysteria&#x27;, 514: &#x27;i&#x27;, 515: &#x27;if&#x27;, 516: &#x27;immense&#x27;, 517: &#x27;in&#x27;, 518: &#x27;in,&#x27;, 519: &#x27;including,&#x27;, 520: &#x27;individual&#x27;, 521: &#x27;inside.&#x27;, 522: &#x27;intensity.&#x27;, 523: &#x27;interested&#x27;, 524: &#x27;into&#x27;, 525: &#x27;is&#x27;, 526: &quot;isn&#x27;t&quot;, 527: &#x27;it&#x27;, 528: &quot;it&#x27;s&quot;, 529: &#x27;it.&#x27;, 530: &#x27;its&#x27;, 531: &#x27;job&#x27;, 532: &#x27;just&#x27;, 533: &#x27;killed&#x27;, 534: &#x27;kind&#x27;, 535: &#x27;knowledge&#x27;, 536: &#x27;known&#x27;, 537: &#x27;last,&#x27;, 538: &#x27;lastly,&#x27;, 539: &#x27;later&#x27;, 540: &#x27;law&#x27;, 541: &#x27;least&#x27;, 542: &#x27;let&#x27;, 543: &#x27;libido.&#x27;, 544: &#x27;life&#x27;, 545: &#x27;like&#x27;, 546: &#x27;live&#x27;, 547: &#x27;local&#x27;, 548: &#x27;loss,&#x27;, 549: &#x27;lost&#x27;, 550: &#x27;lot.&#x27;, 551: &#x27;lots&#x27;, 552: &#x27;love&#x27;, 553: &#x27;love.&#x27;, 554: &#x27;loved&#x27;, 555: &#x27;lucky&#x27;, 556: &#x27;ludicrous&#x27;, 557: &#x27;lured&#x27;, 558: &#x27;made&#x27;, 559: &#x27;majority&#x27;, 560: &#x27;make&#x27;, 561: &#x27;making&#x27;, 562: &#x27;man&#x27;, 563: &#x27;marshal&#x27;, 564: &#x27;marshal!)&#x27;, 565: &#x27;marvels&#x27;, 566: &#x27;matter&#x27;, 567: &#x27;may&#x27;, 568: &#x27;me&#x27;, 569: &#x27;meaning.&#x27;, 570: &#x27;meant&#x27;, 571: &#x27;measured&#x27;, 572: &#x27;mellow&#x27;, 573: &#x27;men&#x27;, 574: &#x27;mentioned,&#x27;, 575: &#x27;microscopically&#x27;, 576: &#x27;mind&#x27;, 577: &#x27;mine)&#x27;, 578: &#x27;missed&#x27;, 579: &#x27;mistaken&#x27;, 580: &#x27;moist.&#x27;, 581: &#x27;more.&#x27;, 582: &#x27;most&#x27;, 583: &#x27;mostly&#x27;, 584: &#x27;mother,&#x27;, 585: &#x27;movie&#x27;, 586: &quot;movie&#x27;s&quot;, 587: &#x27;movie.&#x27;, 588: &#x27;movies&#x27;, 589: &#x27;much&#x27;, 590: &#x27;musical&#x27;, 591: &#x27;musician,&#x27;, 592: &#x27;must&#x27;, 593: &#x27;name&#x27;, 594: &#x27;name.&#x27;, 595: &#x27;nature&#x27;, 596: &#x27;never&#x27;, 597: &#x27;nightmare&#x27;, 598: &#x27;nineties&#x27;, 599: &#x27;no&#x27;, 600: &#x27;nor&#x27;, 601: &#x27;nostalgic&#x27;, 602: &#x27;not&#x27;, 603: &#x27;not.&#x27;, 604: &#x27;nothing&#x27;, 605: &#x27;novel.&lt;br&#x27;, 606: &quot;novelist&#x27;s&quot;, 607: &#x27;novels&quot;:&#x27;, 608: &#x27;novels:&#x27;, 609: &#x27;now&#x27;, 610: &#x27;nude&#x27;, 611: &#x27;occasion&#x27;, 612: &#x27;of&#x27;, 613: &#x27;off&#x27;, 614: &#x27;off.&#x27;, 615: &#x27;offensive&#x27;, 616: &#x27;office&#x27;, 617: &#x27;old&#x27;, 618: &#x27;oldest&#x27;, 619: &#x27;on&#x27;, 620: &#x27;on-screen&#x27;, 621: &#x27;once.&#x27;, 622: &#x27;one&#x27;, 623: &quot;one&#x27;s&quot;, 624: &#x27;one-dimensional&#x27;, 625: &#x27;only&#x27;, 626: &#x27;or&#x27;, 627: &#x27;or,&#x27;, 628: &#x27;oscillators&#x27;, 629: &#x27;other&#x27;, 630: &#x27;others&#x27;, 631: &#x27;out&#x27;, 632: &#x27;outdoors&#x27;, 633: &#x27;outside&#x27;, 634: &#x27;outstanding,&#x27;, 635: &#x27;own&#x27;, 636: &#x27;paddle&#x27;, 637: &#x27;paid&#x27;, 638: &#x27;pair&#x27;, 639: &#x27;parents&#x27;, 640: &#x27;part&#x27;, 641: &#x27;past&#x27;, 642: &#x27;pathetic&#x27;, 643: &#x27;people&#x27;, 644: &#x27;people.&#x27;, 645: &#x27;perfect&#x27;, 646: &#x27;performances&#x27;, 647: &#x27;perhaps&#x27;, 648: &#x27;phony,&#x27;, 649: &#x27;photographs&#x27;, 650: &#x27;picture.&#x27;, 651: &#x27;piece.&#x27;, 652: &#x27;play&#x27;, 653: &#x27;played&#x27;, 654: &#x27;plays&#x27;, 655: &#x27;plot&#x27;, 656: &#x27;plot,&#x27;, 657: &#x27;plug&#x27;, 658: &#x27;poignant&#x27;, 659: &#x27;pointlessly&#x27;, 660: &#x27;popular&#x27;, 661: &#x27;portrayal&#x27;, 662: &#x27;position.&#x27;, 663: &#x27;praise&#x27;, 664: &#x27;precise,&#x27;, 665: &#x27;prepared&#x27;, 666: &#x27;pressure&#x27;, 667: &#x27;pressure,&#x27;, 668: &#x27;prime&#x27;, 669: &#x27;prison&#x27;, 670: &#x27;producers&#x27;, 671: &#x27;propaganda&#x27;, 672: &#x27;proud&#x27;, 673: &#x27;pseudo-love&#x27;, 674: &#x27;pursue&#x27;, 675: &#x27;pursued,&#x27;, 676: &#x27;pursuers&#x27;, 677: &#x27;pursuing&#x27;, 678: &#x27;puts&#x27;, 679: &#x27;quite&#x27;, 680: &#x27;range&#x27;, 681: &#x27;rapids&#x27;, 682: &#x27;re-makings&#x27;, 683: &#x27;reaches&#x27;, 684: &#x27;read&#x27;, 685: &#x27;reading&#x27;, 686: &#x27;real&#x27;, 687: &#x27;really&#x27;, 688: &#x27;reason&#x27;, 689: &#x27;rebels&#x27;, 690: &#x27;received.&#x27;, 691: &#x27;recommend&#x27;, 692: &#x27;redeem&#x27;, 693: &#x27;remotely&#x27;, 694: &#x27;resembling&#x27;, 695: &#x27;resonance&#x27;, 696: &#x27;rest&#x27;, 697: &#x27;revolutions.&#x27;, 698: &#x27;ridiculous&#x27;, 699: &#x27;right,&#x27;, 700: &#x27;right?&lt;br&#x27;, 701: &#x27;rightly&#x27;, 702: &#x27;rising&#x27;, 703: &#x27;role&#x27;, 704: &#x27;roles&#x27;, 705: &#x27;romance&#x27;, 706: &#x27;row&#x27;, 707: &#x27;rubbish.&#x27;, 708: &#x27;ruining&#x27;, 709: &#x27;running&#x27;, 710: &#x27;rush.&#x27;, 711: &#x27;said&#x27;, 712: &#x27;same&#x27;, 713: &#x27;say&#x27;, 714: &#x27;says&#x27;, 715: &#x27;scale&#x27;, 716: &#x27;scene&#x27;, 717: &#x27;scenes&#x27;, 718: &#x27;schools,&#x27;, 719: &#x27;sci-fi&#x27;, 720: &#x27;script.&#x27;, 721: &#x27;see&#x27;, 722: &#x27;seeing&#x27;, 723: &#x27;seem&#x27;, 724: &#x27;seemed&#x27;, 725: &#x27;seen&#x27;, 726: &#x27;sense&#x27;, 727: &#x27;sensitive&#x27;, 728: &#x27;serving&#x27;, 729: &#x27;set&#x27;, 730: &#x27;sette&#x27;, 731: &#x27;setting&#x27;, 732: &#x27;sexiest&#x27;, 733: &#x27;sexual&#x27;, 734: &#x27;sexy&#x27;, 735: &#x27;shake&#x27;, 736: &#x27;shall&#x27;, 737: &#x27;sharp&#x27;, 738: &#x27;she&#x27;, 739: &quot;she&#x27;s&quot;, 740: &#x27;shooting&#x27;, 741: &#x27;should&#x27;, 742: &#x27;show&#x27;, 743: &#x27;show.&#x27;, 744: &#x27;shown&#x27;, 745: &#x27;shows&#x27;, 746: &#x27;side&#x27;, 747: &#x27;side.&#x27;, 748: &#x27;sign:&#x27;, 749: &#x27;simply&#x27;, 750: &#x27;sis&#x27;, 751: &#x27;sit&#x27;, 752: &#x27;sixties&#x27;, 753: &#x27;sixties.&#x27;, 754: &#x27;slow&#x27;, 755: &#x27;snowy&#x27;, 756: &#x27;so&#x27;, 757: &#x27;so.&#x27;, 758: &#x27;some&#x27;, 759: &#x27;somehow,&#x27;, 760: &#x27;soporific&#x27;, 761: &#x27;sort.&lt;br&#x27;, 762: &#x27;soul&#x27;, 763: &#x27;speaking&#x27;, 764: &#x27;spectacular&#x27;, 765: &#x27;spelling,&#x27;, 766: &#x27;spirit&#x27;, 767: &#x27;spotlight&#x27;, 768: &#x27;squeeze.&#x27;, 769: &#x27;standard)&#x27;, 770: &#x27;stars&#x27;, 771: &#x27;still&#x27;, 772: &#x27;story&#x27;, 773: &#x27;story,&#x27;, 774: &#x27;storyline.&#x27;, 775: &#x27;streets&#x27;, 776: &#x27;subject.&#x27;, 777: &#x27;such&#x27;, 778: &#x27;suffering&#x27;, 779: &#x27;sum&#x27;, 780: &#x27;sunlight!&quot;&lt;br&#x27;, 781: &#x27;superb&#x27;, 782: &#x27;supporting&#x27;, 783: &#x27;sympathise&#x27;, 784: &#x27;sympathy&#x27;, 785: &#x27;symptoms.&#x27;, 786: &#x27;target&#x27;, 787: &#x27;tea&#x27;, 788: &#x27;teenage&#x27;, 789: &#x27;tell&#x27;, 790: &#x27;telling&#x27;, 791: &#x27;tells&#x27;, 792: &#x27;temp-jobs&#x27;, 793: &#x27;terrible&#x27;, 794: &#x27;than&#x27;, 795: &#x27;that&#x27;, 796: &quot;that&#x27;s&quot;, 797: &#x27;that.&#x27;, 798: &#x27;thats&#x27;, 799: &#x27;the&#x27;, 800: &#x27;their&#x27;, 801: &#x27;them&#x27;, 802: &#x27;them.&#x27;, 803: &#x27;themselves&#x27;, 804: &#x27;themselves,&#x27;, 805: &#x27;then&#x27;, 806: &#x27;there&#x27;, 807: &#x27;these&#x27;, 808: &#x27;they&#x27;, 809: &#x27;things&#x27;, 810: &#x27;this&#x27;, 811: &#x27;this,&#x27;, 812: &#x27;those&#x27;, 813: &#x27;thought&#x27;, 814: &#x27;three&#x27;, 815: &#x27;three.&#x27;, 816: &#x27;through&#x27;, 817: &#x27;time&#x27;, 818: &#x27;tired,&#x27;, 819: &#x27;to&#x27;, 820: &#x27;today,&#x27;, 821: &#x27;took&#x27;, 822: &#x27;tormented&#x27;, 823: &#x27;town?&#x27;, 824: &#x27;toy-boy&#x27;, 825: &#x27;toy-boy,&#x27;, 826: &#x27;trauma,&#x27;, 827: &#x27;true&#x27;, 828: &#x27;tuned&#x27;, 829: &#x27;turned&#x27;, 830: &#x27;two&#x27;, 831: &#x27;type&#x27;, 832: &#x27;ultimate&#x27;, 833: &#x27;unbearable&#x27;, 834: &#x27;up&#x27;, 835: &#x27;up:&#x27;, 836: &#x27;us&#x27;, 837: &#x27;us.&lt;br&#x27;, 838: &#x27;use&#x27;, 839: &#x27;uses&#x27;, 840: &#x27;usually&#x27;, 841: &#x27;version&#x27;, 842: &#x27;very&#x27;, 843: &#x27;vibrating&#x27;, 844: &#x27;viewers&#x27;, 845: &#x27;waiting&#x27;, 846: &#x27;waiting!&#x27;, 847: &#x27;want&#x27;, 848: &#x27;warm&#x27;, 849: &#x27;warn&#x27;, 850: &#x27;warning&#x27;, 851: &#x27;was&#x27;, 852: &#x27;was,&#x27;, 853: &quot;wasn&#x27;t&quot;, 854: &#x27;watched&#x27;, 855: &#x27;waters,&#x27;, 856: &#x27;we&#x27;, 857: &#x27;well.&#x27;, 858: &#x27;were&#x27;, 859: &#x27;what&#x27;, 860: &#x27;when&#x27;, 861: &#x27;which&#x27;, 862: &#x27;who&#x27;, 863: &#x27;whole&#x27;, 864: &#x27;whom&#x27;, 865: &#x27;why.&#x27;, 866: &#x27;will&#x27;, 867: &#x27;wish&#x27;, 868: &#x27;with&#x27;, 869: &#x27;within&#x27;, 870: &#x27;without&#x27;, 871: &#x27;witness&#x27;, 872: &#x27;witty&#x27;, 873: &#x27;woman&#x27;, 874: &#x27;women&#x27;, 875: &#x27;wonder&#x27;, 876: &#x27;work&#x27;, 877: &#x27;working&#x27;, 878: &#x27;world&#x27;, 879: &#x27;world.&quot;&#x27;, 880: &#x27;worst&#x27;, 881: &#x27;would&#x27;, 882: &#x27;write&#x27;, 883: &#x27;written&#x27;, 884: &#x27;wrote&#x27;, 885: &#x27;year&#x27;, 886: &#x27;years&#x27;, 887: &#x27;you&#x27;, 888: &#x27;young&#x27;, 889: &#x27;younger&#x27;, 890: &#x27;your&#x27;, 891: &#x27;yours.&lt;br&#x27;}&lt; pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Now, perform the mapping to encode the observations in our subset. Note the use of <strong><em>nested
                    list comprehensions</em></strong>!</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [24]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X_proc</span> <span
                            class="o">=</span> <span class="p">[[</span><span class="n">word2idx</span><span
                            class="p">[</span><span class="n">word</span><span class="p">]</span> <span
                            class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">review</span><span
                            class="p">]</span> <span class="k">for</span> <span class="n">review</span> <span
                            class="ow">in</span> <span class="n">X</span><span class="p">]</span>
<span class="n">X_proc</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span
                                class="mi">10</span><span class="p">],</span> <span class="n">X_proc</span><span
                                class="p">[</span><span class="mi">1</span><span class="p">][:</span><span
                                class="mi">10</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[24]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>([211, 851, 272, 233, 793, 587, 109, 303, 557, 517],
 [131, 495, 308, 536, 819, 436, 289, 406, 449, 327])</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><code>X_proc</code> is a list of lists but if we are going to feed it into a <code>keras</code> model
                    we should convert both it and <code>y</code> into <code>numpy</code> arrays.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [25]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X_proc</span> <span
                            class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                            class="n">hstack</span><span class="p">(</span><span class="n">X_proc</span><span class="p">)</span><span
                            class="o">.</span><span class="n">reshape</span><span class="p">(</span><span
                            class="o">-</span><span class="mi">1</span><span class="p">,</span> <span
                            class="n">MAX_LEN</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">array</span><span class="p">(</span><span class="n">y</span><span
                                class="p">)</span>
<span class="n">X_proc</span><span class="p">,</span> <span class="n">y</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[25]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>(array([[211, 851, 272, ...,  69,  69,  69],
        [131, 495, 308, ...,  69,  69,  69],
        [160, 649, 799, ...,  69,  69,  69],
        ...,
        [206, 445, 525, ...,  69,  69,  69],
        [131, 687, 552, ...,  69,  69,  69],
        [201, 810, 622, ...,  69,  69,  69]]),
 array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0]))</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Now, just to prove that we've successfully processed the data, we perform a test train split and feed
                    it into a FNN.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [26]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span
                            class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span
                            class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span
                            class="p">(</span><span class="n">X_proc</span><span class="p">,</span> <span
                            class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span
                            class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span
                            class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [27]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">input_dim</span><span
                                class="o">=</span><span class="n">MAX_LEN</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span
                                class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span
                                class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span
                                class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span
                                class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span
                                class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense (Dense)                (None, 8)                 4008
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 9
=================================================================
Total params: 4,017
Trainable params: 4,017
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/5
4/4 - 2s - loss: 187.6442 - accuracy: 0.2500 - val_loss: 149.4720 - val_accuracy: 0.5000
Epoch 2/5
4/4 - 0s - loss: 16.7689 - accuracy: 0.7500 - val_loss: 332.2443 - val_accuracy: 0.5000
Epoch 3/5
4/4 - 0s - loss: 21.6830 - accuracy: 0.7500 - val_loss: 360.5525 - val_accuracy: 0.5000
Epoch 4/5
4/4 - 0s - loss: 11.4073 - accuracy: 0.7500 - val_loss: 362.2109 - val_accuracy: 0.5000
Epoch 5/5
4/4 - 0s - loss: 1.7824e-10 - accuracy: 1.0000 - val_loss: 346.8892 - val_accuracy: 0.5000
Accuracy: 50.00%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>It worked! But our subset was very small so we shouldn't get excited about the results above.<br/>
                </p>
                <p>The IMDB dataset is very popular so <code>keras</code> also includes an alternative method for
                    loading the data. This method can save us a lot of time for many reasons:</p>
                <ul>
                    <li>Cleaned text with less meaningless punctuation</li>
                    <li>Pre-tokenized and numerically encoded</li>
                    <li>Allows us to specify maximum vocabulary size</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [29]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span
                            class="kn">import</span> <span class="n">imdb</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span
                                class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [30]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small</span>
<span class="n">MAX_VOCAB</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">INDEX_FROM</span> <span class="o">=</span> <span class="mi">3</span>   <span class="c1"># word index offset </span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span
                                class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span
                                class="o">=</span> <span class="n">imdb</span><span class="o">.</span><span class="n">load_data</span><span
                                class="p">(</span><span class="n">num_words</span><span class="o">=</span><span
                                class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="n">index_from</span><span class="o">=</span><span
                                class="n">INDEX_FROM</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><code>get_word_index</code> will load a json object we can store in a dictionary. This gives us the
                    word-to-integer mapping.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [31]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">word2idx</span> <span
                            class="o">=</span> <span class="n">imdb</span><span class="o">.</span><span class="n">get_word_index</span><span
                            class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">'imdb_word_index.json'</span><span
                            class="p">)</span>
<span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span
                                class="p">:(</span><span class="n">v</span> <span class="o">+</span> <span class="n">INDEX_FROM</span><span
                                class="p">)</span> <span class="k">for</span> <span class="n">k</span><span
                                class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">word2idx</span><span
                                class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">word2idx</span><span class="p">[</span><span class="s2">"<pad>"</span><span class="p">]</span> <span
                                class="o">=</span> <span class="mi">0</span>
<span class="n">word2idx</span><span class="p">[</span><span class="s2">"<start>"</span><span class="p">]</span> <span
                                class="o">=</span> <span class="mi">1</span>
<span class="n">word2idx</span><span class="p">[</span><span class="s2">"<unk>"</span><span class="p">]</span> <span
                                class="o">=</span> <span class="mi">2</span>
<span class="n">word2idx</span><span class="p">[</span><span class="s2">"<unused>"</span><span class="p">]</span> <span
                                class="o">=</span> <span class="mi">3</span>
<span class="n">word2idx</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[31]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>{'fawn': 34704,
 'tsukino': 52009,
 'nunnery': 52010,
 'sonja': 16819,
 'vani': 63954,
 'woods': 1411,
 'spiders': 16118,
 'hanging': 2348,
 'woody': 2292,
 'trawling': 52011,
 "hold's": 52012,
 'comically': 11310,
 'localized': 40833,
 'disobeying': 30571,
 "'royale": 52013,
 "harpo's": 40834,
 'canet': 52014,
 'aileen': 19316,
 'acurately': 52015,
 "diplomat's": 52016,
 'rickman': 25245,
 'arranged': 6749,
 'rumbustious': 52017,
 'familiarness': 52018,
 "spider'": 52019,
 'hahahah': 68807,
 "wood'": 52020,
 'transvestism': 40836,
 "hangin'": 34705,
 'bringing': 2341,
 'seamier': 40837,
 'wooded': 34706,
 'bravora': 52021,
 'grueling': 16820,
 'wooden': 1639,
 'wednesday': 16821,
 "'prix": 52022,
 'altagracia': 34707,
 'circuitry': 52023,
 'crotch': 11588,
 'busybody': 57769,
 "tart'n'tangy": 52024,
 'burgade': 14132,
 'thrace': 52026,
 "tom's": 11041,
 'snuggles': 52028,
 'francesco': 29117,
 'complainers': 52030,
 'templarios': 52128,
 '272': 40838,
 '273': 52031,
 'zaniacs': 52133,
 '275': 34709,
 'consenting': 27634,
 'snuggled': 40839,
 'inanimate': 15495,
 'uality': 52033,
 'bronte': 11929,
 'errors': 4013,
 'dialogs': 3233,
 "yomada's": 52034,
 "madman's": 34710,
 'dialoge': 30588,
 'usenet': 52036,
 'videodrome': 40840,
 "kid'": 26341,
 'pawed': 52037,
 "'girlfriend'": 30572,
 "'pleasure": 52038,
 "'reloaded'": 52039,
 "kazakos'": 40842,
 'rocque': 52040,
 'mailings': 52041,
 'brainwashed': 11930,
 'mcanally': 16822,
 "tom''": 52042,
 'kurupt': 25246,
 'affiliated': 21908,
 'babaganoosh': 52043,
 "noe's": 40843,
 'quart': 40844,
 'kids': 362,
 'uplifting': 5037,
 'controversy': 7096,
 'kida': 21909,
 'kidd': 23382,
 "error'": 52044,
 'neurologist': 52045,
 'spotty': 18513,
 'cobblers': 30573,
 'projection': 9881,
 'fastforwarding': 40845,
 'sters': 52046,
 "eggar's": 52047,
 'etherything': 52048,
 'gateshead': 40846,
 'airball': 34711,
 'unsinkable': 25247,
 'stern': 7183,
 "cervi's": 52049,
 'dnd': 40847,
 'dna': 11589,
 'insecurity': 20601,
 "'reboot'": 52050,
 'trelkovsky': 11040,
 'jaekel': 52051,
 'sidebars': 52052,
 "sforza's": 52053,
 'distortions': 17636,
 'mutinies': 52054,
 'sermons': 30605,
 '7ft': 40849,
 'boobage': 52055,
 "o'bannon's": 52056,
 'populations': 23383,
 'chulak': 52057,
 'mesmerize': 27636,
 'quinnell': 52058,
 'yahoo': 10310,
 'meteorologist': 52060,
 'beswick': 42580,
 'boorman': 15496,
 'voicework': 40850,
 "ster'": 52061,
 'blustering': 22925,
 'hj': 52062,
 'intake': 27637,
 'morally': 5624,
 'jumbling': 40852,
 'bowersock': 52063,
 "'porky's'": 52064,
 'gershon': 16824,
 'ludicrosity': 40853,
 'coprophilia': 52065,
 'expressively': 40854,
 "india's": 19503,
 "post's": 34713,
 'wana': 52066,
 'wang': 5286,
 'wand': 30574,
 'wane': 25248,
 'edgeways': 52324,
 'titanium': 34714,
 'pinta': 40855,
 'want': 181,
 'pinto': 30575,
 'whoopdedoodles': 52068,
 'tchaikovsky': 21911,
 'travel': 2106,
 "'victory'": 52069,
 'copious': 11931,
 'gouge': 22436,
 "chapters'": 52070,
 'barbra': 6705,
 'uselessness': 30576,
 "wan'": 52071,
 'assimilated': 27638,
 'petiot': 16119,
 'most\x85and': 52072,
 'dinosaurs': 3933,
 'wrong': 355,
 'seda': 52073,
 'stollen': 52074,
 'sentencing': 34715,
 'ouroboros': 40856,
 'assimilates': 40857,
 'colorfully': 40858,
 'glenne': 27639,
 'dongen': 52075,
 'subplots': 4763,
 'kiloton': 52076,
 'chandon': 23384,
 "effect'": 34716,
 'snugly': 27640,
 'kuei': 40859,
 'welcomed': 9095,
 'dishonor': 30074,
 'concurrence': 52078,
 'stoicism': 23385,
 "guys'": 14899,
 "beroemd'": 52080,
 'butcher': 6706,
 "melfi's": 40860,
 'aargh': 30626,
 'playhouse': 20602,
 'wickedly': 11311,
 'fit': 1183,
 'labratory': 52081,
 'lifeline': 40862,
 'screaming': 1930,
 'fix': 4290,
 'cineliterate': 52082,
 'fic': 52083,
 'fia': 52084,
 'fig': 34717,
 'fmvs': 52085,
 'fie': 52086,
 'reentered': 52087,
 'fin': 30577,
 'doctresses': 52088,
 'fil': 52089,
 'zucker': 12609,
 'ached': 31934,
 'counsil': 52091,
 'paterfamilias': 52092,
 'songwriter': 13888,
 'shivam': 34718,
 'hurting': 9657,
 'effects': 302,
 'slauther': 52093,
 "'flame'": 52094,
 'sommerset': 52095,
 'interwhined': 52096,
 'whacking': 27641,
 'bartok': 52097,
 'barton': 8778,
 'frewer': 21912,
 "fi'": 52098,
 'ingrid': 6195,
 'stribor': 30578,
 'approporiately': 52099,
 'wobblyhand': 52100,
 'tantalisingly': 52101,
 'ankylosaurus': 52102,
 'parasites': 17637,
 'childen': 52103,
 "jenkins'": 52104,
 'metafiction': 52105,
 'golem': 17638,
 'indiscretion': 40863,
 "reeves'": 23386,
 "inamorata's": 57784,
 'brittannica': 52107,
 'adapt': 7919,
 "russo's": 30579,
 'guitarists': 48249,
 'abbott': 10556,
 'abbots': 40864,
 'lanisha': 17652,
 'magickal': 40866,
 'mattter': 52108,
 "'willy": 52109,
 'pumpkins': 34719,
 'stuntpeople': 52110,
 'estimate': 30580,
 'ugghhh': 40867,
 'gameplay': 11312,
 "wern't": 52111,
 "n'sync": 40868,
 'sickeningly': 16120,
 'chiara': 40869,
 'disturbed': 4014,
 'portmanteau': 40870,
 'ineffectively': 52112,
 "duchonvey's": 82146,
 "nasty'": 37522,
 'purpose': 1288,
 'lazers': 52115,
 'lightened': 28108,
 'kaliganj': 52116,
 'popularism': 52117,
 "damme's": 18514,
 'stylistics': 30581,
 'mindgaming': 52118,
 'spoilerish': 46452,
 "'corny'": 52120,
 'boerner': 34721,
 'olds': 6795,
 'bakelite': 52121,
 'renovated': 27642,
 'forrester': 27643,
 "lumiere's": 52122,
 'gaskets': 52027,
 'needed': 887,
 'smight': 34722,
 'master': 1300,
 "edie's": 25908,
 'seeber': 40871,
 'hiya': 52123,
 'fuzziness': 52124,
 'genesis': 14900,
 'rewards': 12610,
 'enthrall': 30582,
 "'about": 40872,
 "recollection's": 52125,
 'mutilated': 11042,
 'fatherlands': 52126,
 "fischer's": 52127,
 'positively': 5402,
 '270': 34708,
 'ahmed': 34723,
 'zatoichi': 9839,
 'bannister': 13889,
 'anniversaries': 52130,
 "helm's": 30583,
 "'work'": 52131,
 'exclaimed': 34724,
 "'unfunny'": 52132,
 '274': 52032,
 'feeling': 547,
 "wanda's": 52134,
 'dolan': 33269,
 '278': 52136,
 'peacoat': 52137,
 'brawny': 40873,
 'mishra': 40874,
 'worlders': 40875,
 'protags': 52138,
 'skullcap': 52139,
 'dastagir': 57599,
 'affairs': 5625,
 'wholesome': 7802,
 'hymen': 52140,
 'paramedics': 25249,
 'unpersons': 52141,
 'heavyarms': 52142,
 'affaire': 52143,
 'coulisses': 52144,
 'hymer': 40876,
 'kremlin': 52145,
 'shipments': 30584,
 'pixilated': 52146,
 "'00s": 30585,
 'diminishing': 18515,
 'cinematic': 1360,
 'resonates': 14901,
 'simplify': 40877,
 "nature'": 40878,
 'temptresses': 40879,
 'reverence': 16825,
 'resonated': 19505,
 'dailey': 34725,
 '2\x85': 52147,
 'treize': 27644,
 'majo': 52148,
 'kiya': 21913,
 'woolnough': 52149,
 'thanatos': 39800,
 'sandoval': 35734,
 'dorama': 40882,
 "o'shaughnessy": 52150,
 'tech': 4991,
 'fugitives': 32021,
 'teck': 30586,
 "'e'": 76128,
 'doesn’t': 40884,
 'purged': 52152,
 'saying': 660,
 "martians'": 41098,
 'norliss': 23421,
 'dickey': 27645,
 'dicker': 52155,
 "'sependipity": 52156,
 'padded': 8425,
 'ordell': 57795,
 "sturges'": 40885,
 'independentcritics': 52157,
 'tempted': 5748,
 "atkinson's": 34727,
 'hounded': 25250,
 'apace': 52158,
 'clicked': 15497,
 "'humor'": 30587,
 "martino's": 17180,
 "'supporting": 52159,
 'warmongering': 52035,
 "zemeckis's": 34728,
 'lube': 21914,
 'shocky': 52160,
 'plate': 7479,
 'plata': 40886,
 'sturgess': 40887,
 "nerds'": 40888,
 'plato': 20603,
 'plath': 34729,
 'platt': 40889,
 'mcnab': 52162,
 'clumsiness': 27646,
 'altogether': 3902,
 'massacring': 42587,
 'bicenntinial': 52163,
 'skaal': 40890,
 'droning': 14363,
 'lds': 8779,
 'jaguar': 21915,
 "cale's": 34730,
 'nicely': 1780,
 'mummy': 4591,
 "lot's": 18516,
 'patch': 10089,
 'kerkhof': 50205,
 "leader's": 52164,
 "'movie": 27647,
 'uncomfirmed': 52165,
 'heirloom': 40891,
 'wrangle': 47363,
 'emotion\x85': 52166,
 "'stargate'": 52167,
 'pinoy': 40892,
 'conchatta': 40893,
 'broeke': 41131,
 'advisedly': 40894,
 "barker's": 17639,
 'descours': 52169,
 'lots': 775,
 'lotr': 9262,
 'irs': 9882,
 'lott': 52170,
 'xvi': 40895,
 'irk': 34731,
 'irl': 52171,
 'ira': 6890,
 'belzer': 21916,
 'irc': 52172,
 'ire': 27648,
 'requisites': 40896,
 'discipline': 7696,
 'lyoko': 52964,
 'extend': 11313,
 'nature': 876,
 "'dickie'": 52173,
 'optimist': 40897,
 'lapping': 30589,
 'superficial': 3903,
 'vestment': 52174,
 'extent': 2826,
 'tendons': 52175,
 "heller's": 52176,
 'quagmires': 52177,
 'miyako': 52178,
 'moocow': 20604,
 "coles'": 52179,
 'lookit': 40898,
 'ravenously': 52180,
 'levitating': 40899,
 'perfunctorily': 52181,
 'lookin': 30590,
 "lot'": 40901,
 'lookie': 52182,
 'fearlessly': 34873,
 'libyan': 52184,
 'fondles': 40902,
 'gopher': 35717,
 'wearying': 40904,
 "nz's": 52185,
 'minuses': 27649,
 'puposelessly': 52186,
 'shandling': 52187,
 'decapitates': 31271,
 'humming': 11932,
 "'nother": 40905,
 'smackdown': 21917,
 'underdone': 30591,
 'frf': 40906,
 'triviality': 52188,
 'fro': 25251,
 'bothers': 8780,
 "'kensington": 52189,
 'much': 76,
 'muco': 34733,
 'wiseguy': 22618,
 "richie's": 27651,
 'tonino': 40907,
 'unleavened': 52190,
 'fry': 11590,
 "'tv'": 40908,
 'toning': 40909,
 'obese': 14364,
 'sensationalized': 30592,
 'spiv': 40910,
 'spit': 6262,
 'arkin': 7367,
 'charleton': 21918,
 'jeon': 16826,
 'boardroom': 21919,
 'doubts': 4992,
 'spin': 3087,
 'hepo': 53086,
 'wildcat': 27652,
 'venoms': 10587,
 'misconstrues': 52194,
 'mesmerising': 18517,
 'misconstrued': 40911,
 'rescinds': 52195,
 'prostrate': 52196,
 'majid': 40912,
 'climbed': 16482,
 'canoeing': 34734,
 'majin': 52198,
 'animie': 57807,
 'sylke': 40913,
 'conditioned': 14902,
 'waddell': 40914,
 '3\x85': 52199,
 'hyperdrive': 41191,
 'conditioner': 34735,
 'bricklayer': 53156,
 'hong': 2579,
 'memoriam': 52201,
 'inventively': 30595,
 "levant's": 25252,
 'portobello': 20641,
 'remand': 52203,
 'mummified': 19507,
 'honk': 27653,
 'spews': 19508,
 'visitations': 40915,
 'mummifies': 52204,
 'cavanaugh': 25253,
 'zeon': 23388,
 "jungle's": 40916,
 'viertel': 34736,
 'frenchmen': 27654,
 'torpedoes': 52205,
 'schlessinger': 52206,
 'torpedoed': 34737,
 'blister': 69879,
 'cinefest': 52207,
 'furlough': 34738,
 'mainsequence': 52208,
 'mentors': 40917,
 'academic': 9097,
 'stillness': 20605,
 'academia': 40918,
 'lonelier': 52209,
 'nibby': 52210,
 "losers'": 52211,
 'cineastes': 40919,
 'corporate': 4452,
 'massaging': 40920,
 'bellow': 30596,
 'absurdities': 19509,
 'expetations': 53244,
 'nyfiken': 40921,
 'mehras': 75641,
 'lasse': 52212,
 'visability': 52213,
 'militarily': 33949,
 "elder'": 52214,
 'gainsbourg': 19026,
 'hah': 20606,
 'hai': 13423,
 'haj': 34739,
 'hak': 25254,
 'hal': 4314,
 'ham': 4895,
 'duffer': 53262,
 'haa': 52216,
 'had': 69,
 'advancement': 11933,
 'hag': 16828,
 "hand'": 25255,
 'hay': 13424,
 'mcnamara': 20607,
 "mozart's": 52217,
 'duffel': 30734,
 'haq': 30597,
 'har': 13890,
 'has': 47,
 'hat': 2404,
 'hav': 40922,
 'haw': 30598,
 'figtings': 52218,
 'elders': 15498,
 'underpanted': 52219,
 'pninson': 52220,
 'unequivocally': 27655,
 "barbara's": 23676,
 "bello'": 52222,
 'indicative': 13000,
 'yawnfest': 40923,
 'hexploitation': 52223,
 "loder's": 52224,
 'sleuthing': 27656,
 "justin's": 32625,
 "'ball": 52225,
 "'summer": 52226,
 "'demons'": 34938,
 "mormon's": 52228,
 "laughton's": 34740,
 'debell': 52229,
 'shipyard': 39727,
 'unabashedly': 30600,
 'disks': 40404,
 'crowd': 2293,
 'crowe': 10090,
 "vancouver's": 56437,
 'mosques': 34741,
 'crown': 6630,
 'culpas': 52230,
 'crows': 27657,
 'surrell': 53347,
 'flowless': 52232,
 'sheirk': 52233,
 "'three": 40926,
 "peterson'": 52234,
 'ooverall': 52235,
 'perchance': 40927,
 'bottom': 1324,
 'chabert': 53366,
 'sneha': 52236,
 'inhuman': 13891,
 'ichii': 52237,
 'ursla': 52238,
 'completly': 30601,
 'moviedom': 40928,
 'raddick': 52239,
 'brundage': 51998,
 'brigades': 40929,
 'starring': 1184,
 "'goal'": 52240,
 'caskets': 52241,
 'willcock': 52242,
 "threesome's": 52243,
 "mosque'": 52244,
 "cover's": 52245,
 'spaceships': 17640,
 'anomalous': 40930,
 'ptsd': 27658,
 'shirdan': 52246,
 'obscenity': 21965,
 'lemmings': 30602,
 'duccio': 30603,
 "levene's": 52247,
 "'gorby'": 52248,
 "teenager's": 25258,
 'marshall': 5343,
 'honeymoon': 9098,
 'shoots': 3234,
 'despised': 12261,
 'okabasho': 52249,
 'fabric': 8292,
 'cannavale': 18518,
 'raped': 3540,
 "tutt's": 52250,
 'grasping': 17641,
 'despises': 18519,
 "thief's": 40931,
 'rapes': 8929,
 'raper': 52251,
 "eyre'": 27659,
 'walchek': 52252,
 "elmo's": 23389,
 'perfumes': 40932,
 'spurting': 21921,
 "exposition'\x85": 52253,
 'denoting': 52254,
 'thesaurus': 34743,
 "shoot'": 40933,
 'bonejack': 49762,
 'simpsonian': 52256,
 'hebetude': 30604,
 "hallow's": 34744,
 'desperation\x85': 52257,
 'incinerator': 34745,
 'congratulations': 10311,
 'humbled': 52258,
 "else's": 5927,
 'trelkovski': 40848,
 "rape'": 52259,
 "'chapters'": 59389,
 '1600s': 52260,
 'martian': 7256,
 'nicest': 25259,
 'eyred': 52262,
 'passenger': 9460,
 'disgrace': 6044,
 'moderne': 52263,
 'barrymore': 5123,
 'yankovich': 52264,
 'moderns': 40934,
 'studliest': 52265,
 'bedsheet': 52266,
 'decapitation': 14903,
 'slurring': 52267,
 "'nunsploitation'": 52268,
 "'character'": 34746,
 'cambodia': 9883,
 'rebelious': 52269,
 'pasadena': 27660,
 'crowne': 40935,
 "'bedchamber": 52270,
 'conjectural': 52271,
 'appologize': 52272,
 'halfassing': 52273,
 'paycheque': 57819,
 'palms': 20609,
 "'islands": 52274,
 'hawked': 40936,
 'palme': 21922,
 'conservatively': 40937,
 'larp': 64010,
 'palma': 5561,
 'smelling': 21923,
 'aragorn': 13001,
 'hawker': 52275,
 'hawkes': 52276,
 'explosions': 3978,
 'loren': 8062,
 "pyle's": 52277,
 'shootout': 6707,
 "mike's": 18520,
 "driscoll's": 52278,
 'cogsworth': 40938,
 "britian's": 52279,
 'childs': 34747,
 "portrait's": 52280,
 'chain': 3629,
 'whoever': 2500,
 'puttered': 52281,
 'childe': 52282,
 'maywether': 52283,
 'chair': 3039,
 "rance's": 52284,
 'machu': 34748,
 'ballet': 4520,
 'grapples': 34749,
 'summerize': 76155,
 'freelance': 30606,
 "andrea's": 52286,
 '\x91very': 52287,
 'coolidge': 45882,
 'mache': 18521,
 'balled': 52288,
 'grappled': 40940,
 'macha': 18522,
 'underlining': 21924,
 'macho': 5626,
 'oversight': 19510,
 'machi': 25260,
 'verbally': 11314,
 'tenacious': 21925,
 'windshields': 40941,
 'paychecks': 18560,
 'jerk': 3399,
 "good'": 11934,
 'prancer': 34751,
 'prances': 21926,
 'olympus': 52289,
 'lark': 21927,
 'embark': 10788,
 'gloomy': 7368,
 'jehaan': 52290,
 'turaqui': 52291,
 "child'": 20610,
 'locked': 2897,
 'pranced': 52292,
 'exact': 2591,
 'unattuned': 52293,
 'minute': 786,
 'skewed': 16121,
 'hodgins': 40943,
 'skewer': 34752,
 'think\x85': 52294,
 'rosenstein': 38768,
 'helmit': 52295,
 'wrestlemanias': 34753,
 'hindered': 16829,
 "martha's": 30607,
 'cheree': 52296,
 "pluckin'": 52297,
 'ogles': 40944,
 'heavyweight': 11935,
 'aada': 82193,
 'chopping': 11315,
 'strongboy': 61537,
 'hegemonic': 41345,
 'adorns': 40945,
 'xxth': 41349,
 'nobuhiro': 34754,
 'capitães': 52301,
 'kavogianni': 52302,
 'antwerp': 13425,
 'celebrated': 6541,
 'roarke': 52303,
 'baggins': 40946,
 'cheeseburgers': 31273,
 'matras': 52304,
 "nineties'": 52305,
 "'craig'": 52306,
 'celebrates': 13002,
 'unintentionally': 3386,
 'drafted': 14365,
 'climby': 52307,
 '303': 52308,
 'oldies': 18523,
 'climbs': 9099,
 'honour': 9658,
 'plucking': 34755,
 '305': 30077,
 'address': 5517,
 'menjou': 40947,
 "'freak'": 42595,
 'dwindling': 19511,
 'benson': 9461,
 'white’s': 52310,
 'shamelessness': 40948,
 'impacted': 21928,
 'upatz': 52311,
 'cusack': 3843,
 "flavia's": 37570,
 'effette': 52312,
 'influx': 34756,
 'boooooooo': 52313,
 'dimitrova': 52314,
 'houseman': 13426,
 'bigas': 25262,
 'boylen': 52315,
 'phillipenes': 52316,
 'fakery': 40949,
 "grandpa's": 27661,
 'darnell': 27662,
 'undergone': 19512,
 'handbags': 52318,
 'perished': 21929,
 'pooped': 37781,
 'vigour': 27663,
 'opposed': 3630,
 'etude': 52319,
 "caine's": 11802,
 'doozers': 52320,
 'photojournals': 34757,
 'perishes': 52321,
 'constrains': 34758,
 'migenes': 40951,
 'consoled': 30608,
 'alastair': 16830,
 'wvs': 52322,
 'ooooooh': 52323,
 'approving': 34759,
 'consoles': 40952,
 'disparagement': 52067,
 'futureistic': 52325,
 'rebounding': 52326,
 "'date": 52327,
 'gregoire': 52328,
 'rutherford': 21930,
 'americanised': 34760,
 'novikov': 82199,
 'following': 1045,
 'munroe': 34761,
 "morita'": 52329,
 'christenssen': 52330,
 'oatmeal': 23109,
 'fossey': 25263,
 'livered': 40953,
 'listens': 13003,
 "'marci": 76167,
 "otis's": 52333,
 'thanking': 23390,
 'maude': 16022,
 'extensions': 34762,
 'ameteurish': 52335,
 "commender's": 52336,
 'agricultural': 27664,
 'convincingly': 4521,
 'fueled': 17642,
 'mahattan': 54017,
 "paris's": 40955,
 'vulkan': 52339,
 'stapes': 52340,
 'odysessy': 52341,
 'harmon': 12262,
 'surfing': 4255,
 'halloran': 23497,
 'unbelieveably': 49583,
 "'offed'": 52342,
 'quadrant': 30610,
 'inhabiting': 19513,
 'nebbish': 34763,
 'forebears': 40956,
 'skirmish': 34764,
 'ocassionally': 52343,
 "'resist": 52344,
 'impactful': 21931,
 'spicier': 52345,
 'touristy': 40957,
 "'football'": 52346,
 'webpage': 40958,
 'exurbia': 52348,
 'jucier': 52349,
 'professors': 14904,
 'structuring': 34765,
 'jig': 30611,
 'overlord': 40959,
 'disconnect': 25264,
 'sniffle': 82204,
 'slimeball': 40960,
 'jia': 40961,
 'milked': 16831,
 'banjoes': 40962,
 'jim': 1240,
 'workforces': 52351,
 'jip': 52352,
 'rotweiller': 52353,
 'mundaneness': 34766,
 "'ninja'": 52354,
 "dead'": 11043,
 "cipriani's": 40963,
 'modestly': 20611,
 "professor'": 52355,
 'shacked': 40964,
 'bashful': 34767,
 'sorter': 23391,
 'overpowering': 16123,
 'workmanlike': 18524,
 'henpecked': 27665,
 'sorted': 18525,
 "jōb's": 52357,
 "'always": 52358,
 "'baptists": 34768,
 'dreamcatchers': 52359,
 "'silence'": 52360,
 'hickory': 21932,
 'fun\x97yet': 52361,
 'breakumentary': 52362,
 'didn': 15499,
 'didi': 52363,
 'pealing': 52364,
 'dispite': 40965,
 "italy's": 25265,
 'instability': 21933,
 'quarter': 6542,
 'quartet': 12611,
 'padmé': 52365,
 "'bleedmedry": 52366,
 'pahalniuk': 52367,
 'honduras': 52368,
 'bursting': 10789,
 "pablo's": 41468,
 'irremediably': 52370,
 'presages': 40966,
 'bowlegged': 57835,
 'dalip': 65186,
 'entering': 6263,
 'newsradio': 76175,
 'presaged': 54153,
 "giallo's": 27666,
 'bouyant': 40967,
 'amerterish': 52371,
 'rajni': 18526,
 'leeves': 30613,
 'macauley': 34770,
 'seriously': 615,
 'sugercoma': 52372,
 'grimstead': 52373,
 "'fairy'": 52374,
 'zenda': 30614,
 "'twins'": 52375,
 'realisation': 17643,
 'highsmith': 27667,
 'raunchy': 7820,
 'incentives': 40968,
 'flatson': 52377,
 'snooker': 35100,
 'crazies': 16832,
 'crazier': 14905,
 'grandma': 7097,
 'napunsaktha': 52378,
 'workmanship': 30615,
 'reisner': 52379,
 "sanford's": 61309,
 '\x91doña': 52380,
 'modest': 6111,
 "everything's": 19156,
 'hamer': 40969,
 "couldn't'": 52382,
 'quibble': 13004,
 'socking': 52383,
 'tingler': 21934,
 'gutman': 52384,
 'lachlan': 40970,
 'tableaus': 52385,
 'headbanger': 52386,
 'spoken': 2850,
 'cerebrally': 34771,
 "'road": 23493,
 'tableaux': 21935,
 "proust's": 40971,
 'periodical': 40972,
 "shoveller's": 52388,
 'tamara': 25266,
 'affords': 17644,
 'concert': 3252,
 "yara's": 87958,
 'someome': 52389,
 'lingering': 8427,
 "abraham's": 41514,
 'beesley': 34772,
 'cherbourg': 34773,
 'kagan': 28627,
 'snatch': 9100,
 "miyazaki's": 9263,
 'absorbs': 25267,
 "koltai's": 40973,
 'tingled': 64030,
 'crossroads': 19514,
 'rehab': 16124,
 'falworth': 52392,
 'sequals': 52393,
 ...}</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [32]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">idx2word</span> <span
                            class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span
                            class="n">k</span> <span class="k">for</span> <span class="n">k</span><span
                            class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">word2idx</span><span
                            class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">idx2word</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[32]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>{34704: 'fawn',
 52009: 'tsukino',
 52010: 'nunnery',
 16819: 'sonja',
 63954: 'vani',
 1411: 'woods',
 16118: 'spiders',
 2348: 'hanging',
 2292: 'woody',
 52011: 'trawling',
 52012: "hold's",
 11310: 'comically',
 40833: 'localized',
 30571: 'disobeying',
 52013: "'royale",
 40834: "harpo's",
 52014: 'canet',
 19316: 'aileen',
 52015: 'acurately',
 52016: "diplomat's",
 25245: 'rickman',
 6749: 'arranged',
 52017: 'rumbustious',
 52018: 'familiarness',
 52019: "spider'",
 68807: 'hahahah',
 52020: "wood'",
 40836: 'transvestism',
 34705: "hangin'",
 2341: 'bringing',
 40837: 'seamier',
 34706: 'wooded',
 52021: 'bravora',
 16820: 'grueling',
 1639: 'wooden',
 16821: 'wednesday',
 52022: "'prix",
 34707: 'altagracia',
 52023: 'circuitry',
 11588: 'crotch',
 57769: 'busybody',
 52024: "tart'n'tangy",
 14132: 'burgade',
 52026: 'thrace',
 11041: "tom's",
 52028: 'snuggles',
 29117: 'francesco',
 52030: 'complainers',
 52128: 'templarios',
 40838: '272',
 52031: '273',
 52133: 'zaniacs',
 34709: '275',
 27634: 'consenting',
 40839: 'snuggled',
 15495: 'inanimate',
 52033: 'uality',
 11929: 'bronte',
 4013: 'errors',
 3233: 'dialogs',
 52034: "yomada's",
 34710: "madman's",
 30588: 'dialoge',
 52036: 'usenet',
 40840: 'videodrome',
 26341: "kid'",
 52037: 'pawed',
 30572: "'girlfriend'",
 52038: "'pleasure",
 52039: "'reloaded'",
 40842: "kazakos'",
 52040: 'rocque',
 52041: 'mailings',
 11930: 'brainwashed',
 16822: 'mcanally',
 52042: "tom''",
 25246: 'kurupt',
 21908: 'affiliated',
 52043: 'babaganoosh',
 40843: "noe's",
 40844: 'quart',
 362: 'kids',
 5037: 'uplifting',
 7096: 'controversy',
 21909: 'kida',
 23382: 'kidd',
 52044: "error'",
 52045: 'neurologist',
 18513: 'spotty',
 30573: 'cobblers',
 9881: 'projection',
 40845: 'fastforwarding',
 52046: 'sters',
 52047: "eggar's",
 52048: 'etherything',
 40846: 'gateshead',
 34711: 'airball',
 25247: 'unsinkable',
 7183: 'stern',
 52049: "cervi's",
 40847: 'dnd',
 11589: 'dna',
 20601: 'insecurity',
 52050: "'reboot'",
 11040: 'trelkovsky',
 52051: 'jaekel',
 52052: 'sidebars',
 52053: "sforza's",
 17636: 'distortions',
 52054: 'mutinies',
 30605: 'sermons',
 40849: '7ft',
 52055: 'boobage',
 52056: "o'bannon's",
 23383: 'populations',
 52057: 'chulak',
 27636: 'mesmerize',
 52058: 'quinnell',
 10310: 'yahoo',
 52060: 'meteorologist',
 42580: 'beswick',
 15496: 'boorman',
 40850: 'voicework',
 52061: "ster'",
 22925: 'blustering',
 52062: 'hj',
 27637: 'intake',
 5624: 'morally',
 40852: 'jumbling',
 52063: 'bowersock',
 52064: "'porky's'",
 16824: 'gershon',
 40853: 'ludicrosity',
 52065: 'coprophilia',
 40854: 'expressively',
 19503: "india's",
 34713: "post's",
 52066: 'wana',
 5286: 'wang',
 30574: 'wand',
 25248: 'wane',
 52324: 'edgeways',
 34714: 'titanium',
 40855: 'pinta',
 181: 'want',
 30575: 'pinto',
 52068: 'whoopdedoodles',
 21911: 'tchaikovsky',
 2106: 'travel',
 52069: "'victory'",
 11931: 'copious',
 22436: 'gouge',
 52070: "chapters'",
 6705: 'barbra',
 30576: 'uselessness',
 52071: "wan'",
 27638: 'assimilated',
 16119: 'petiot',
 52072: 'most\x85and',
 3933: 'dinosaurs',
 355: 'wrong',
 52073: 'seda',
 52074: 'stollen',
 34715: 'sentencing',
 40856: 'ouroboros',
 40857: 'assimilates',
 40858: 'colorfully',
 27639: 'glenne',
 52075: 'dongen',
 4763: 'subplots',
 52076: 'kiloton',
 23384: 'chandon',
 34716: "effect'",
 27640: 'snugly',
 40859: 'kuei',
 9095: 'welcomed',
 30074: 'dishonor',
 52078: 'concurrence',
 23385: 'stoicism',
 14899: "guys'",
 52080: "beroemd'",
 6706: 'butcher',
 40860: "melfi's",
 30626: 'aargh',
 20602: 'playhouse',
 11311: 'wickedly',
 1183: 'fit',
 52081: 'labratory',
 40862: 'lifeline',
 1930: 'screaming',
 4290: 'fix',
 52082: 'cineliterate',
 52083: 'fic',
 52084: 'fia',
 34717: 'fig',
 52085: 'fmvs',
 52086: 'fie',
 52087: 'reentered',
 30577: 'fin',
 52088: 'doctresses',
 52089: 'fil',
 12609: 'zucker',
 31934: 'ached',
 52091: 'counsil',
 52092: 'paterfamilias',
 13888: 'songwriter',
 34718: 'shivam',
 9657: 'hurting',
 302: 'effects',
 52093: 'slauther',
 52094: "'flame'",
 52095: 'sommerset',
 52096: 'interwhined',
 27641: 'whacking',
 52097: 'bartok',
 8778: 'barton',
 21912: 'frewer',
 52098: "fi'",
 6195: 'ingrid',
 30578: 'stribor',
 52099: 'approporiately',
 52100: 'wobblyhand',
 52101: 'tantalisingly',
 52102: 'ankylosaurus',
 17637: 'parasites',
 52103: 'childen',
 52104: "jenkins'",
 52105: 'metafiction',
 17638: 'golem',
 40863: 'indiscretion',
 23386: "reeves'",
 57784: "inamorata's",
 52107: 'brittannica',
 7919: 'adapt',
 30579: "russo's",
 48249: 'guitarists',
 10556: 'abbott',
 40864: 'abbots',
 17652: 'lanisha',
 40866: 'magickal',
 52108: 'mattter',
 52109: "'willy",
 34719: 'pumpkins',
 52110: 'stuntpeople',
 30580: 'estimate',
 40867: 'ugghhh',
 11312: 'gameplay',
 52111: "wern't",
 40868: "n'sync",
 16120: 'sickeningly',
 40869: 'chiara',
 4014: 'disturbed',
 40870: 'portmanteau',
 52112: 'ineffectively',
 82146: "duchonvey's",
 37522: "nasty'",
 1288: 'purpose',
 52115: 'lazers',
 28108: 'lightened',
 52116: 'kaliganj',
 52117: 'popularism',
 18514: "damme's",
 30581: 'stylistics',
 52118: 'mindgaming',
 46452: 'spoilerish',
 52120: "'corny'",
 34721: 'boerner',
 6795: 'olds',
 52121: 'bakelite',
 27642: 'renovated',
 27643: 'forrester',
 52122: "lumiere's",
 52027: 'gaskets',
 887: 'needed',
 34722: 'smight',
 1300: 'master',
 25908: "edie's",
 40871: 'seeber',
 52123: 'hiya',
 52124: 'fuzziness',
 14900: 'genesis',
 12610: 'rewards',
 30582: 'enthrall',
 40872: "'about",
 52125: "recollection's",
 11042: 'mutilated',
 52126: 'fatherlands',
 52127: "fischer's",
 5402: 'positively',
 34708: '270',
 34723: 'ahmed',
 9839: 'zatoichi',
 13889: 'bannister',
 52130: 'anniversaries',
 30583: "helm's",
 52131: "'work'",
 34724: 'exclaimed',
 52132: "'unfunny'",
 52032: '274',
 547: 'feeling',
 52134: "wanda's",
 33269: 'dolan',
 52136: '278',
 52137: 'peacoat',
 40873: 'brawny',
 40874: 'mishra',
 40875: 'worlders',
 52138: 'protags',
 52139: 'skullcap',
 57599: 'dastagir',
 5625: 'affairs',
 7802: 'wholesome',
 52140: 'hymen',
 25249: 'paramedics',
 52141: 'unpersons',
 52142: 'heavyarms',
 52143: 'affaire',
 52144: 'coulisses',
 40876: 'hymer',
 52145: 'kremlin',
 30584: 'shipments',
 52146: 'pixilated',
 30585: "'00s",
 18515: 'diminishing',
 1360: 'cinematic',
 14901: 'resonates',
 40877: 'simplify',
 40878: "nature'",
 40879: 'temptresses',
 16825: 'reverence',
 19505: 'resonated',
 34725: 'dailey',
 52147: '2\x85',
 27644: 'treize',
 52148: 'majo',
 21913: 'kiya',
 52149: 'woolnough',
 39800: 'thanatos',
 35734: 'sandoval',
 40882: 'dorama',
 52150: "o'shaughnessy",
 4991: 'tech',
 32021: 'fugitives',
 30586: 'teck',
 76128: "'e'",
 40884: 'doesn’t',
 52152: 'purged',
 660: 'saying',
 41098: "martians'",
 23421: 'norliss',
 27645: 'dickey',
 52155: 'dicker',
 52156: "'sependipity",
 8425: 'padded',
 57795: 'ordell',
 40885: "sturges'",
 52157: 'independentcritics',
 5748: 'tempted',
 34727: "atkinson's",
 25250: 'hounded',
 52158: 'apace',
 15497: 'clicked',
 30587: "'humor'",
 17180: "martino's",
 52159: "'supporting",
 52035: 'warmongering',
 34728: "zemeckis's",
 21914: 'lube',
 52160: 'shocky',
 7479: 'plate',
 40886: 'plata',
 40887: 'sturgess',
 40888: "nerds'",
 20603: 'plato',
 34729: 'plath',
 40889: 'platt',
 52162: 'mcnab',
 27646: 'clumsiness',
 3902: 'altogether',
 42587: 'massacring',
 52163: 'bicenntinial',
 40890: 'skaal',
 14363: 'droning',
 8779: 'lds',
 21915: 'jaguar',
 34730: "cale's",
 1780: 'nicely',
 4591: 'mummy',
 18516: "lot's",
 10089: 'patch',
 50205: 'kerkhof',
 52164: "leader's",
 27647: "'movie",
 52165: 'uncomfirmed',
 40891: 'heirloom',
 47363: 'wrangle',
 52166: 'emotion\x85',
 52167: "'stargate'",
 40892: 'pinoy',
 40893: 'conchatta',
 41131: 'broeke',
 40894: 'advisedly',
 17639: "barker's",
 52169: 'descours',
 775: 'lots',
 9262: 'lotr',
 9882: 'irs',
 52170: 'lott',
 40895: 'xvi',
 34731: 'irk',
 52171: 'irl',
 6890: 'ira',
 21916: 'belzer',
 52172: 'irc',
 27648: 'ire',
 40896: 'requisites',
 7696: 'discipline',
 52964: 'lyoko',
 11313: 'extend',
 876: 'nature',
 52173: "'dickie'",
 40897: 'optimist',
 30589: 'lapping',
 3903: 'superficial',
 52174: 'vestment',
 2826: 'extent',
 52175: 'tendons',
 52176: "heller's",
 52177: 'quagmires',
 52178: 'miyako',
 20604: 'moocow',
 52179: "coles'",
 40898: 'lookit',
 52180: 'ravenously',
 40899: 'levitating',
 52181: 'perfunctorily',
 30590: 'lookin',
 40901: "lot'",
 52182: 'lookie',
 34873: 'fearlessly',
 52184: 'libyan',
 40902: 'fondles',
 35717: 'gopher',
 40904: 'wearying',
 52185: "nz's",
 27649: 'minuses',
 52186: 'puposelessly',
 52187: 'shandling',
 31271: 'decapitates',
 11932: 'humming',
 40905: "'nother",
 21917: 'smackdown',
 30591: 'underdone',
 40906: 'frf',
 52188: 'triviality',
 25251: 'fro',
 8780: 'bothers',
 52189: "'kensington",
 76: 'much',
 34733: 'muco',
 22618: 'wiseguy',
 27651: "richie's",
 40907: 'tonino',
 52190: 'unleavened',
 11590: 'fry',
 40908: "'tv'",
 40909: 'toning',
 14364: 'obese',
 30592: 'sensationalized',
 40910: 'spiv',
 6262: 'spit',
 7367: 'arkin',
 21918: 'charleton',
 16826: 'jeon',
 21919: 'boardroom',
 4992: 'doubts',
 3087: 'spin',
 53086: 'hepo',
 27652: 'wildcat',
 10587: 'venoms',
 52194: 'misconstrues',
 18517: 'mesmerising',
 40911: 'misconstrued',
 52195: 'rescinds',
 52196: 'prostrate',
 40912: 'majid',
 16482: 'climbed',
 34734: 'canoeing',
 52198: 'majin',
 57807: 'animie',
 40913: 'sylke',
 14902: 'conditioned',
 40914: 'waddell',
 52199: '3\x85',
 41191: 'hyperdrive',
 34735: 'conditioner',
 53156: 'bricklayer',
 2579: 'hong',
 52201: 'memoriam',
 30595: 'inventively',
 25252: "levant's",
 20641: 'portobello',
 52203: 'remand',
 19507: 'mummified',
 27653: 'honk',
 19508: 'spews',
 40915: 'visitations',
 52204: 'mummifies',
 25253: 'cavanaugh',
 23388: 'zeon',
 40916: "jungle's",
 34736: 'viertel',
 27654: 'frenchmen',
 52205: 'torpedoes',
 52206: 'schlessinger',
 34737: 'torpedoed',
 69879: 'blister',
 52207: 'cinefest',
 34738: 'furlough',
 52208: 'mainsequence',
 40917: 'mentors',
 9097: 'academic',
 20605: 'stillness',
 40918: 'academia',
 52209: 'lonelier',
 52210: 'nibby',
 52211: "losers'",
 40919: 'cineastes',
 4452: 'corporate',
 40920: 'massaging',
 30596: 'bellow',
 19509: 'absurdities',
 53244: 'expetations',
 40921: 'nyfiken',
 75641: 'mehras',
 52212: 'lasse',
 52213: 'visability',
 33949: 'militarily',
 52214: "elder'",
 19026: 'gainsbourg',
 20606: 'hah',
 13423: 'hai',
 34739: 'haj',
 25254: 'hak',
 4314: 'hal',
 4895: 'ham',
 53262: 'duffer',
 52216: 'haa',
 69: 'had',
 11933: 'advancement',
 16828: 'hag',
 25255: "hand'",
 13424: 'hay',
 20607: 'mcnamara',
 52217: "mozart's",
 30734: 'duffel',
 30597: 'haq',
 13890: 'har',
 47: 'has',
 2404: 'hat',
 40922: 'hav',
 30598: 'haw',
 52218: 'figtings',
 15498: 'elders',
 52219: 'underpanted',
 52220: 'pninson',
 27655: 'unequivocally',
 23676: "barbara's",
 52222: "bello'",
 13000: 'indicative',
 40923: 'yawnfest',
 52223: 'hexploitation',
 52224: "loder's",
 27656: 'sleuthing',
 32625: "justin's",
 52225: "'ball",
 52226: "'summer",
 34938: "'demons'",
 52228: "mormon's",
 34740: "laughton's",
 52229: 'debell',
 39727: 'shipyard',
 30600: 'unabashedly',
 40404: 'disks',
 2293: 'crowd',
 10090: 'crowe',
 56437: "vancouver's",
 34741: 'mosques',
 6630: 'crown',
 52230: 'culpas',
 27657: 'crows',
 53347: 'surrell',
 52232: 'flowless',
 52233: 'sheirk',
 40926: "'three",
 52234: "peterson'",
 52235: 'ooverall',
 40927: 'perchance',
 1324: 'bottom',
 53366: 'chabert',
 52236: 'sneha',
 13891: 'inhuman',
 52237: 'ichii',
 52238: 'ursla',
 30601: 'completly',
 40928: 'moviedom',
 52239: 'raddick',
 51998: 'brundage',
 40929: 'brigades',
 1184: 'starring',
 52240: "'goal'",
 52241: 'caskets',
 52242: 'willcock',
 52243: "threesome's",
 52244: "mosque'",
 52245: "cover's",
 17640: 'spaceships',
 40930: 'anomalous',
 27658: 'ptsd',
 52246: 'shirdan',
 21965: 'obscenity',
 30602: 'lemmings',
 30603: 'duccio',
 52247: "levene's",
 52248: "'gorby'",
 25258: "teenager's",
 5343: 'marshall',
 9098: 'honeymoon',
 3234: 'shoots',
 12261: 'despised',
 52249: 'okabasho',
 8292: 'fabric',
 18518: 'cannavale',
 3540: 'raped',
 52250: "tutt's",
 17641: 'grasping',
 18519: 'despises',
 40931: "thief's",
 8929: 'rapes',
 52251: 'raper',
 27659: "eyre'",
 52252: 'walchek',
 23389: "elmo's",
 40932: 'perfumes',
 21921: 'spurting',
 52253: "exposition'\x85",
 52254: 'denoting',
 34743: 'thesaurus',
 40933: "shoot'",
 49762: 'bonejack',
 52256: 'simpsonian',
 30604: 'hebetude',
 34744: "hallow's",
 52257: 'desperation\x85',
 34745: 'incinerator',
 10311: 'congratulations',
 52258: 'humbled',
 5927: "else's",
 40848: 'trelkovski',
 52259: "rape'",
 59389: "'chapters'",
 52260: '1600s',
 7256: 'martian',
 25259: 'nicest',
 52262: 'eyred',
 9460: 'passenger',
 6044: 'disgrace',
 52263: 'moderne',
 5123: 'barrymore',
 52264: 'yankovich',
 40934: 'moderns',
 52265: 'studliest',
 52266: 'bedsheet',
 14903: 'decapitation',
 52267: 'slurring',
 52268: "'nunsploitation'",
 34746: "'character'",
 9883: 'cambodia',
 52269: 'rebelious',
 27660: 'pasadena',
 40935: 'crowne',
 52270: "'bedchamber",
 52271: 'conjectural',
 52272: 'appologize',
 52273: 'halfassing',
 57819: 'paycheque',
 20609: 'palms',
 52274: "'islands",
 40936: 'hawked',
 21922: 'palme',
 40937: 'conservatively',
 64010: 'larp',
 5561: 'palma',
 21923: 'smelling',
 13001: 'aragorn',
 52275: 'hawker',
 52276: 'hawkes',
 3978: 'explosions',
 8062: 'loren',
 52277: "pyle's",
 6707: 'shootout',
 18520: "mike's",
 52278: "driscoll's",
 40938: 'cogsworth',
 52279: "britian's",
 34747: 'childs',
 52280: "portrait's",
 3629: 'chain',
 2500: 'whoever',
 52281: 'puttered',
 52282: 'childe',
 52283: 'maywether',
 3039: 'chair',
 52284: "rance's",
 34748: 'machu',
 4520: 'ballet',
 34749: 'grapples',
 76155: 'summerize',
 30606: 'freelance',
 52286: "andrea's",
 52287: '\x91very',
 45882: 'coolidge',
 18521: 'mache',
 52288: 'balled',
 40940: 'grappled',
 18522: 'macha',
 21924: 'underlining',
 5626: 'macho',
 19510: 'oversight',
 25260: 'machi',
 11314: 'verbally',
 21925: 'tenacious',
 40941: 'windshields',
 18560: 'paychecks',
 3399: 'jerk',
 11934: "good'",
 34751: 'prancer',
 21926: 'prances',
 52289: 'olympus',
 21927: 'lark',
 10788: 'embark',
 7368: 'gloomy',
 52290: 'jehaan',
 52291: 'turaqui',
 20610: "child'",
 2897: 'locked',
 52292: 'pranced',
 2591: 'exact',
 52293: 'unattuned',
 786: 'minute',
 16121: 'skewed',
 40943: 'hodgins',
 34752: 'skewer',
 52294: 'think\x85',
 38768: 'rosenstein',
 52295: 'helmit',
 34753: 'wrestlemanias',
 16829: 'hindered',
 30607: "martha's",
 52296: 'cheree',
 52297: "pluckin'",
 40944: 'ogles',
 11935: 'heavyweight',
 82193: 'aada',
 11315: 'chopping',
 61537: 'strongboy',
 41345: 'hegemonic',
 40945: 'adorns',
 41349: 'xxth',
 34754: 'nobuhiro',
 52301: 'capitães',
 52302: 'kavogianni',
 13425: 'antwerp',
 6541: 'celebrated',
 52303: 'roarke',
 40946: 'baggins',
 31273: 'cheeseburgers',
 52304: 'matras',
 52305: "nineties'",
 52306: "'craig'",
 13002: 'celebrates',
 3386: 'unintentionally',
 14365: 'drafted',
 52307: 'climby',
 52308: '303',
 18523: 'oldies',
 9099: 'climbs',
 9658: 'honour',
 34755: 'plucking',
 30077: '305',
 5517: 'address',
 40947: 'menjou',
 42595: "'freak'",
 19511: 'dwindling',
 9461: 'benson',
 52310: 'white’s',
 40948: 'shamelessness',
 21928: 'impacted',
 52311: 'upatz',
 3843: 'cusack',
 37570: "flavia's",
 52312: 'effette',
 34756: 'influx',
 52313: 'boooooooo',
 52314: 'dimitrova',
 13426: 'houseman',
 25262: 'bigas',
 52315: 'boylen',
 52316: 'phillipenes',
 40949: 'fakery',
 27661: "grandpa's",
 27662: 'darnell',
 19512: 'undergone',
 52318: 'handbags',
 21929: 'perished',
 37781: 'pooped',
 27663: 'vigour',
 3630: 'opposed',
 52319: 'etude',
 11802: "caine's",
 52320: 'doozers',
 34757: 'photojournals',
 52321: 'perishes',
 34758: 'constrains',
 40951: 'migenes',
 30608: 'consoled',
 16830: 'alastair',
 52322: 'wvs',
 52323: 'ooooooh',
 34759: 'approving',
 40952: 'consoles',
 52067: 'disparagement',
 52325: 'futureistic',
 52326: 'rebounding',
 52327: "'date",
 52328: 'gregoire',
 21930: 'rutherford',
 34760: 'americanised',
 82199: 'novikov',
 1045: 'following',
 34761: 'munroe',
 52329: "morita'",
 52330: 'christenssen',
 23109: 'oatmeal',
 25263: 'fossey',
 40953: 'livered',
 13003: 'listens',
 76167: "'marci",
 52333: "otis's",
 23390: 'thanking',
 16022: 'maude',
 34762: 'extensions',
 52335: 'ameteurish',
 52336: "commender's",
 27664: 'agricultural',
 4521: 'convincingly',
 17642: 'fueled',
 54017: 'mahattan',
 40955: "paris's",
 52339: 'vulkan',
 52340: 'stapes',
 52341: 'odysessy',
 12262: 'harmon',
 4255: 'surfing',
 23497: 'halloran',
 49583: 'unbelieveably',
 52342: "'offed'",
 30610: 'quadrant',
 19513: 'inhabiting',
 34763: 'nebbish',
 40956: 'forebears',
 34764: 'skirmish',
 52343: 'ocassionally',
 52344: "'resist",
 21931: 'impactful',
 52345: 'spicier',
 40957: 'touristy',
 52346: "'football'",
 40958: 'webpage',
 52348: 'exurbia',
 52349: 'jucier',
 14904: 'professors',
 34765: 'structuring',
 30611: 'jig',
 40959: 'overlord',
 25264: 'disconnect',
 82204: 'sniffle',
 40960: 'slimeball',
 40961: 'jia',
 16831: 'milked',
 40962: 'banjoes',
 1240: 'jim',
 52351: 'workforces',
 52352: 'jip',
 52353: 'rotweiller',
 34766: 'mundaneness',
 52354: "'ninja'",
 11043: "dead'",
 40963: "cipriani's",
 20611: 'modestly',
 52355: "professor'",
 40964: 'shacked',
 34767: 'bashful',
 23391: 'sorter',
 16123: 'overpowering',
 18524: 'workmanlike',
 27665: 'henpecked',
 18525: 'sorted',
 52357: "jōb's",
 52358: "'always",
 34768: "'baptists",
 52359: 'dreamcatchers',
 52360: "'silence'",
 21932: 'hickory',
 52361: 'fun\x97yet',
 52362: 'breakumentary',
 15499: 'didn',
 52363: 'didi',
 52364: 'pealing',
 40965: 'dispite',
 25265: "italy's",
 21933: 'instability',
 6542: 'quarter',
 12611: 'quartet',
 52365: 'padmé',
 52366: "'bleedmedry",
 52367: 'pahalniuk',
 52368: 'honduras',
 10789: 'bursting',
 41468: "pablo's",
 52370: 'irremediably',
 40966: 'presages',
 57835: 'bowlegged',
 65186: 'dalip',
 6263: 'entering',
 76175: 'newsradio',
 54153: 'presaged',
 27666: "giallo's",
 40967: 'bouyant',
 52371: 'amerterish',
 18526: 'rajni',
 30613: 'leeves',
 34770: 'macauley',
 615: 'seriously',
 52372: 'sugercoma',
 52373: 'grimstead',
 52374: "'fairy'",
 30614: 'zenda',
 52375: "'twins'",
 17643: 'realisation',
 27667: 'highsmith',
 7820: 'raunchy',
 40968: 'incentives',
 52377: 'flatson',
 35100: 'snooker',
 16832: 'crazies',
 14905: 'crazier',
 7097: 'grandma',
 52378: 'napunsaktha',
 30615: 'workmanship',
 52379: 'reisner',
 61309: "sanford's",
 52380: '\x91doña',
 6111: 'modest',
 19156: "everything's",
 40969: 'hamer',
 52382: "couldn't'",
 13004: 'quibble',
 52383: 'socking',
 21934: 'tingler',
 52384: 'gutman',
 40970: 'lachlan',
 52385: 'tableaus',
 52386: 'headbanger',
 2850: 'spoken',
 34771: 'cerebrally',
 23493: "'road",
 21935: 'tableaux',
 40971: "proust's",
 40972: 'periodical',
 52388: "shoveller's",
 25266: 'tamara',
 17644: 'affords',
 3252: 'concert',
 87958: "yara's",
 52389: 'someome',
 8427: 'lingering',
 41514: "abraham's",
 34772: 'beesley',
 34773: 'cherbourg',
 28627: 'kagan',
 9100: 'snatch',
 9263: "miyazaki's",
 25267: 'absorbs',
 40973: "koltai's",
 64030: 'tingled',
 19514: 'crossroads',
 16124: 'rehab',
 52392: 'falworth',
 52393: 'sequals',
 ...}</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We can see that the text data is already preprocessed for us.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [33]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span
                            class="p">(</span><span class="s1">'Number of reviews'</span><span class="p">,</span> <span
                            class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span
                                class="s1">'Length of first and fifth review before padding'</span><span
                                class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">[</span><span class="mi">0</span><span class="p">])</span> <span
                                class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'First review'</span><span
                                class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span
                                class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'First label'</span><span class="p">,</span> <span
                                class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span
                                class="p">])</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Number of reviews 25000
Length of first and fifth review before padding 218 147
First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
First label 1
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Here's an example review using the index-to-word mapping we created from the loaded JSON file to view
                    the a review in its original form.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [34]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">show_review</span><span
                            class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">review</span> <span class="o">=</span> <span class="s1">' '</span><span class="o">.</span><span
                                class="n">join</span><span class="p">([</span><span class="n">idx2word</span><span
                                class="p">[</span><span class="n">idx</span><span class="p">]</span> <span
                                class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span
                                class="n">x</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">review</span><span class="p">)</span>

<span class="n">show_review</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span
                                class="mi">0</span><span class="p">])</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre><start> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <unk> is an amazing actor and now the same being director <unk> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <unk> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <unk> to the two little boy's that played the <unk> of norman and paul they were just brilliant children are often left out of the <unk> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>The only thing what isn't done for us is the padding. Looking at the distribution of lengths will
                    help us determine what a reasonable length to pad to will be.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [35]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span
                            class="o">.</span><span class="n">hist</span><span class="p">([</span><span
                            class="nb">len</span><span class="p">(</span><span class="n">x</span><span
                            class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span
                            class="ow">in</span> <span class="n">X_train</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'review lengths'</span><span
                                class="p">);</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_png output_subarea">
                        <img src="data:image/png;base64,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
"/>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We saw one way of doing this earlier, but Keras actually has a built in <code>pad_sequences</code>
                    helper function. This handles both padding and truncating. By default padding is added to the <em>beginning</em>
                    of a sequence.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#b3e6ff">
                    <b>Q</b>: Why might we want to truncate? Why might we want to pad from the beginning?
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [36]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span
                            class="kn">import</span> <span class="n">pad_sequences</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [37]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">MAX_LEN</span> <span class="o">=</span> <span
                            class="mi">500</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pad_sequences</span><span
                                class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span
                                class="n">maxlen</span><span class="o">=</span><span class="n">MAX_LEN</span><span
                                class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span
                                class="n">X_test</span><span class="p">,</span> <span class="n">maxlen</span><span
                                class="o">=</span><span class="n">MAX_LEN</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span
                                class="s1">'Length of first and fifth review after padding'</span><span
                                class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">[</span><span class="mi">0</span><span class="p">])</span> <span
                                class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Length of first and fifth review after padding 500 500
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="FFNN"><b>Model 1: Naive Feed-Forward Network</b></div>
                </br></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Let us build a single-layer feed-forward net with a hidden layer of 250 nodes. Each input would be a
                    500-dim vector of tokens since we padded all our sequences to size 500.</p>
                <p><br/></p>
                <div class="exercise" style="background-color:#b3e6ff">
                    <b>Q</b>: How would you calculate the number of parameters in this network?
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [40]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span
                            class="o">=</span><span class="s1">'Naive_FFNN'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">input_dim</span><span
                                class="o">=</span><span class="n">MAX_LEN</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span
                                class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span
                                class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span
                                class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span
                                class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span
                                class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "Naive_FFNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_2 (Dense)              (None, 250)               125250
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 251
=================================================================
Total params: 125,501
Trainable params: 125,501
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/10
196/196 - 1s - loss: 178.4060 - accuracy: 0.4996 - val_loss: 91.7812 - val_accuracy: 0.4996
Epoch 2/10
196/196 - 0s - loss: 48.6640 - accuracy: 0.5822 - val_loss: 48.4361 - val_accuracy: 0.5026
Epoch 3/10
196/196 - 0s - loss: 17.7305 - accuracy: 0.6612 - val_loss: 31.7317 - val_accuracy: 0.5022
Epoch 4/10
196/196 - 0s - loss: 7.5028 - accuracy: 0.7264 - val_loss: 21.0285 - val_accuracy: 0.5017
Epoch 5/10
196/196 - 0s - loss: 3.9465 - accuracy: 0.7623 - val_loss: 15.6753 - val_accuracy: 0.5025
Epoch 6/10
196/196 - 0s - loss: 2.2523 - accuracy: 0.7980 - val_loss: 12.4736 - val_accuracy: 0.5039
Epoch 7/10
196/196 - 0s - loss: 1.4916 - accuracy: 0.8150 - val_loss: 10.7774 - val_accuracy: 0.5057
Epoch 8/10
196/196 - 0s - loss: 1.1314 - accuracy: 0.8334 - val_loss: 9.6000 - val_accuracy: 0.5060
Epoch 9/10
196/196 - 0s - loss: 0.8617 - accuracy: 0.8504 - val_loss: 8.9963 - val_accuracy: 0.5055
Epoch 10/10
196/196 - 0s - loss: 0.7458 - accuracy: 0.8602 - val_loss: 8.7728 - val_accuracy: 0.5083
Accuracy: 50.83%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#b3e6ff">
                    <b>Q</b>: Why was the performance so poor? How could we improve our tokenization?
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="emb"><b>Model 2: Feed-Forward Network /w Embeddings</b></div>
                </br>
                <img src="fig/wordembedding2.png" width="450px"/></p>
                <p>One can view the embedding process as a linear projection from one vector space to another. For NLP,
                    we usually use embeddings to project the sparse one-hot encodings of words on to a lower-dimensional
                    continuous space so that the input surface is 'dense' and possibly smooth. Thus, one can view this
                    embedding layer process as just a transformation from $\mathbb{R}^{inp}$ to $\mathbb{R}^{emb}$</p>
                <p>This not only reduces dimensionality but also allows semantic similarities between tokens to be
                    captured by 'similiarities' between the embedding vectors. This was not possible with one-hot
                    encoding as all vectors there were orthogonal to one another.</p>
                <p><img src="fig/wordembedding.png" width="450px"/></p>
                <p>It is also possible to load pretrained embeddings that were learned from giant corpora. This would be
                    an instance of transfer learning.</p>
                <p>If you are interested in learning more, start with the astromonically impactful papers of <a
                        href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">word2vec</a>
                    and <a href="https://www.aclweb.org/anthology/D14-1162.pdf">GloVe</a>.</p>
                <p>In Keras we use the <a
                        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"><code>Embedding</code></a>
                    layer:</p>
                <pre><code>tf.keras.layers.Embedding(
    input_dim, output_dim, embeddings_initializer='uniform',
    embeddings_regularizer=None, activity_regularizer=None,
    embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs
)</code></pre>
                <p>We'll need to specify the <code>input_dim</code> and <code>output_dim</code>. If working with
                    sequences, as we are, you'll also need to set the <code>input_length</code>.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [42]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">EMBED_DIM</span> <span
                            class="o">=</span> <span class="mi">100</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span
                                class="n">name</span><span class="o">=</span><span class="s1">'FFNN_EMBED'</span><span
                                class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span
                                class="p">(</span><span class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="n">EMBED_DIM</span><span class="p">,</span> <span
                                class="n">input_length</span><span class="o">=</span><span class="n">MAX_LEN</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span
                                class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span
                                class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span
                                class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span
                                class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span
                                class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span
                                class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "FFNN_EMBED"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_1 (Embedding)      (None, 500, 100)          1000000
_________________________________________________________________
flatten_1 (Flatten)          (None, 50000)             0
_________________________________________________________________
dense_6 (Dense)              (None, 250)               12500250
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 251
=================================================================
Total params: 13,500,501
Trainable params: 13,500,501
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/2
196/196 - 6s - loss: 0.6433 - accuracy: 0.6078 - val_loss: 0.3630 - val_accuracy: 0.8497
Epoch 2/2
196/196 - 6s - loss: 0.2349 - accuracy: 0.9025 - val_loss: 0.2977 - val_accuracy: 0.8747
Accuracy: 87.47%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="cnn"><b>Model 3: 1-Dimensional Convolutional Network</b></div>
                </br>
                <img src="fig/1D-CNN.png"/></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Text can be thought of as 1-dimensional sequence (a single, long vector) and we can apply 1D
                    Convolutions over a set of word embeddings.<br/></p>
                <p>More information on convolutions on text data can be found on <a
                        href="http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/">this
                    blog</a>. If you want to learn more, read this <a
                        href="https://www.aclweb.org/anthology/I17-1026.pdf">published and well-cited paper</a> from
                    Eleni's friend, Byron Wallace.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#b3e6ff">
                    <b>Q</b>: Why do we use Conv1D if our input, a sequence of word embeddings, is 2D?
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [43]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span
                            class="o">=</span><span class="s1">'1D_CNN'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span
                                class="p">(</span><span class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="n">EMBED_DIM</span><span class="p">,</span> <span
                                class="n">input_length</span><span class="o">=</span><span class="n">MAX_LEN</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span
                                class="p">(</span><span class="n">filters</span><span class="o">=</span><span
                                class="mi">200</span><span class="p">,</span> <span class="n">kernel_size</span><span
                                class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span
                                class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span
                                class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool1D</span><span
                                class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span
                                class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span
                                class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span
                                class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span
                                class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "1D_CNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_2 (Embedding)      (None, 500, 100)          1000000
_________________________________________________________________
conv1d (Conv1D)              (None, 500, 200)          60200
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 250, 200)          0
_________________________________________________________________
flatten_2 (Flatten)          (None, 50000)             0
_________________________________________________________________
dense_8 (Dense)              (None, 250)               12500250
_________________________________________________________________
dense_9 (Dense)              (None, 1)                 251
=================================================================
Total params: 13,560,701
Trainable params: 13,560,701
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/2
196/196 [==============================] - 9s 34ms/step - loss: 0.5958 - accuracy: 0.6403
Epoch 2/2
196/196 [==============================] - 7s 34ms/step - loss: 0.1796 - accuracy: 0.9358
Accuracy: 88.69%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="rnn"><b>Model 4: Simple RNN</b></div>
                </br>
                <img src="fig/simplernn.png" width="300px"/></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>At a high-level, an RNN is similar to a feed-forward neural network (FFNN) in that there is an input
                    layer, a hidden layer, and an output layer. The input layer is fully connected to the hidden layer,
                    and the hidden layer is fully connected to the output layer. However, the crux of what makes it a
                    <strong>recurrent</strong> neural network is that the hidden layer for a given time <em>t</em> is
                    not only based on the input layer at time <em>t</em> but also the hidden layer from time
                    <em>t-1</em>.</p>
                <p>Here's a popular blog post on <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The
                    Unreasonable Effectiveness of Recurrent Neural Networks</a>.</p>
                <p>In Keras, the vanilla RNN unit is implemented the<code>SimpleRNN</code> layer:</p>
                <pre><code>tf.keras.layers.SimpleRNN(
    units, activation='tanh', use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros', kernel_regularizer=None,
    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,
    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,
    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,
    go_backwards=False, stateful=False, unroll=False, **kwargs
)</code></pre>
                <p>As you can see, recurrent layers in Keras take many arguments. We only need to be concerned with
                    <code>units</code>, which specifies the size of the hidden state, and <code>return_sequences</code>,
                    which will be discussed shortly. For the moment is it fine to leave this set to the default of
                    <code>False</code>.</p>
                <p>Due to the limitations of the vanilla RNN unit (more on that next) it tends not to be used much in
                    practice. For this reason it seems that the Keras developers neglected to implement GPU acceleration
                    for this layer! Notice how much slower the trainig is even for a network with far fewer
                    parameters.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [45]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span
                            class="o">=</span><span class="s1">'SimpleRNN'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span
                                class="p">(</span><span class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="n">EMBED_DIM</span><span class="p">,</span> <span
                                class="n">input_length</span><span class="o">=</span><span class="n">MAX_LEN</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">SimpleRNN</span><span
                                class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span
                                class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span
                                class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "SimpleRNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_3 (Embedding)      (None, 500, 100)          1000000
_________________________________________________________________
simple_rnn (SimpleRNN)       (None, 100)               20100
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 101
=================================================================
Total params: 1,020,201
Trainable params: 1,020,201
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/3
196/196 [==============================] - 53s 267ms/step - loss: 0.6720 - accuracy: 0.5660
Epoch 2/3
196/196 [==============================] - 52s 266ms/step - loss: 0.5283 - accuracy: 0.7444
Epoch 3/3
196/196 [==============================] - 52s 265ms/step - loss: 0.3406 - accuracy: 0.8588
Accuracy: 83.13%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="vanish"><b>Vanishing/Exploding Gradients</b></div>
                </br></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><img src="fig/backprop.png" width="500px"/>
                    <br/></p>
                <p>We need to backpropogate through every time step to calculate the gradients used for our weight
                    updates.</p>
                <p>This requires the use of the chain rule which amounts to repeated multiplications.</p>
                <p>This can cause two types of problems. First, this product can quickly 'explode,' becoming large,
                    causing destructive updates to the model and numerical overflow. One hack to solve this problem is
                    to <strong>clip</strong> the gradient at some threshold.</p>
                <p>Alternatively, the gradient can 'vanish,' getting smaller and smaller as the gradient moves backwards
                    in time. Gradient clipping will not help us here. If we can't propogate gradients suffuciently far
                    back in time then our network will be unable to learn long temporal dependencies. This problem
                    motivates the architecture of the GRU and LSTM units as substitutes for the 'vanilla' RNN.</p>
                <p>For a more detailed look at the vanishing/exploding gradient problem, please see <a
                        href="https://edstem.org/us/courses/3773/lessons/11753/slides/56629">Marios's excellent Advanced
                    Section</a>.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="gru"><b>Model 5: GRU</b></div>
                </br>
                <img src="fig/gru.png" width="800px"/></p>
                <p>$X_{t}$: input<br/>
                    $U$, $V$, and $\beta$: parameter matrices and vector<br/>
                    $\tilde{h_t}$: candidate activation vector<br/>
                    $h_{t}$: output vector<br/>
                    $R_t$: reset gate<br/>
                    $Z_t$: update gate<br/></p>
                <p>The gates of the GRU allow for the gradients to flow more freely to previous time steps, helping to
                    mitigate the vanishing gradient problem.</p>
                <p>In Keras, the <a
                        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU"><code>GRU</code></a> layer
                    is used in exactly the same way as the <code>SimpleRNN</code> layer.</p>
                <pre><code>tf.keras.layers.GRU(
    units, activation='tanh', recurrent_activation='sigmoid',
    use_bias=True, kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros', kernel_regularizer=None,
    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,
    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,
    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,
    go_backwards=False, stateful=False, unroll=False, time_major=False,
    reset_after=True, **kwargs
)</code></pre>
                <p>Here we just swap it in to the previous architecture. Note how much faster it trains with GPU
                    excelleration than the simple RNN!</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [48]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span
                            class="o">=</span><span class="s1">'GRU'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span
                                class="p">(</span><span class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="n">EMBED_DIM</span><span class="p">,</span> <span
                                class="n">input_length</span><span class="o">=</span><span class="n">MAX_LEN</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">GRU</span><span
                                class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span
                                class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span
                                class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "GRU"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_6 (Embedding)      (None, 500, 100)          1000000
_________________________________________________________________
gru_1 (GRU)                  (None, 100)               60600
_________________________________________________________________
dense_13 (Dense)             (None, 1)                 101
=================================================================
Total params: 1,060,701
Trainable params: 1,060,701
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/3
391/391 [==============================] - 13s 30ms/step - loss: 0.5626 - accuracy: 0.6781
Epoch 2/3
391/391 [==============================] - 12s 30ms/step - loss: 0.2510 - accuracy: 0.9011
Epoch 3/3
391/391 [==============================] - 12s 30ms/step - loss: 0.1757 - accuracy: 0.9349
Accuracy: 88.02%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="lstm"><b>Model 6: LSTM</b></div>
                </br>
                <img src="fig/lstm.png" width="600px"/></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>The LSTM lacks the GRU's 'short cut' connection (see GRU's $h_t$ above).</p>
                <p>The LSTM also has a distinct 'cell state' in addition to the hidden state.</p>
                <p>Futher reading:</p>
                <ul>
                    <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM
                        Networks</a></li>
                    <li><a href="https://arxiv.org/abs/1503.04069">LSTM: A Search Space Odyssey</a></li>
                    <li><a href="http://proceedings.mlr.press/v37/jozefowicz15.pdf">An Empirical Exploration of
                        Recurrent Network Architectures</a></li>
                </ul>
                <p>Again, Kera's <a
                        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>LSTM</code></a>
                    works like all the other recurrent layers.</p>
                <pre><code>tf.keras.layers.LSTM(
    units, activation='tanh', recurrent_activation='sigmoid',
    use_bias=True, kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros', unit_forget_bias=True,
    kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,
    activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None,
    bias_constraint=None, dropout=0.0, recurrent_dropout=0.0,
    return_sequences=False, return_state=False, go_backwards=False, stateful=False,
    time_major=False, unroll=False, **kwargs
)</code></pre>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [47]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span
                            class="o">=</span><span class="s1">'LSTM'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span
                                class="p">(</span><span class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="n">EMBED_DIM</span><span class="p">,</span> <span
                                class="n">input_length</span><span class="o">=</span><span class="n">MAX_LEN</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span
                                class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                                class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span
                                class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span
                                class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span
                                class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "LSTM"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_5 (Embedding)      (None, 500, 100)          1000000
_________________________________________________________________
lstm (LSTM)                  (None, 100)               80400
_________________________________________________________________
dense_12 (Dense)             (None, 1)                 101
=================================================================
Total params: 1,080,501
Trainable params: 1,080,501
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/3
391/391 [==============================] - 14s 33ms/step - loss: 0.5209 - accuracy: 0.7265
Epoch 2/3
391/391 [==============================] - 13s 33ms/step - loss: 0.3275 - accuracy: 0.8671
Epoch 3/3
391/391 [==============================] - 13s 33ms/step - loss: 0.2021 - accuracy: 0.9268
Accuracy: 86.39%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="bidir"><b>BiDirectional Layer</b></div>
                </br>
                <img src="fig/birnn1.png" width="600px"/></p>
                <p>We may want our model to learn dependencies in either direction. A <strong>BiDirectional RNN</strong>
                    consists of two separate recurrent units. One processing the sequence from left to right, the other
                    processes that same sequence but in reverse, from right to left. The output of the two units are
                    then merged together (typically concatenated) and feed to the next layer of the network.<br/></p>
                <p>Creating a Bidirection RNN in Keras is quite simple. We just 'wrap' a recurrent layer in the <a
                        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"><code>Bidirectional</code></a>
                    layer. The default behavior is to concatenate the output from each direction.</p>
                <pre><code>tf.keras.layers.Bidirectional(
    layer, merge_mode='concat', weights=None, backward_layer=None,
    **kwargs
)</code></pre>
                <p>Example:</p>
                <pre><code>model = Sequential()
...
model.add(Bidirectional(SimpleRNN(n_nodes))
...</code></pre>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="deep"><b>Deep RNNs</b></div>
                </br>
                <img src="fig/deeprnn.png" width="600px"/></p>
                <p>We may want to stack RNN layers one after another. But there is a problem. A recurrent layer expects
                    to be given a sequence as input, and yet we can see that the recurrent layer in each of our models
                    above outputs a single vector. This is because the default behavior of Keras's recurrent layers is
                    to suppress the output until the final time step. If we want to have two recurrent units in a row
                    then the first will have to given an output after each time step, thus providing a sequence to the
                    2nd recurrent layer.</p>
                <p>We can have our recurrent layers output at each time step setting <code>return_sequences=True</code>.<br/>
                    Example:</p>
                <pre><code>model = Sequential()
...
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100)
...</code></pre>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="timedist"><b>TimeDistributed Layer</b></div>
                </br></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"><code>TimeDistributed</code></a>
                    is a 'wrapper' that applies a layer to all time steps of an input sequence.</p>
                <pre><code>tf.keras.layers.TimeDistributed(
    layer, **kwargs
)</code></pre>
                <p>We use <code>TimeDistributed</code> when we want to input a sequence into a layer that doesn't
                    normally expect a time dimension, such as <code>Dense</code>.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [146]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span
                                class="p">(</span><span class="n">Dense</span><span class="p">(</span><span
                                class="mi">8</span><span class="p">),</span> <span class="n">input_shape</span><span
                                class="o">=</span><span class="p">(</span><span class="mi">3</span><span
                                class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="n">input_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">random</span><span class="o">.</span><span class="n">randint</span><span
                                class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span
                                class="o">=</span><span class="p">(</span><span class="mi">1</span><span
                                class="p">,</span><span class="mi">3</span><span class="p">,</span><span
                                class="mi">5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Shape of input : "</span><span class="p">,</span> <span
                                class="n">input_array</span><span class="o">.</span><span class="n">shape</span><span
                                class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="s1">'rmsprop'</span><span class="p">,</span> <span class="s1">'mse'</span><span
                                class="p">)</span>
<span class="n">output_array</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">predict</span><span class="p">(</span><span class="n">input_array</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Shape of output : "</span><span
                                class="p">,</span> <span class="n">output_array</span><span class="o">.</span><span
                                class="n">shape</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Shape of input :  (1, 3, 5)
Shape of output :  (1, 3, 8)
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="repeatvec"><b>RepeatVector Layer</b></div>
                </br></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"><code>RepeatVector</code></a>
                    repeats the vector a specified number of times. Dimension changes from <br/>
                    (batch_size, number_of_elements)<br/>
                    to<br/>
                    (batch_size, number_of_repetitions, number_of_elements)</p>
                <p>This effectively generates a sequence from a single input.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [88]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_dim</span><span
                                class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RepeatVector</span><span
                                class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_37 (Dense)             (None, 2)                 4
_________________________________________________________________
repeat_vector_5 (RepeatVecto (None, 3, 2)              0
=================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
_________________________________________________________________
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="cnnrnn"><b>Model 7: CNN + RNN</b></div>
                </br></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>CNNs are good at learning spatial features, and sentences can be thought of as 1-D spatial vectors
                    (dimensionality is determined by the number of words in the sentence). We can then take the features
                    learned by the CNN (after a maxpooling layer) and feed them into an RNN! We expect the CNN to be
                    able to pick out invariant features across the 1-D spatial structure (i.e., sentence) that
                    characterize good and bad sentiment. This learned spatial features may then be learned as sequences
                    by a reccurent layer. The classification step is then performed by a final dense layer.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#F5E4C3">
                    <b>Exercise:</b> Build a CNN + Deep, BiDirectional GRU Model
                </div>
                <p>Let's put together everything we've learned so far.<br/>
                    Create a network with:</p>
                <ul>
                    <li>word embeddings in a 100-dimensional space</li>
                    <li>conv layer with 32 filters, kernels of width 3, 'same' padding, and ReLU activate</li>
                    <li>max pooling of size 2</li>
                    <li>2 bidirectional GRU layers, each with 50 units <em>per direction</em></li>
                    <li>dense output layer for binary classification</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [39]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span
                            class="o">=</span><span class="s1">'CNN_GRU'</span><span class="p">)</span>
<span class="c1"># your code here</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span
                                class="p">(</span><span class="n">MAX_VOCAB</span><span class="p">,</span> <span
                                class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span
                                class="o">=</span><span class="n">MAX_LEN</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span
                                class="p">(</span><span class="n">filters</span><span class="o">=</span><span
                                class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span
                                class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span
                                class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span
                                class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span
                                class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool1D</span><span
                                class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span
                                class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span
                                class="p">(</span><span class="n">GRU</span><span class="p">(</span><span
                                class="mi">50</span><span class="p">,</span> <span
                                class="n">return_sequences</span><span class="o">=</span><span
                                class="kc">True</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span
                                class="p">(</span><span class="n">GRU</span><span class="p">(</span><span
                                class="mi">50</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [40]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span
                            class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                            class="n">loss</span><span class="o">=</span><span
                            class="s1">'binary_crossentropy'</span><span class="p">,</span> <span
                            class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span
                            class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span
                            class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span
                                class="n">summary</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span
                                class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span
                                class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span
                                class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span
                                class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span
                                class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span
                                class="si">%.2f%%</span><span class="s2">"</span> <span class="o">%</span> <span
                                class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span
                                class="p">]</span><span class="o">*</span><span class="mi">100</span><span
                                class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "CNN_GRU"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        (None, 500, 100)          1000000
_________________________________________________________________
conv1d (Conv1D)              (None, 500, 32)           9632
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 250, 32)           0
_________________________________________________________________
bidirectional (Bidirectional (None, 250, 100)          25200
_________________________________________________________________
bidirectional_1 (Bidirection (None, 100)               45600
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 101
=================================================================
Total params: 1,080,533
Trainable params: 1,080,533
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/3
391/391 [==============================] - 27s 43ms/step - loss: 0.5076 - accuracy: 0.7144
Epoch 2/3
391/391 [==============================] - 17s 43ms/step - loss: 0.1790 - accuracy: 0.9341
Epoch 3/3
391/391 [==============================] - 17s 43ms/step - loss: 0.1019 - accuracy: 0.9653
Accuracy: 87.90%
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>What is the <em>worst</em> movie review in the test set according to your model? 🍅</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [41]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span
                            class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span
                            class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">worst_review</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span
                                class="n">preds</span><span class="o">.</span><span class="n">argmin</span><span
                                class="p">()]</span>
<span class="n">show_review</span><span class="p">(</span><span class="n">worst_review</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre><pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <start> steven seagal has made a really dull bad and boring movie steven seagal plays a doctor this movie has got a few action scenes but they are poorly directed and have nothing to do with the rest of the movie a group of american nazis spread a lethal virus which is able to wipe out the state of montana wesley seagal s character tries desperately to find a cure and that is the story of the <unk> the <unk> is an extremely boring film because nothing happens it is filled with boring dialogue and illogical gaps between events and stupid actors steven seagal has totally up in this movie and i would not recommend this <unk> to my worst enemy 3 10
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>What is the <em>best</em> movie review in the test set according to your model? 🏆</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [78]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">best_review</span> <span
                            class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">preds</span><span
                            class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
<span class="n">show_review</span><span class="p">(</span><span class="n">best_review</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre><unk> village with their artist uncle john <unk> after the death of their parents <unk> sister and john's brother simon has given up trying to convince john to allow he and susan to take care of the children and have <unk> to using private detectives to catch him in either <unk> behavior or unemployed and therefore unable to care for the children properly susan finally decides to take matters into her own hands and goes to <unk> village herself posing as an actress to try to gain information and or <unk> him to see reason what she discovers however is that she not only likes the free and artistic lifestyle john and his friends are living and that the girls are being brought up well but that she is quickly falling in love with john inevitably her true identity is discovered and she is faced with the task of convincing everyone on both sides of the custody debate who should belong with whom br br i really enjoyed this film and found that its very short running time 70 minutes was the perfect length to spin this simple but endearing story <unk> hopkins one of the great 1930's 1940's actresses is delightful in this film her energy style and wholesome beauty really lend themselves to creating an endearing character even though you know that she's pulling a fast one on the people she quickly befriends this is the earliest film i've seen ray <unk> in and he was actually young and non <unk> looking and apparently three years younger than his co star his energy and <unk> manner in wise girl were a refreshing change to the demeanor he affects in his usual darker films honestly though i am usually not remotely a fan of child actors i really enjoyed the two young girls who played <unk> <unk> they were <unk> <unk> and were really the <unk> of the film unfortunately i can't dig up any other films that either of them were subsequently in after this one which is a shame since both <unk> a large amount of natural talent br br wise girl was a film that was made three years after the hollywood code was and to some extent this was <unk> clear by the quick happy ending and the pie in the sky and ease with which the characters lived the alleged <unk> co <unk> was in fact a gorgeous <unk> de <unk> where the artists lived for free or for trade and everything is tied up very nicely throughout fortunately this was a light enough film and the characters were charming enough to make <unk> for its <unk> and short <unk> and i was able to just take wise girl for what it was a good old fashioned love story that was as entertaining as it was endearing unfortunately films of the romantic comedy drama genre today are considerably less intelligent and entertaining or i wouldn't find myself continuously returning to the classics 7 10
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#F5E4C3">
                    <b>End of Exercise:</b> Please return to the main room
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <h2 id="Heavy-Metal-Lyric-Generator">Heavy Metal Lyric Generator
                    <div id="metal"></div>
                    <a class="anchor-link" href="#Heavy-Metal-Lyric-Generator">¶</a></h2>
                <p><img src="fig/many2manyNN.png" width="400px"/></p>
                <p>Here we'll design an RNN to generate song lyrics character by character!</p>
                <p>The model will take in a sequences of 40 character 'windows' of text and predict the most probable
                    next character. This new character is then appended to the original sequence, the first character is
                    dropped, and this new sequene is fed back into the model. We can repeat this process for as long as
                    we like to generate output of arbitrary length.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [89]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">metal_df</span> <span
                            class="o">=</span> <span class="n">pd</span><span class="o">.</span><span
                            class="n">read_csv</span><span class="p">(</span><span
                            class="s1">'data/metal_lyrics_PG.csv'</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [90]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">metal_df</span><span class="o">.</span><span
                            class="n">shape</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[90]:</div>
                    <div class="output_text output_subarea output_execute_result">
                        <pre>(4785, 4)</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>How to we know these are heavy metal lyrics?</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [91]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">metal_df</span><span class="p">[</span><span
                            class="n">metal_df</span><span class="o">.</span><span class="n">lyrics</span><span
                            class="o">.</span><span class="n">str</span><span class="o">.</span><span
                            class="n">contains</span><span class="p">(</span><span class="s1">'elves'</span><span
                            class="p">)]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[91]:</div>
                    <div class="output_html rendered_html output_subarea output_execute_result">
                        <div>
                            <style scoped="">
                                .dataframe tbody tr th:only-of-type {
                                    vertical-align: middle;
                                }

                                .dataframe tbody tr th {
                                    vertical-align: top;
                                }

                                .dataframe thead th {
                                    text-align: right;
                                }
                            </style>
                            <table border="1" class="dataframe">
                                <thead>
                                <tr style="text-align: right;">
                                    <th></th>
                                    <th>song</th>
                                    <th>year</th>
                                    <th>artist</th>
                                    <th>lyrics</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                    <th>116</th>
                                    <td>vinvm-sabbati</td>
                                    <td>2001</td>
                                    <td>behemoth</td>
                                    <td>waters running down\nby the silver moon rays\n...</td>
                                </tr>
                                <tr>
                                    <th>197</th>
                                    <td>gaya-s-dream</td>
                                    <td>1993</td>
                                    <td>the-gathering</td>
                                    <td>open the gates of the past\nwith the key to ou...</td>
                                </tr>
                                <tr>
                                    <th>202</th>
                                    <td>generations</td>
                                    <td>2013</td>
                                    <td>answer-with-metal</td>
                                    <td>look around, the air is full with fear and we ...</td>
                                </tr>
                                <tr>
                                    <th>250</th>
                                    <td>dark-of-the-sun</td>
                                    <td>2006</td>
                                    <td>arch-enemy</td>
                                    <td>like insects of the night, we are drawn into t...</td>
                                </tr>
                                <tr>
                                    <th>258</th>
                                    <td>shadows-and-dust</td>
                                    <td>2006</td>
                                    <td>arch-enemy</td>
                                    <td>at the mercy of our conscience\nconfined withi...</td>
                                </tr>
                                <tr>
                                    <th>...</th>
                                    <td>...</td>
                                    <td>...</td>
                                    <td>...</td>
                                    <td>...</td>
                                </tr>
                                <tr>
                                    <th>4589</th>
                                    <td>scorn</td>
                                    <td>2006</td>
                                    <td>allegiance</td>
                                    <td>likes rats we strip the earth\nrampant animals...</td>
                                </tr>
                                <tr>
                                    <th>4600</th>
                                    <td>armies-of-valinor</td>
                                    <td>2007</td>
                                    <td>galadriel</td>
                                    <td>into the battle we ride again\nagainst the dar...</td>
                                </tr>
                                <tr>
                                    <th>4609</th>
                                    <td>new-priesthood</td>
                                    <td>2006</td>
                                    <td>dark-angel</td>
                                    <td>history's shown you that answers can't be foun...</td>
                                </tr>
                                <tr>
                                    <th>4704</th>
                                    <td>ride-for-glory</td>
                                    <td>2007</td>
                                    <td>dragonland</td>
                                    <td>\n"we yearn for the battle and the glory...but...</td>
                                </tr>
                                <tr>
                                    <th>4782</th>
                                    <td>principle-of-speed</td>
                                    <td>2007</td>
                                    <td>drifter</td>
                                    <td>i made an experience\nit just happens once in ...</td>
                                </tr>
                                </tbody>
                            </table>
                            <p>107 rows × 4 columns</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Ok, I'm convinced.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [92]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">n_samples</span> <span
                            class="o">=</span> <span class="mi">1000</span>
<span class="n">lyrics_sample</span> <span class="o">=</span> <span class="n">metal_df</span><span
                                class="o">.</span><span class="n">sample</span><span class="p">(</span><span
                                class="n">n</span><span class="o">=</span><span class="n">n_samples</span><span
                                class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span
                                class="mi">109</span><span class="p">)</span><span class="o">.</span><span class="n">lyrics</span><span
                                class="o">.</span><span class="n">values</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [93]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">raw_text</span> <span
                            class="o">=</span> <span class="s1">' </span><span class="se">\n</span><span
                            class="s1"> '</span><span class="o">.</span><span class="n">join</span><span
                            class="p">(</span><span class="n">lyrics_sample</span><span class="p">)</span>
<span class="c1"># remove bad chars</span>
<span class="n">raw_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span
                                class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"[^\s\w']"</span><span
                                class="p">,</span> <span class="s2">""</span><span class="p">,</span> <span class="n">raw_text</span><span
                                class="p">)</span>

<span class="n">chars</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span
                                class="nb">sorted</span><span class="p">(</span><span class="n">raw_text</span><span
                                class="p">))</span>
<span class="n">char2idx</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span
                                class="n">c</span><span class="p">,</span><span class="n">i</span><span
                                class="p">)</span> <span class="k">for</span> <span class="n">i</span><span
                                class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">))</span>
<span class="n">idx2char</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span
                                class="n">i</span><span class="p">,</span> <span class="n">c</span><span
                                class="p">)</span> <span class="k">for</span> <span class="n">i</span><span
                                class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">))</span>

<span class="n">n_chars</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span
                                class="n">raw_text</span><span class="p">)</span>
<span class="n">n_vocab</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span
                                class="n">chars</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span
                                class="s1">'Sample Corpus Length: </span><span class="si">{</span><span
                                class="nb">len</span><span class="p">(</span><span class="n">raw_text</span><span
                                class="p">)</span><span class="si">}</span><span class="s1">'</span><span
                                class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Sample Corpus Length: 720944
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="pairs"><b>Creating Input/Target Pairs</b></div>
                </br></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We need to slice up our lyric data to create input and target pairs that can be to our model for its
                    supervised prediction task. Each input with be a sequence of <code>seq_len</code> characters. This
                    can be though of as a sliding window across the concatenated lyric data. The response is the
                    character after the end of that window in the training data.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [94]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># prepare the dataset of input to output pairs encoded as integers</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">seqs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span
                                class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_chars</span> <span
                                class="o">-</span> <span class="n">seq_len</span><span class="p">):</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">raw_text</span><span class="p">[</span><span
                                class="n">i</span><span class="p">:</span><span class="n">i</span> <span
                                class="o">+</span> <span class="n">seq_len</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">raw_text</span><span class="p">[</span><span
                                class="n">i</span> <span class="o">+</span> <span class="n">seq_len</span><span
                                class="p">]</span>
    <span class="n">seqs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span
                                class="n">char2idx</span><span class="p">[</span><span class="n">char</span><span
                                class="p">]</span> <span class="k">for</span> <span class="n">char</span> <span
                                class="ow">in</span> <span class="n">seq</span><span class="p">])</span>
    <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span
                                class="n">char2idx</span><span class="p">[</span><span class="n">target</span><span
                                class="p">])</span>
<span class="n">n_seqs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span
                                class="n">seqs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Total Char Sequences: "</span><span
                                class="p">,</span> <span class="n">n_seqs</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Total Char Sequences:  720904
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We can create a one-hot encoding by indexing into an <code>n_vocab</code> sized identity matrix using
                    the character index values.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [95]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span
                            class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                            class="n">reshape</span><span class="p">(</span><span class="n">seqs</span><span
                            class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span
                            class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<span class="n">eye</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">eye</span><span class="p">(</span><span class="n">n_vocab</span><span
                                class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">eye</span><span class="p">[</span><span
                                class="n">seqs</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">eye</span><span class="p">[</span><span class="n">targets</span><span
                                class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [96]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span><span
                            class="o">.</span><span class="n">shape</span><span class="p">,</span> <span
                            class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[96]:</div>
                    <div class="output_text output_subarea output_execute_result">
                        <pre>((720904, 40, 29), (720904, 29))</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [97]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># remove some large variables from memory</span>
<span class="k">del</span> <span class="n">metal_df</span>
<span class="k">del</span> <span class="n">lyrics_sample</span>
<span class="k">del</span> <span class="n">seqs</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>
                <div class="exercise" id="lambdacall"><b>LambdaCallback</b></div>
                </br></p>
                <p>The loss score is usually not the best way to judge if our language model is learning to generate
                    'quality' test. It would be better if we could periodically see examples of the kind of text it can
                    generate as it trains so we can judge for ourselves.</p>
                <p>The <code>LambdaCallback</code> allows us to execute arbitary functions at different points in the
                    training process and why they are useful when evaluating generative models. We'll use it to generate
                    some sample text at the end of every other epoch.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [98]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span
                            class="kn">import</span> <span class="n">LambdaCallback</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [99]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">on_epoch_end</span><span
                            class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span
                            class="n">_</span><span class="p">):</span>
    <span class="c1"># only triggers on every 2nd epoch</span>
    <span class="k">if</span><span class="p">((</span><span class="n">epoch</span> <span class="o">+</span> <span
                                class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span
                                class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="c1"># select a random seed sequence</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">random</span><span class="o">.</span><span class="n">randint</span><span
                                class="p">(</span><span class="mi">0</span><span class="p">,</span> <span
                                class="nb">len</span><span class="p">(</span><span class="n">X</span><span
                                class="p">)</span><span class="o">-</span><span class="mi">1</span><span
                                class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span
                                class="n">start</span><span class="p">]</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span
                                class="n">join</span><span class="p">([</span><span class="n">idx2char</span><span
                                class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span
                                class="p">(</span><span class="n">x</span><span class="p">)]</span> <span
                                class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span
                                class="n">seq</span><span class="p">])</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"---Seed: </span><span
                                class="se">\"</span><span class="si">{</span><span class="nb">repr</span><span
                                class="p">(</span><span class="n">seed</span><span class="p">)</span><span
                                class="si">}</span><span class="se">\"</span><span class="s2">---"</span><span
                                class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span
                                class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s2">"</span><span
                                class="p">,</span> <span class="n">end</span><span class="o">=</span><span
                                class="s1">''</span><span class="p">)</span>
        <span class="c1"># generate characters</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span
                                class="nb">range</span><span class="p">(</span><span class="mi">200</span><span
                                class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span
                                class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span
                                class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span
                                class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span
                                class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span
                                class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span
                                class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span
                                class="p">]</span>
            <span class="c1"># sampling gives us more 'serendipity' than argmax</span>
<span class="c1">#             index = np.argmax(pred)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">random</span><span class="o">.</span><span class="n">choice</span><span
                                class="p">(</span><span class="n">n_vocab</span><span class="p">,</span> <span
                                class="n">p</span><span class="o">=</span><span class="n">pred</span><span
                                class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">idx2char</span><span
                                class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span
                                class="n">write</span><span class="p">(</span><span class="n">result</span><span
                                class="p">)</span>
            <span class="c1"># shift sequence over</span>
            <span class="n">seq</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span
                                class="p">]</span> <span class="o">=</span> <span class="n">seq</span><span
                                class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span
                                class="p">]</span> <span class="o">=</span> <span class="n">eye</span><span
                                class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">()</span>

<span class="n">generate_text</span> <span class="o">=</span> <span class="n">LambdaCallback</span><span
                                class="p">(</span><span class="n">on_epoch_end</span><span class="o">=</span><span
                                class="n">on_epoch_end</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>We then add the <code>LambdaCallback</code> to the <code>callbacks</code> list along with <code>ModelCheckpoint</code>
                    and <code>EarlyStopping</code> to be passed to the <code>fit()</code> method at train time.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [100]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># define the checkpoint</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">'metal-char'</span>
<span class="n">filepath</span><span class="o">=</span><span class="sa">f</span><span class="s1">'models/</span><span
                                class="si">{</span><span class="n">model_name</span><span class="si">}</span><span
                                class="s1">.hdf5'</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span
                                class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span
                                class="n">monitor</span><span class="o">=</span><span class="s1">'loss'</span><span
                                class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span
                                class="mi">1</span><span class="p">,</span>
                             <span class="n">save_weights_only</span><span class="o">=</span><span
                                class="kc">False</span><span class="p">,</span>
                             <span class="n">save_best_only</span><span class="o">=</span><span
                                class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span
                                class="o">=</span><span class="s1">'min'</span><span class="p">)</span>
<span class="n">es</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span
                                class="n">monitor</span><span class="o">=</span><span class="s1">'loss'</span><span
                                class="p">,</span> <span class="n">patience</span><span class="o">=</span><span
                                class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span
                                class="o">=</span><span class="mi">0</span><span class="p">,</span>
                   <span class="n">mode</span><span class="o">=</span><span class="s1">'auto'</span><span
                                class="p">,</span><span class="n">restore_best_weights</span><span
                                class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">callbacks_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">checkpoint</span><span
                                class="p">,</span> <span class="n">generate_text</span><span class="p">,</span> <span
                                class="n">es</span><span class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#F5E4C3">
                    <b>Exercise:</b> Build a Character Based Lyric Generator
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><strong>Architecture</strong></p>
                <ul>
                    <li>Bidirection LSTM with a hidden dimension of 128 <em>in each direction</em></li>
                    <li>BatchNormalization to speed up training (Don't tell Pavlos!)</li>
                    <li>Dense output layer to predict the next character</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [11]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># your code here</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span
                                class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">hidden_dim</span><span
                                class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span
                                class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span
                                class="n">n_vocab</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span
                                class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span
                                class="p">(</span><span class="n">n_vocab</span><span class="p">,</span> <span
                                class="n">activation</span><span class="o">=</span><span
                                class="s1">'softmax'</span><span class="p">))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [12]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span
                            class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                            class="n">loss</span><span class="o">=</span><span
                            class="s1">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span
                            class="o">=</span><span class="s1">'adam'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
bidirectional (Bidirectional (None, 256)               161792
_________________________________________________________________
batch_normalization (BatchNo (None, 256)               1024
_________________________________________________________________
dense (Dense)                (None, 29)                7453
=================================================================
Total params: 170,269
Trainable params: 169,757
Non-trainable params: 512
_________________________________________________________________
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [16]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span
                            class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span
                            class="p">,</span> <span class="n">y</span><span class="p">,</span> <span
                            class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span
                            class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span
                            class="mi">128</span><span class="p">,</span> <span class="n">callbacks</span><span
                            class="o">=</span><span class="n">callbacks_list</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1/30
5633/5633 [==============================] - 47s 7ms/step - loss: 2.1992

Epoch 00001: loss improved from inf to 2.02026, saving model to models/metal-char.hdf5
Epoch 2/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.7666

Epoch 00002: loss improved from 2.02026 to 1.72630, saving model to models/metal-char.hdf5
---Seed: ""eeze but why\nwon't you believe\nwon't you""---
eeze but why
won't you believe
won't you will how kyon't now it you've goe
th
so noundapop and so shand will foring hpres
and like heredging return
all the stong love a cryttgin
fe i'm noblows the glort
comes evermeht ard a gong holortions
Epoch 3/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.6288

Epoch 00003: loss improved from 1.72630 to 1.60815, saving model to models/metal-char.hdf5
Epoch 4/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.5469

Epoch 00004: loss improved from 1.60815 to 1.53622, saving model to models/metal-char.hdf5
---Seed: "'l know me inside and outside\ni am here t'"---
l know me inside and outside
i am here the scarture for some no andon gounder time
nair a flose the the shorious of perpore
and glow but arourd she we just you bark for go darkness's need the missing oh your
were the sinsh away
the sky thei
Epoch 5/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.4940

Epoch 00005: loss improved from 1.53622 to 1.48602, saving model to models/metal-char.hdf5
Epoch 6/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.4527

Epoch 00006: loss improved from 1.48602 to 1.44772, saving model to models/metal-char.hdf5
---Seed: "'y your kindred watch us pronouncing sent'"---
y your kindred watch us pronouncing sently life
we go the conceliw
 a watmallch age here release
wrock agard to the haunhown
your life worntly way
too just right yer so many you makes me am now
homence the geas to get away
take it down gol
Epoch 7/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.4195

Epoch 00007: loss improved from 1.44772 to 1.41843, saving model to models/metal-char.hdf5
Epoch 8/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.3948

Epoch 00008: loss improved from 1.41843 to 1.39306, saving model to models/metal-char.hdf5
---Seed: "'here\ndespite what you may think jesus wi'"---
here
despite what you may think jesus with that just sunkness
memories remember free
this i can never knees i am drifted
laying here
infore through the sign
i can't be the completeh something burning stamber cross
reach to our arms
than her
Epoch 9/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.3715

Epoch 00009: loss improved from 1.39306 to 1.37268, saving model to models/metal-char.hdf5
Epoch 10/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.3516

Epoch 00010: loss improved from 1.37268 to 1.35401, saving model to models/metal-char.hdf5
---Seed: "'ence what does it mean\nare you satisfied'"---
ence what does it mean
are you satisfied
are yelcary
our voodes breathled with
weak the pullable do ac truth
there's a neck reach we let me good yoursele
relefcion around freedom
noin the burnt to beer with home
sail becauses
let down it ha
Epoch 11/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.3380

Epoch 00011: loss improved from 1.35401 to 1.33822, saving model to models/metal-char.hdf5
Epoch 12/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.3220

Epoch 00012: loss improved from 1.33822 to 1.32432, saving model to models/metal-char.hdf5
---Seed: "' know now all the reasons\nand all the co'"---
 know now all the reasons
and all the corcely game time
with me
but image tears
the the mairin
take hault me ashameeds
at lay in the ocean of time leave
my hope repouration you're my tears of your heart
like the day is let the way
driving t
Epoch 13/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.3081

Epoch 00013: loss improved from 1.32432 to 1.31071, saving model to models/metal-char.hdf5
Epoch 14/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2938

Epoch 00014: loss improved from 1.31071 to 1.29885, saving model to models/metal-char.hdf5
---Seed: "'ars\nnow beneath\nelectrical skies\nartille'"---
ars
now beneath
electrical skies
artilleds i tollimily
fall it comes
dopt all to eternity
eternity the humand as the flaughhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhohhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahhahh
hhoh
Epoch 15/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2828

Epoch 00015: loss improved from 1.29885 to 1.28791, saving model to models/metal-char.hdf5
Epoch 16/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2699

Epoch 00016: loss improved from 1.28791 to 1.27619, saving model to models/metal-char.hdf5
---Seed: "'ady to strike\ncall for us and you will s'"---
ady to strike
call for us and you will sounds shin frozes
into the rold you will open
now you wake up why will come us to me open death
i cannot toll me
i am not envence i could is sidt
the sun away from the tender
on the cloud silfit exuce
Epoch 17/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2591

Epoch 00017: loss improved from 1.27619 to 1.26540, saving model to models/metal-char.hdf5
Epoch 18/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2475

Epoch 00018: loss improved from 1.26540 to 1.25442, saving model to models/metal-char.hdf5
---Seed: ""'ve never been hurt\nact like you don't n""---
've never been hurt
act like you don't never gike and read
i know you sustappine everypherians
to my fingers hard to go away
you feel ignorade suffer to
your vicions free put the grave
tils murriors wind skies of sangring imputting building
Epoch 19/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2388

Epoch 00019: loss improved from 1.25442 to 1.24440, saving model to models/metal-char.hdf5
Epoch 20/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2306

Epoch 00020: loss improved from 1.24440 to 1.23521, saving model to models/metal-char.hdf5
---Seed: "'bered by offsprings\nwitness the end of m'"---
bered by offsprings
witness the end of madical preceine
but exists havelen't mind too much to avoid
walk harnts of the just throne
an a land lay it leaves
the beauth around myself
this hopely staity that a long shall
it fears the grances of
Epoch 21/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2214

Epoch 00021: loss improved from 1.23521 to 1.22585, saving model to models/metal-char.hdf5
Epoch 22/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.2067

Epoch 00022: loss improved from 1.22585 to 1.21731, saving model to models/metal-char.hdf5
---Seed: "' deep world of darkness so infinite and '"---
 deep world of darkness so infinite and shelter
 like spring distones tormore
the winds spow i never asween
but when the reasons puning it
i don't know we'll shane this girl enthroid
what do you pane
break what you can led by once was too
Epoch 23/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1998

Epoch 00023: loss improved from 1.21731 to 1.20829, saving model to models/metal-char.hdf5
Epoch 24/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1934

Epoch 00024: loss improved from 1.20829 to 1.19972, saving model to models/metal-char.hdf5
---Seed: "'ed\nascend\nto darkness we sail\neternal re'"---
ed
ascend
to darkness we sail
eternal remember
the
when is the humand time is hurt
with each in chance spread the indectimeom the journey begin
we'll be fixano just exploit for us
into the bett it returned
by both our kinn light
i am still
Epoch 25/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1828

Epoch 00025: loss improved from 1.19972 to 1.19151, saving model to models/metal-char.hdf5
Epoch 26/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1740

Epoch 00026: loss improved from 1.19151 to 1.18459, saving model to models/metal-char.hdf5
---Seed: "'christian society\nwomen were anathematiz'"---
christian society
women were anathematized of goes
 guestival powers of apotheration fass of my north
no fiust
gliding the provelosms of distrocked
snowledged who now
and this impromise
fullles me stays
before we melding
denied will reveag
Epoch 27/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1674

Epoch 00027: loss improved from 1.18459 to 1.17692, saving model to models/metal-char.hdf5
Epoch 28/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1592

Epoch 00028: loss improved from 1.17692 to 1.16979, saving model to models/metal-char.hdf5
---Seed: "'only void just about everywhere\nso i din'"---
only void just about everywhere
so i dinacl
fear for rain and carvable
we shared by it
give us the way that a commour and darows from althounds
now always creatured down and beside the soul
life i never be seen
your nevered in things and tw
Epoch 29/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1546

Epoch 00029: loss improved from 1.16979 to 1.16355, saving model to models/metal-char.hdf5
Epoch 30/30
5633/5633 [==============================] - 39s 7ms/step - loss: 1.1478

Epoch 00030: loss improved from 1.16355 to 1.15678, saving model to models/metal-char.hdf5
---Seed: "'ng\nwhen the life forsaken again is the o'"---
ng
when the life forsaken again is the one
and dough i can't take her
feel myself spected
sucame alone
my last is always we pride on me
the darkness ofwerd the times
there's a strength to last
the words' dies anate majesty
triet
the old ste
</pre>
                    </div>
                </div>
                <div class="output_area">
                    <div class="prompt output_prompt">Out[16]:</div>
                    <div class="output_text output_subarea output_execute_result">
                        <pre><tensorflow.python.keras.callbacks.history at 0x7f9100e086a0></pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#F5E4C3">
                    <b>End of Exercise:</b> Please return to the main room
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [20]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span
                            class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span
                            class="sa">f</span><span class="s1">'models/</span><span class="si">{</span><span class="n">model_name</span><span
                            class="si">}</span><span class="s1">.hdf5'</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>With some helper functions we can generate text from an arbitrary seed string.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [16]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sample</span><span
                            class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">temperature</span><span
                            class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># helper function to sample an index from a probability array</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">asarray</span><span class="p">(</span><span class="n">preds</span><span
                                class="p">)</span><span class="o">.</span><span class="n">astype</span><span
                                class="p">(</span><span class="s1">'float64'</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">log</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span> <span
                                class="o">/</span> <span class="n">temperature</span>
    <span class="n">exp_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">exp</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">exp_preds</span> <span
                                class="o">/</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">sum</span><span class="p">(</span><span class="n">exp_preds</span><span
                                class="p">)</span>
    <span class="n">probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">preds</span><span
                                class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span
                                class="p">(</span><span class="n">probas</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gen_text_char</span><span class="p">(</span><span class="n">seq</span><span
                                class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span
                                class="mf">0.3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Seed:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\"</span><span
                                class="s2">"</span><span class="p">,</span> <span class="s1">''</span><span
                                class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">idx2char</span><span
                                class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span
                                class="p">(</span><span class="n">x</span><span class="p">)]</span> <span
                                class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span
                                class="n">seq</span><span class="p">]),</span> <span class="s2">"</span><span
                                class="se">\"</span><span class="s2">"</span><span class="p">,</span> <span class="n">end</span><span
                                class="o">=</span><span class="s1">''</span><span class="p">)</span>
    <span class="c1"># generate characters</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span
                                class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span
                                class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span
                                class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span
                                class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span
                                class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span
                                class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span
                                class="n">pred</span><span class="p">,</span> <span class="n">temperature</span><span
                                class="p">)</span>
<span class="c1">#             index = np.argmax(pred)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">idx2char</span><span
                                class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span
                                class="n">write</span><span class="p">(</span><span class="n">result</span><span
                                class="p">)</span>
        <span class="c1"># shift sequence over</span>
        <span class="n">seq</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span
                                class="p">]</span> <span class="o">=</span> <span class="n">seq</span><span
                                class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span
                                class="p">]</span> <span class="o">=</span> <span class="n">eye</span><span
                                class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">text_from_seed</span><span class="p">(</span><span class="n">s</span><span
                                class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span
                                class="mf">0.3</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span
                                class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span
                                class="p">(</span><span class="sa">r</span><span class="s2">"[^\s\w']"</span><span
                                class="p">,</span> <span class="s2">""</span><span class="p">,</span> <span
                                class="n">s</span><span class="p">)</span>
    <span class="n">char2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span
                                class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span
                                class="n">i</span><span class="p">,</span> <span class="n">c</span> <span
                                class="ow">in</span> <span class="n">idx2char</span><span class="o">.</span><span
                                class="n">items</span><span class="p">()}</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">char2idx</span><span
                                class="p">[</span><span class="n">c</span><span class="p">]</span> <span
                                class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span
                                class="n">s</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span
                                class="p">)</span> <span class="o"><</span> <span class="n">seq_len</span><span
                                class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Seed must be at least </span><span
                                class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span
                                class="s1"> characters long!'</span><span class="p">)</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">seq</span><span class="p">[:</span><span
                                class="n">seq_len</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span
                                class="p">(</span><span class="n">np</span><span class="o">.</span><span
                                class="n">array</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">eye</span><span class="p">[</span><span class="n">x</span><span
                                class="p">]</span>
    <span class="n">gen_text_char</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span
                                class="n">temperature</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Set a seed string and see where your model takes it.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [30]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">seed</span> <span
                            class="o">=</span> <span class="s2">"Sunshine, lollipops and rainbows</span><span
                            class="se">\n</span><span class="s2">Everything that's wonderful is what I feel when we're together "</span>
<span class="n">text_from_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Seed:
" sunshine lollipops and rainbows
everythi "ng is not all the world
the shadows of a dead my life
in the world in the wind
the dark and see the land
the soul of a devolut of the darkness
we are the ones who says there a blackened with the way
the world is speak
i want to go back to me
the beauty of the darkness and destroyed are so come
the reason to the battle
with the wind is not all the world
i want to love you met you
i'm doing to me
i want to see you and i don't know
i was so were we see
to be the one we left behind
the fear of the dark in a desert of the ancient life
i will not come to me
the more i can't see the past
and i was the one who was now
the season where the streets of my heart
i wish i was better one the sea
we go to care in the sun of the attic
i will find a world in the wind
the world in the wind
the sands of the halls of a new world
i can't see the fire is all i need
the silence of the beast never been said
and survive on the waves
i'm waiting for an answer
the land of the sun
in the wind to carry on the wind
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise" style="background-color:#b3e6ff">
                    <b>Q</b>: How might you improve upon this simple model architecture?
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><a id="#math"></a></p>
                <h2 id="Arithmetic-with-an-RNN">Arithmetic with an RNN
                    <div id="math"></div>
                    <a class="anchor-link" href="#Arithmetic-with-an-RNN">¶</a></h2>
                <p><img src="fig/manytomanyMN.png" width="400"/></p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><em>Thanks go to Eleni for this code example.</em></p>
                <p>In this exercise, we are going to teach addition to our model. Given two numbers (<999), the model
                    outputs their sum (<9999). The input is provided as a string '231+432' and the model will provide
                    its output as ' 663' (Here the empty space is the padding character). We are not going to use any
                    external dataset and are going to construct our own dataset for this exercise.</p>
                <p>The exercise we attempt to do effectively "translates" a sequence of characters '231+432' to another
                    sequence of characters ' 663' and hence, this class of models are called sequence-to-sequence models
                    (aka seq2seq). Such architectures have profound applications in several real-life tasks such as
                    machine translation, summarization, image captioning etc.</p>
                <p>To be clear, sequence-to-sequence (aka seq2seq) models take as input a sequence of length N and
                    return a sequence of length M, where N and M may or may not differ, and every single
                    observation/input may be of different values, too. For example, machine translation concerns
                    converting text from one natural language to another (e.g., translating English to French). Google
                    Translate is an example, and their system is a seq2seq model. The input (e.g., an English sentence)
                    can be of any length, and the output (e.g., a French sentence) may be of any length.</p>
                <p><strong>Background knowledge:</strong> The earliest and most simple seq2seq model works by having one
                    RNN for the input, just like we've always done, and we refer to it as being an "encoder." The final
                    hidden state of the encoder RNN is fed as input to another RNN that we refer to as the "decoder."
                    The job of the decoder is to generate each token, one word at a time. This may seem really limiting,
                    as it relies on the encoder encapsulating the entire input sequence with just 1 hidden layer. It
                    seems unrealistic that we could encode an entire meaning of a sentence with just one hidden layer.
                    Yet, results even in this simplistic manner can be quite impressive. In fact, these early results
                    were compelling enough that these models immediately replaced the decades of earlier machine
                    translation work.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [134]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span
                            class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span
                                class="n">Dense</span><span class="p">,</span> <span class="n">RepeatVector</span><span
                                class="p">,</span> <span class="n">TimeDistributed</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <h4 id="data-generation-and-preprocessing">data generation and preprocessing<a class="anchor-link"
                                                                                               href="#data-generation-and-preprocessing">¶</a>
                </h4>
                <p>We can simply generate all the training data we need.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [141]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CharacterTable</span><span
                            class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span
                                class="p">,</span> <span class="n">chars</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chars</span> <span class="o">=</span> <span
                                class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span> <span class="o">=</span> <span
                                class="p">{</span><span class="n">c</span><span class="p">:</span> <span
                                class="n">i</span> <span class="k">for</span> <span class="n">i</span><span
                                class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span
                                class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span
                                class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span> <span class="o">=</span> <span
                                class="p">{</span><span class="n">i</span><span class="p">:</span> <span
                                class="n">c</span> <span class="k">for</span> <span class="n">i</span><span
                                class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span
                                class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span
                                class="p">)}</span>

    <span class="c1"># converts a String of characters into a one-hot embedding/vector</span>
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span
                                class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">num_rows</span><span
                                class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">zeros</span><span class="p">((</span><span class="n">num_rows</span><span
                                class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span
                                class="o">.</span><span class="n">chars</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span
                                class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span
                                class="n">C</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span
                                class="bp">self</span><span class="o">.</span><span class="n">char_indices</span><span
                                class="p">[</span><span class="n">c</span><span class="p">]]</span> <span
                                class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># converts a one-hot embedding/vector into a String of characters</span>
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span
                                class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">calc_argmax</span><span
                                class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">calc_argmax</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span
                                class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span
                                class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span
                                class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span><span
                                class="p">[</span><span class="n">x</span><span class="p">]</span> <span
                                class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span
                                class="n">x</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [142]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">TRAINING_SIZE</span> <span
                            class="o">=</span> <span class="mi">50000</span>
<span class="n">DIGITS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">MAXOUTPUTLEN</span> <span class="o">=</span> <span class="n">DIGITS</span> <span
                                class="o">+</span> <span class="mi">1</span>
<span class="n">MAXLEN</span> <span class="o">=</span> <span class="n">DIGITS</span> <span class="o">+</span> <span
                                class="mi">1</span> <span class="o">+</span> <span class="n">DIGITS</span>

<span class="n">chars</span> <span class="o">=</span> <span class="s1">'0123456789+ '</span>
<span class="n">ctable</span> <span class="o">=</span> <span class="n">CharacterTable</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [143]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">return_random_digit</span><span
                            class="p">():</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span
                                class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span
                                class="p">(</span><span class="s1">'0123456789'</span><span class="p">))</span>

<span class="c1"># generate a new number of length `DIGITS`</span>
<span class="k">def</span> <span class="nf">generate_number</span><span class="p">():</span>
    <span class="n">num_digits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">random</span><span class="o">.</span><span class="n">randint</span><span
                                class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">DIGITS</span> <span
                                class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span
                                class="o">.</span><span class="n">join</span><span class="p">(</span> <span class="n">return_random_digit</span><span
                                class="p">()</span>
                      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span
                                class="p">(</span><span class="n">num_digits</span><span class="p">)))</span>

<span class="c1"># generate `TRAINING_SIZE` # of pairs of random numbers</span>
<span class="k">def</span> <span class="nf">data_generate</span><span class="p">(</span><span
                                class="n">num_examples</span><span class="p">):</span>
    <span class="n">questions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">answers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generating data...'</span><span
                                class="p">)</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span
                                class="n">questions</span><span class="p">)</span> <span class="o"><</span> <span
                                class="n">TRAINING_SIZE</span><span class="p">:</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span
                                class="n">generate_number</span><span class="p">(),</span> <span class="n">generate_number</span><span
                                class="p">()</span>

        <span class="c1"># don't allow duplicates; this is good practice for training,</span>
        <span class="c1"># as we will minimize memorizing seen examples</span>
        <span class="n">key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span
                                class="nb">sorted</span><span class="p">((</span><span class="n">a</span><span
                                class="p">,</span> <span class="n">b</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">seen</span><span
                                class="p">:</span>
            <span class="k">continue</span>
        <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span
                                class="n">key</span><span class="p">)</span>

        <span class="c1"># pad the data with spaces so that the length is always MAXLEN.</span>
        <span class="n">q</span> <span class="o">=</span> <span class="s1">'</span><span class="si">{}</span><span
                                class="s1">+</span><span class="si">{}</span><span class="s1">'</span><span
                                class="o">.</span><span class="n">format</span><span class="p">(</span><span
                                class="n">a</span><span class="p">,</span> <span class="n">b</span><span
                                class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span
                                class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAXLEN</span> <span
                                class="o">-</span> <span class="nb">len</span><span class="p">(</span><span
                                class="n">q</span><span class="p">))</span>
        <span class="n">ans</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span
                                class="n">a</span> <span class="o">+</span> <span class="n">b</span><span
                                class="p">)</span>

        <span class="c1"># answers can be of maximum size DIGITS + 1.</span>
        <span class="n">ans</span> <span class="o">+=</span> <span class="s1">' '</span> <span class="o">*</span> <span
                                class="p">(</span><span class="n">MAXOUTPUTLEN</span> <span class="o">-</span> <span
                                class="nb">len</span><span class="p">(</span><span class="n">ans</span><span class="p">))</span>
        <span class="n">questions</span><span class="o">.</span><span class="n">append</span><span
                                class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span
                                class="n">ans</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Total addition questions:'</span><span
                                class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span
                                class="p">))</span>
    <span class="k">return</span> <span class="n">questions</span><span class="p">,</span> <span
                                class="n">answers</span>

<span class="k">def</span> <span class="nf">encode_examples</span><span class="p">(</span><span
                                class="n">questions</span><span class="p">,</span> <span class="n">answers</span><span
                                class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span
                                class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span
                                class="p">),</span> <span class="n">MAXLEN</span><span class="p">,</span> <span
                                class="nb">len</span><span class="p">(</span><span class="n">chars</span><span
                                class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span
                                class="n">np</span><span class="o">.</span><span class="n">bool</span><span
                                class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span
                                class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span
                                class="p">),</span> <span class="n">DIGITS</span> <span class="o">+</span> <span
                                class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">)),</span> <span
                                class="n">dtype</span><span class="o">=</span><span class="n">np</span><span
                                class="o">.</span><span class="n">bool</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span
                                class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span
                                class="n">questions</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span
                                class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span
                                class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span
                                class="n">MAXLEN</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span
                                class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span
                                class="n">answers</span><span class="p">):</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span
                                class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span
                                class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span
                                class="n">DIGITS</span> <span class="o">+</span> <span class="mi">1</span><span
                                class="p">)</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">arange</span><span class="p">(</span><span class="nb">len</span><span
                                class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span
                                class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span
                                class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="n">indices</span><span
                                class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">indices</span><span
                                class="p">]</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [144]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">q</span><span
                            class="p">,</span><span class="n">a</span> <span class="o">=</span> <span class="n">data_generate</span><span
                            class="p">(</span><span class="n">TRAINING_SIZE</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">encode_examples</span><span
                                class="p">(</span><span class="n">q</span><span class="p">,</span><span
                                class="n">a</span><span class="p">)</span>

<span class="c1"># divides our data into training and validation</span>
<span class="n">split_at</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span
                                class="n">x</span><span class="p">)</span> <span class="o">-</span> <span
                                class="nb">len</span><span class="p">(</span><span class="n">x</span><span
                                class="p">)</span> <span class="o">//</span> <span class="mi">10</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span
                                class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span
                                class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">split_at</span><span
                                class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">split_at</span><span
                                class="p">:],</span><span class="n">y</span><span class="p">[:</span><span class="n">split_at</span><span
                                class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">split_at</span><span
                                class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Training Data shape:'</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'X : '</span><span class="p">,</span> <span
                                class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span
                                class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Y : '</span><span class="p">,</span> <span
                                class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span
                                class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Sample Question(in encoded form) : '</span><span
                                class="p">,</span> <span class="n">x_train</span><span class="p">[</span><span
                                class="mi">0</span><span class="p">],</span> <span class="n">y_train</span><span
                                class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Sample Question(in decoded form) : '</span><span
                                class="p">,</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span
                                class="p">(</span><span class="n">x_train</span><span class="p">[</span><span
                                class="mi">0</span><span class="p">]),</span><span
                                class="s1">'Sample Output : '</span><span class="p">,</span> <span
                                class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span
                                class="p">(</span><span class="n">y_train</span><span class="p">[</span><span
                                class="mi">0</span><span class="p">]))</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Generating data...
Total addition questions: 50000
Training Data shape:
X :  (45000, 7, 12)
Y :  (45000, 4, 12)
Sample Question(in encoded form) :  [[False False False False False False False  True False False False False]
 [False False False False False False False False False False False  True]
 [False False  True False False False False False False False False False]
 [False  True False False False False False False False False False False]
 [False False False False False False False False  True False False False]
 [False False False False False False False False False False  True False]
 [ True False False False False False False False False False False False]] [[False False False False False False False False  True False False False]
 [False False False False False False False  True False False False False]
 [False False False False False False False False False False  True False]
 [ True False False False False False False False False False False False]]
Sample Question(in decoded form) :  590+68  Sample Output :  658
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [145]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="n">x_train</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt output_prompt">Out[145]:</div>
                    <div class="output_text output_subarea output_execute_result">
<pre>array([[[False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False,  True],
        [False, False,  True, ..., False, False, False],
        ...,
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False,  True, False],
        [ True, False, False, ..., False, False, False]],

       [[False, False, False, ..., False,  True, False],
        [False, False, False, ...,  True, False, False],
        [False, False, False, ..., False, False, False],
        ...,
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False,  True, False],
        [ True, False, False, ..., False, False, False]],

       [[False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False,  True, False],
        ...,
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [ True, False, False, ..., False, False, False]],

       ...,

       [[False, False, False, ...,  True, False, False],
        [False,  True, False, ..., False, False, False],
        [False, False, False, ...,  True, False, False],
        ...,
        [False, False, False, ..., False, False, False],
        [ True, False, False, ..., False, False, False],
        [ True, False, False, ..., False, False, False]],

       [[False, False, False, ..., False, False, False],
        [False, False, False, ..., False,  True, False],
        [False,  True, False, ..., False, False, False],
        ...,
        [False, False,  True, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [ True, False, False, ..., False, False, False]],

       [[False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        ...,
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [ True, False, False, ..., False, False, False]]])</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <div class="exercise">
                    <b></b> Build an RNN for Arithmetic
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><strong>Note:</strong> Whenever you are initializing a LSTM in Keras, by the default the option
                    <code>return_sequences = False</code>. This means that at the end of the step the next component
                    will only get to see the final hidden layer's values. On the other hand, if you set <code>return_sequences
                        = True</code>, the LSTM component will return the hidden layer at each time step. It means that
                    the next component should be able to consume inputs in that form.</p>
                <p>Think how this statement is relevant in terms of this model architecture and the TimeDistributed
                    module we just learned.</p>
                <p>Build an encoder and decoder both single layer 128 nodes and an appropriate dense layer as needed by
                    the model.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [150]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Hyperaparams</span>
<span class="n">HIDDEN_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">LAYERS</span> <span class="o">=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Build model...'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c1">#ENCODING</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span
                                class="p">(</span><span class="n">HIDDEN_SIZE</span><span class="p">,</span> <span
                                class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span
                                class="n">MAXLEN</span><span class="p">,</span> <span class="nb">len</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">))))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RepeatVector</span><span
                                class="p">(</span><span class="n">MAXOUTPUTLEN</span><span class="p">))</span>

<span class="c1">#DECODING</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span
                                class="p">(</span><span class="n">LAYERS</span><span class="p">):</span>
    <span class="c1"># return hidden layer at each time step</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span
                                class="n">LSTM</span><span class="p">(</span><span class="n">HIDDEN_SIZE</span><span
                                class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span
                                class="kc">True</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span
                                class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span
                                class="p">(</span><span class="n">chars</span><span class="p">),</span> <span class="n">activation</span><span
                                class="o">=</span><span class="s1">'softmax'</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span
                                class="n">loss</span><span class="o">=</span><span
                                class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span
                                class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span
                                class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>Build model...
Model: "sequential_22"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_16 (LSTM)               (None, 128)               72192
_________________________________________________________________
repeat_vector_2 (RepeatVecto (None, 4, 128)            0
_________________________________________________________________
lstm_17 (LSTM)               (None, 4, 128)            131584
_________________________________________________________________
time_distributed_1 (TimeDist (None, 4, 12)             1548
=================================================================
Total params: 205,324
Trainable params: 205,324
Non-trainable params: 0
_________________________________________________________________
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p>Let's check how well our model trained.</p>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing code_cell rendered">
        <div class="input">
            <div class="prompt input_prompt">In [151]:</div>
            <div class="inner_cell">
                <div class="input_area">
                    <div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">iteration</span> <span
                            class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span
                            class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span
                            class="p">):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span
                                class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span
                                class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
              <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span
                                class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
    <span class="c1"># Select 10 samples from the validation set at random so</span>
    <span class="c1"># we can visualize errors.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Finished iteration '</span><span
                                class="p">,</span> <span class="n">iteration</span><span class="p">)</span>
    <span class="n">numcorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">numtotal</span> <span class="o">=</span> <span class="mi">20</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span
                                class="p">(</span><span class="n">numtotal</span><span class="p">):</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span
                                class="n">random</span><span class="o">.</span><span class="n">randint</span><span
                                class="p">(</span><span class="mi">0</span><span class="p">,</span> <span
                                class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span
                                class="p">))</span>
        <span class="n">rowx</span><span class="p">,</span> <span class="n">rowy</span> <span class="o">=</span> <span
                                class="n">x_val</span><span class="p">[</span><span class="n">np</span><span
                                class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span
                                class="p">])],</span> <span class="n">y_val</span><span class="p">[</span><span
                                class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span
                                class="n">ind</span><span class="p">])]</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span
                                class="n">predict_classes</span><span class="p">(</span><span class="n">rowx</span><span
                                class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span
                                class="mi">0</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span
                                class="n">decode</span><span class="p">(</span><span class="n">rowx</span><span
                                class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span
                                class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowy</span><span
                                class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span
                                class="n">decode</span><span class="p">(</span><span class="n">preds</span><span
                                class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">calc_argmax</span><span
                                class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Question'</span><span class="p">,</span> <span
                                class="n">q</span><span class="p">,</span> <span class="n">end</span><span
                                class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'True'</span><span
                                class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span
                                class="n">end</span><span class="o">=</span><span class="s1">' '</span><span
                                class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Guess'</span><span
                                class="p">,</span> <span class="n">guess</span><span class="p">,</span> <span class="n">end</span><span
                                class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">correct</span> <span
                                class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Good job'</span><span
                                class="p">)</span>
            <span class="n">numcorrect</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Fail'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'The model scored '</span><span
                                class="p">,</span> <span class="n">numcorrect</span><span class="o">*</span><span
                                class="mi">100</span><span class="o">/</span><span class="n">numtotal</span><span
                                class="p">,</span><span class="s1">' </span><span class="si">% i</span><span class="s1">n its test.'</span><span
                                class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
        </div>
        <div class="output_wrapper">
            <div class="output">
                <div class="output_area">
                    <div class="prompt"></div>
                    <div class="output_subarea output_stream output_stdout output_text">
<pre>
Epoch 1/20
352/352 [==============================] - 5s 6ms/step - loss: 2.0174 - accuracy: 0.2885 - val_loss: 1.7942 - val_accuracy: 0.3441
Epoch 2/20
352/352 [==============================] - 2s 5ms/step - loss: 1.7772 - accuracy: 0.3434 - val_loss: 1.7091 - val_accuracy: 0.3704
Epoch 3/20
352/352 [==============================] - 2s 5ms/step - loss: 1.6578 - accuracy: 0.3831 - val_loss: 1.5676 - val_accuracy: 0.4189
Epoch 4/20
352/352 [==============================] - 2s 5ms/step - loss: 1.5156 - accuracy: 0.4305 - val_loss: 1.4042 - val_accuracy: 0.4744
Epoch 5/20
352/352 [==============================] - 2s 5ms/step - loss: 1.3777 - accuracy: 0.4846 - val_loss: 1.2784 - val_accuracy: 0.5250
Epoch 6/20
352/352 [==============================] - 2s 5ms/step - loss: 1.2598 - accuracy: 0.5288 - val_loss: 1.1922 - val_accuracy: 0.5512
Epoch 7/20
352/352 [==============================] - 2s 5ms/step - loss: 1.1605 - accuracy: 0.5640 - val_loss: 1.1004 - val_accuracy: 0.5843
Epoch 8/20
352/352 [==============================] - 2s 5ms/step - loss: 1.0719 - accuracy: 0.5927 - val_loss: 1.0159 - val_accuracy: 0.6151
Epoch 9/20
352/352 [==============================] - 2s 5ms/step - loss: 0.9892 - accuracy: 0.6241 - val_loss: 0.9340 - val_accuracy: 0.6425
Epoch 10/20
352/352 [==============================] - 2s 5ms/step - loss: 0.8879 - accuracy: 0.6606 - val_loss: 0.8111 - val_accuracy: 0.6805
Epoch 11/20
352/352 [==============================] - 2s 5ms/step - loss: 0.7547 - accuracy: 0.7132 - val_loss: 0.6573 - val_accuracy: 0.7530
Epoch 12/20
352/352 [==============================] - 2s 5ms/step - loss: 0.6126 - accuracy: 0.7778 - val_loss: 0.5363 - val_accuracy: 0.8079
Epoch 13/20
352/352 [==============================] - 2s 5ms/step - loss: 0.4939 - accuracy: 0.8360 - val_loss: 0.4236 - val_accuracy: 0.8686
Epoch 14/20
352/352 [==============================] - 2s 5ms/step - loss: 0.3952 - accuracy: 0.8824 - val_loss: 0.3391 - val_accuracy: 0.9003
Epoch 15/20
352/352 [==============================] - 2s 5ms/step - loss: 0.3146 - accuracy: 0.9160 - val_loss: 0.2851 - val_accuracy: 0.9208
Epoch 16/20
352/352 [==============================] - 2s 5ms/step - loss: 0.2535 - accuracy: 0.9382 - val_loss: 0.2221 - val_accuracy: 0.9458
Epoch 17/20
352/352 [==============================] - 2s 5ms/step - loss: 0.2063 - accuracy: 0.9535 - val_loss: 0.1934 - val_accuracy: 0.9529
Epoch 18/20
352/352 [==============================] - 2s 5ms/step - loss: 0.1786 - accuracy: 0.9584 - val_loss: 0.1608 - val_accuracy: 0.9613
Epoch 19/20
352/352 [==============================] - 2s 5ms/step - loss: 0.1441 - accuracy: 0.9707 - val_loss: 0.1314 - val_accuracy: 0.9708
Epoch 20/20
352/352 [==============================] - 2s 5ms/step - loss: 0.1210 - accuracy: 0.9758 - val_loss: 0.1156 - val_accuracy: 0.9750
Finished iteration  1
Question 579+42  True 621  Guess 621  Good job
Question 778+40  True 818  Guess 818  Good job
Question 34+574  True 608  Guess 608  Good job
Question 5+553   True 558  Guess 558  Good job
Question 2+27    True 29   Guess 29   Good job
Question 506+30  True 536  Guess 536  Good job
Question 51+714  True 765  Guess 765  Good job
Question 258+31  True 289  Guess 289  Good job
Question 9+70    True 79   Guess 89   Fail
Question 14+83   True 97   Guess 97   Good job
Question 59+378  True 437  Guess 437  Good job
Question 94+836  True 930  Guess 920  Fail
Question 875+483 True 1358 Guess 1358 Good job
Question 482+34  True 516  Guess 516  Good job
Question 257+49  True 306  Guess 306  Good job
Question 591+5   True 596  Guess 596  Good job
Question 88+771  True 859  Guess 859  Good job
Question 248+86  True 334  Guess 334  Good job
Question 27+929  True 956  Guess 956  Good job
Question 23+854  True 877  Guess 877  Good job
The model scored  90.0  % in its test.
</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="cell border-box-sizing text_cell rendered">
        <div class="prompt input_prompt">
        </div>
        <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">
                <p><strong>Possible Experimentation</strong></p>
                <ul>
                    <li><p>Try changing the hyperparams, use other RNNs, more layers, check if increasing the number of
                        epochs is useful.</p>
                    </li>
                    <li><p>Try reversing the data from validation set and check if commutative property of addition is
                        learned by the model.</p>
                    </li>
                    <li>Try printing the hidden layer with two inputs that are commutative and check if the hidden
                        representations it learned are same or similar. Do we expect it to be true? If so, why? If not
                        why? You can access the layer using an index with model.layers and layer.output will give the
                        output of that layer.
                    </li>
                </ul>
                <ul>
                    <li>Try doing addition in the RNN model the same way we do by hand. Reverse the order of digits and
                        at each time step, input two digits get an output use the hidden layer and input next two digits
                        and so on.(units in the first time step, tens in the second time step etc.)
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            " linebreaks: { automatic: true, width: '95% container' }, " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
    </script>
</main>

<footer class="footer">
    <div class="container">
    <span class="text-muted">Copyright 2021 &copy;
      <a class="text-muted" href="https://iacs.seas.harvard.edu/">Institute for Applied Computational Science</a>
    </span>
    </div>
</footer>     <!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script
        src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"
></script>
<script
        src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js"
        integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T"
        crossorigin="anonymous"
></script>
</body>
</html>