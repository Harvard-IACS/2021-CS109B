{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressing-exhaust",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science \n",
    "## Section 2: Convolutional Neural Networks II\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2021**<br/>\n",
    "**Instructors**: Mark Glickman, Pavlos Protopapas, and Chris Tanner <br/>\n",
    "**Authors**: Hayden Joy, Marios Mattheakis, and Pavlos Protopapas \n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-hometown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you should be able to:\n",
    "* Take advantage of **GPUs** on SEAS Jupyter Hub for doing computationally intensive course work\n",
    "* Explain the history and significance of receptive fields in convolutional neural networks (CNNs)\n",
    "* Interpret CNNs through feature maps, saliency maps, and GradCam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-conspiracy",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "- [**CNN Recap**](#recap)\n",
    "    - [Callbacks and Model Checkpoints](#recap) \n",
    "    - [Calculating Output Size](#recap)\n",
    "    - [Big O notation](#recap)\n",
    "- [**Receptive Fields**](#fields)\n",
    "    - [Ice Breaker](#icebreaker)\n",
    "    - [Nuerobiology connection](#nuerobio)\n",
    "    - [Dilated Convolution](#dilated)\n",
    "- [**Visualizing and interpreting CNNs I**](#f_maps)\n",
    "    - [Feature Maps](#f_maps)\n",
    "- [**Break Out Room 1**](#BO1)\n",
    "    - [Interpreting VGG Feature Maps](#BO1A)\n",
    "    - [Generating Feature Maps](#BO1B)\n",
    "- [**Visualizing and interpreting CNNs II**](#attention)\n",
    "    - [Attention and Salience](#attention)\n",
    "    - [Saliency Maps](#salience)\n",
    "    - [Grad Cam](#gradcam)\n",
    "- [**Break Out Room 2**](#BO2)\n",
    "    - [Background](#BO2A)\n",
    "    - [Exercise](#BO2B)\n",
    "- [**SOTA models and visualizations**](#sota)\n",
    "    - [Architectures and related papers](#sota)\n",
    "    - [Google DeepDream](#deepdream)\n",
    "- [**\\[Bonus\\]Gradient Tape**](#tl)\n",
    "    - [Motivation](#tl_theory)\n",
    "    - [Example](#tl_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-discretion",
   "metadata": {},
   "source": [
    "<a id='jupyterhub'></a>\n",
    "## Using SEAS Jupyter Hub [^](#contents \"Back to Contents\")\n",
    "\n",
    "**PLEASE READ**: [Instructions for Using SEAS JupyterHub](https://canvas.harvard.edu/courses/84598/pages/instructions-for-using-seas-jupyterhub?module_item_id=871908)\n",
    "\n",
    "SEAS and FAS are providing you with a platform in AWS to use for the class (accessible from the 'JupyterHub-GPU' menu link in Canvas). These are AWS p2 instances with a GPU, 10GB of disk space, and 61 GB of RAM,  for faster training for your networks. Most of the libraries such as keras, tensorflow, pandas, etc. are pre-installed. If a library is missing you may install it via the Terminal.\n",
    "\n",
    "**NOTE : The AWS platform is funded by SEAS and FAS for the purposes of the class. It is not running against your individual credit.**\n",
    "\n",
    "**NOTE NOTE NOTE: You are not allowed to use it for purposes not related to this course.**\n",
    "\n",
    "**Help us keep this service: Make sure you stop your instance as soon as you do not need it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "import tensorflow as tf\n",
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment\n",
    "# that evaluates operations immediately, without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(f\"tensorflow version {tf.__version__}\")\n",
    "print(f\"keras version {tf.keras.__version__}\")\n",
    "print(f\"Eager Execution Enabled: {tf.executing_eagerly()}\\n\")\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"\\nNumber of replicas: {strategy.num_replicas_in_sync}\\n\")\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(f\"Devices: {devices}\\n\")\n",
    "print(f\"{tf.config.experimental.list_logical_devices('GPU')}\\n\")\n",
    "\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\\n\")\n",
    "print(f\"All Pysical Devices: {tf.config.list_physical_devices()}\")\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/datac_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.random.set_seed(2266)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-damage",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "# CNN Recap\n",
    "\n",
    "<img src='https://miro.medium.com/max/1872/1*Y-u7dH4WC-dXyn9jOG4w0w.png' width = 600>\n",
    "\n",
    "So far we have seen that CNNs are an incredibly powerful black box method for various tasks including classification and regression on images. However, there is *a loss of interpretability* cost that results from the increased model complexit. Today we will shine a bit of light inside these models by examining **feature maps**, **vanilla saliency maps**, and another type of saliency map tool known as **grad cam**. We will learn how to manipulate tensorflow neural network layers and activations.\n",
    "\n",
    "First some review and important details about CNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-cabin",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Keras Callbacks</b></div></br>\n",
    "\n",
    "Training CNNs can take a long time. We should checkpoint our model so we don't lose progress and stop early if we don't see improvement or see overfitting. That can help to save computational time used for the training.</br>\n",
    "\n",
    "[Keras Callbacks Documentation](https://keras.io/api/callbacks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "DATA_DIR = f'data/'\n",
    "\n",
    "#these were introduced in the previous section and are here simply for demonstration purposes.\n",
    "#please refer to section 1 to see ModelCheckpoint implimentation.\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "mc = ModelCheckpoint(DATA_DIR+'/models', monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "# checkpointing actually slows down training\n",
    "# just add it to the call back list if you'd like to include it, as shown in the comment below.\n",
    "#callbacks = [es]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-thought",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Computational complexity and backpropagation</b></div></br>\n",
    "\n",
    "The algorithmic complexity of feed forward neural networks is $O(mn)$ where m is the number of neurons in layer $l_i$ and $m$ is the number of nodes in layer $l_{i+1}$. This can be shown to be equivalent to $O(\\psi)$ where $\\psi$ is the number of synapses of the network.\n",
    "\n",
    "$\\bullet$ In feed forward neural networks if you double the number of layers in the network that will in general increase the number of operations that the network needs to complete by (2x). \n",
    "\n",
    "$\\bullet$ Doubling the number of neurons in a layer increases the number of operations by 4x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-disorder",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Stride vs pooling: how do I decide how to downsample?</b></div></br>\n",
    "\n",
    "$\\bullet$ Here is an interesting post discussing situations in which to use the two methods (striding vs pooling): https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling/387522\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-sperm",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Dropout in the Convolution Layers vs Dropout in the Dense layers</b></div></br>\n",
    "\n",
    "$\\bullet$ Dropout in the convolution layers is mathematically equivalent to \"multiplying Bernoulli into the feature maps of the network\". For more information see this <a href='https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2'> article</a>.\n",
    "\n",
    "$\\bullet$ Dropout in the dense layers effectively makes the network an ensemble of models of the same architecture but with different activated nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-breast",
   "metadata": {},
   "source": [
    "<a id='fields'></a>\n",
    "# Receptive Fields \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-heading",
   "metadata": {},
   "source": [
    "<a id='icebreaker'></a>\n",
    "## Receptive Fields Ice Breaker\n",
    "\n",
    "Which of the following squares is the *receptive field* of the purple neuron shown at right?\n",
    "\n",
    "i. The blue 5x5 square in layer 1\n",
    "\n",
    "ii. The red 3x3 square in layer 1\n",
    "\n",
    "iii. the purple 2x2 square in layer 2\n",
    "\n",
    "<img src='fig/q7b_1.jpg' alt='MaxPool'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-hanging",
   "metadata": {},
   "source": [
    "<a id='nuerobio'></a>\n",
    "### Definition (neurobiology)  from the <a href = \"https://www.britannica.com/science/receptive-field\"> encylopedia Britannica </a>:\n",
    "Receptive field, region in the sensory periphery within which stimuli can influence the electrical activity of sensory cells. The receptive field encompasses the sensory receptors that feed into sensory neurons and thus includes specific receptors on a neuron as well as collectives of receptors that are capable of activating a neuron via synaptic connections. [...] **The concept of the receptive field is central to sensory neurobiology, because it provides a description of the location at which a sensory stimulus must be presented in order to elicit a response from a sensory cell.*\n",
    "\n",
    "<img src='https://i.stack.imgur.com/br9Zo.jpg' width = 500> \n",
    "\n",
    "[In 1956] American physiologist Haldan Keffer Hartline became the first to isolate and record electrical responses from single optic nerve fibres of vertebrate eyes. Hartline defined the receptive field of a retinal ganglion cell as the retinal area from which an increase in the frequency of action potentials could be elicited.\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://brainconnection.brainhq.com/wp-content/uploads/2004/03/photoreceptors-f2.png' width=500>\n",
    "\n",
    "Each eye cell may only activate based on a smaller part of the image\n",
    "\n",
    "# Receptive fields formal definition\n",
    "\n",
    "\"The receptive field is defined as the region in the input space that a\n",
    "particular CNN’s feature is looking at (i.e. be affected by).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-canyon",
   "metadata": {},
   "source": [
    "\n",
    "<img src='https://www.bing.com/th/id/OGC.242d7bb9a68dd589641f09a157823f62?pid=1.7&rurl=https%3a%2f%2ftechnion046195.netlify.app%2fstatic%2freceptive_field-242d7bb9a68dd589641f09a157823f62.gif&ehk=Kc3YWUkKeUtmTgWIl8GuxdU2LkVGHxcd9qx7EpMkl2g%3d' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-sessions",
   "metadata": {},
   "source": [
    "<a id='dilated'></a>\n",
    "<div class='exercise'><b>Dilated Convolution</b></div></br>\n",
    "\n",
    "Resource : https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25\n",
    "\n",
    "• We can “inflate” the receptive field by inserting holes between the kernel elements.\n",
    "\n",
    "• **Dilation rate** indicates how much the kernel is widened.\n",
    "\n",
    "**Dilation plays a key role in *semantic segmentation*.**\n",
    "\n",
    "\n",
    "<img src='https://nicolovaligi.com/dilated_convolution.gif' width = 220>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://miro.medium.com/max/1575/0*k8ejti9_6CHwxzFQ.gif' width = 350>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import imageio\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import requests\n",
    "import scipy.ndimage as ndimage\n",
    "import tensorflow as tf\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout \n",
    "from tensorflow.keras.layers import Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Using TensorFlow Datasets version: {tfds.__version__}\")\n",
    "print(f\"Using TensorFlow AddOns version: {tfa.__version__}\")\n",
    "\n",
    "tf.random.set_seed(\n",
    "    109\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.py\n",
    "# here we import data augmentation and plotting helper functions which were introduced in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-netherlands",
   "metadata": {},
   "source": [
    "<a id='f_maps'></a>\n",
    "\n",
    "## Visualizing Feature Maps\n",
    "**Going Deeper...** \n",
    "\n",
    "Definition:\n",
    "\"The activation maps, called feature maps, capture the result of applying the filters to input, such as the input image or another feature map. [...] **The expectation would be that the feature maps close to the input detect small or fine-grained detail, whereas feature maps close to the output of the model capture more general features.\"**\n",
    "\n",
    "https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n",
    "\n",
    "<img src='https://miro.medium.com/max/3308/1*OuxhgVj1WDDfo5UO5GIhgA@2x.png'> </img>\n",
    "\n",
    "\n",
    "We don't simply want to show the weights of the filter maps but rather how those convolutional layers activate given a specific input image).\n",
    "\n",
    "**Steps to visualize a feature map:**\n",
    "\n",
    "1. Train a CNN of your favorite architecture.\n",
    "\n",
    "2. Create a new model \"that is a subset of the layers in the full model\" with the inputs of the original model and with the output set to the desired feature map sets' corresponding **activation layer** ( for example your convolved feature map has already been activated with `relu`). \n",
    "\n",
    "3. Perform a forward pass (i.e. predict) with the modified model and extract the feature map.\n",
    "\n",
    "\"We can use this information and design a new model that is a subset of the layers in the full VGG16 model. The model would have the *same input layer* as the original model, but the output would be the *output of a given convolutional layer*, which we know would be the activation of the layer or the feature map.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-spokesman",
   "metadata": {},
   "source": [
    "## Today we will be using the cycle_gan dataset from tensorflow.\n",
    "\n",
    "This dataset was created in Berkely, California and consists of many different binary classfication datasets. This is <a href='https://www.tensorflow.org/datasets/catalog/cycle_gan'> the link </a>to the tensorflow dataset documentation  and a\n",
    "    <a href='https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/'> webpage </a> from which you can download one of the numerous cycle_gan datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-ethiopia",
   "metadata": {},
   "source": [
    "***The neural networks in this section have been pre-trained. If you would prefer to train them yourself, simply uncomment the `MODELS_TRAINED` boolean variable below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainA, trainB, testA, testB), ds_info = tfds.load('cycle_gan/horse2zebra', \n",
    "                                         split = [\"trainA\", \"trainB\", \"testA\", \"testB\"], \n",
    "                                         shuffle_files=True, as_supervised=True, with_info=True)\n",
    "\n",
    "MODELS_TRAINED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Dataset cardinality: horse train {int(trainA.cardinality())} images, zebra train {int(trainA.cardinality())} images.')\n",
    "print(f'                      horse test {int(testA.cardinality())} images, zebra test {int(testB.cardinality())} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds_info.homepage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_imgs():\n",
    "    \"\"\"\n",
    "    A helper function that displays a four images from the horse2zebra cycle gan dataset, two for each class.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2,2, figsize = (12, 8)); ax = ax.flatten()\n",
    "\n",
    "    horse_imgs, zebra_imgs = [], []\n",
    "    for i, (image, label) in enumerate(trainA.take(2)):\n",
    "        plt.sca(ax[2*i + 1])\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"class {label}\")\n",
    "        plt.axis('off')\n",
    "        horse_imgs.append(image)\n",
    "\n",
    "    for i, (image, label) in enumerate(trainB.take(2)):\n",
    "        #plt.sca() switches the current pyplot\n",
    "        plt.sca(ax[(i - 1) * 2])\n",
    "        plt.imshow(image, cmap = \"twilight_r\")\n",
    "        plt.title(f\"class {label}\")\n",
    "        plt.axis('off')\n",
    "        zebra_imgs.append(image)\n",
    "    print('image size: ', image.shape)\n",
    "    \n",
    "    return horse_imgs, zebra_imgs\n",
    "horse_imgs, zebra_imgs = display_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-pointer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(zebra_imgs[0].shape)\n",
    "sns.heatmap(zebra_imgs[0][:,:,0])\n",
    "plt.axis('off');\n",
    "\n",
    "#cool right? What are we forgetting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-edinburgh",
   "metadata": {},
   "source": [
    "What is wrong with our implimentation so far? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can combine tensorflow datasets with a simple concatenation!\n",
    "ds_train = trainA.concatenate(trainB)\n",
    "ds_test  = testA.concatenate(testB)\n",
    "\n",
    "H = W = 256//2\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def normalize_img(img, label):\n",
    "    return tf.cast(img, tf.float32)/255.0, label\n",
    "\n",
    "def resize_img(img, label):\n",
    "    return tf.image.resize(img, size=[H, W]), label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE).map(resize_img, AUTOTUNE)\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE).map(resize_img, AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets extract the normalized class specific images from the tensorflow dataset\n",
    "horse_imgs, zebra_imgs = [], []\n",
    "for img, label in ds_train.as_numpy_iterator():\n",
    "    \n",
    "    if not label: #if label != 0\n",
    "        horse_imgs.append(img)\n",
    "    else:\n",
    "        zebra_imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = {\"activation\" : \"relu\", \"strides\" : 2}\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Input(shape = (H, W, 3)))\n",
    "model.add(Conv2D(input_shape = (H, W, 3), filters = 16, kernel_size = (2,2), \n",
    "                 strides = 1, activation = 'relu', padding = \"same\"))\n",
    "model.add(Conv2D(input_shape = (H, W, 3), filters = 16, kernel_size = (3,3), **act))\n",
    "model.add(Conv2D(filters = 17, kernel_size = (3,3), **act))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (2,2), **act))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (2,2), **act))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-temperature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n",
    "mc = ModelCheckpoint(DATA_DIR + \"cnn1/cp.ckpt\", monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "if not MODELS_TRAINED:\n",
    "    history = model.fit(ds_train.cache().shuffle(buffer_size = ds_train.cardinality()).batch(BATCH_SIZE).prefetch(AUTOTUNE),#.map(augment, num_parallel_calls=AUTOTUNE),\n",
    "                        validation_data=ds_test.cache().shuffle(buffer_size = ds_test.cardinality()).batch(BATCH_SIZE).prefetch(AUTOTUNE),\n",
    "                        epochs=EPOCHS, callbacks = [es]#, mc]\n",
    "        )\n",
    "    plot_loss(history)\n",
    "    \n",
    "    #save the model\n",
    "    model.save(DATA_DIR + 'cnn1')\n",
    "else:\n",
    "    #load the model\n",
    "    model = tf.keras.models.load_model(DATA_DIR + 'cnn1')\n",
    "\n",
    "    # Check its architecture\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why does this have 4 dimensions?\n",
    "model.layers[1].weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a new model with the layer we want to examine extracted.\n",
    "model_ = Model(inputs=[model.input], outputs=[model.layers[0].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zebra_img = tf.expand_dims(zebra_imgs[0], axis = 0)\n",
    "horse_img = tf.expand_dims(horse_imgs[0], axis = 0)\n",
    "\n",
    "#COMMENT EXPLAINING WHY WE DO THE TRANSPOSE\n",
    "zebra_layer0_feature_maps = model_.predict(zebra_img)[0,:,:,:]\n",
    "horse_layer0_feature_maps = model_.predict(horse_img)[0,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-dining",
   "metadata": {},
   "source": [
    "#### Visualizing feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(np_arr):\n",
    "    fig, ax = plt.subplots(1,5, figsize = (15,5))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(5):\n",
    "        plt.sca(ax[i])\n",
    "        plt.imshow(np_arr[i,:,:], cmap = 'Blues_r')\n",
    "        plt.axis('off')\n",
    "\n",
    "print()\n",
    "quick_display(zebra_layer0_feature_maps)\n",
    "quick_display(horse_layer0_feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-final",
   "metadata": {},
   "source": [
    "What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zebra_layer0_feature_maps.shape)\n",
    "print(zebra_layer0_feature_maps.transpose((2,0,1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(np_arr, title = None):\n",
    "    fig, ax = plt.subplots(1,5, figsize = (15,5))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(5):\n",
    "        plt.sca(ax[i])\n",
    "        plt.imshow(np_arr[:, :, i], cmap = 'Blues_r')\n",
    "        plt.axis('off')\n",
    "        if title and i == 2:\n",
    "            plt.title(title)\n",
    "\n",
    "quick_display(zebra_layer0_feature_maps)\n",
    "quick_display(horse_layer0_feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-tracy",
   "metadata": {},
   "source": [
    "\n",
    "$\\bullet$ <font color='blue'>what can we say about the above feature maps? What is our network detecting?</font>\n",
    "\n",
    "$\\bullet$ <font color='blue'>Does the performance of model suggest our network has learned rotational invariance? why? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-asset",
   "metadata": {},
   "source": [
    "<a id='BO1'></a>\n",
    "\n",
    "## Break Out Room 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-family",
   "metadata": {},
   "source": [
    "<a id='BO1A'></a>\n",
    "Consider these 3 sets of filters from VGG16:\n",
    "\n",
    "A             |  B         | C            \n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src='fig/q7_3a.png'>   |  <img src='fig/q7_3c.png'> | <img src='fig/q7_3b.png'>\n",
    "\n",
    "Can you arrange them in order representing their place in the network(Shallow --> Deep)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-witch",
   "metadata": {},
   "source": [
    "<a id='BO1B'></a>\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "Extract some feature maps from the network above's deeper layers. Try to interpret what you find.\n",
    "\n",
    "***To load the exercise please uncomment and run the cell below. The solutions are in the following cell. Don't open them until you've attempted the problem with your group!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chief-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load exercises/ex1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divine-guest",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load sol/bo1_sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-lightning",
   "metadata": {},
   "source": [
    "<a id='attention'></a>\n",
    "\n",
    "# Attention\n",
    "\n",
    "**Neural network attention** is analogous to cognitive attention. Attention highlights the important parts of the input data and fades out the rest.\n",
    "\n",
    "This concept is helpful in understanding the topic of saliency maps.\n",
    "\n",
    "\n",
    "## What do you notice about the way that the network was discriminating between classes?\n",
    "\n",
    "$\\bullet$ This image is from this [***paper***](https://arxiv.org/abs/1602.04938) where the task was to classify huskies and wolves.\n",
    "\n",
    "$\\bullet$ For each pair of images we have the original image at left and an image with the unimportant pixels occluded at right.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*G9Pok8mnm_lCHxda.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-ordinance",
   "metadata": {},
   "source": [
    "<a id='salience'></a>\n",
    "## Image Class specific Saliency Maps\n",
    "\n",
    "**Salience**: The quality of being particularly noticable or important; prominance. (Definition from Oxford Languages)\n",
    "\n",
    "In general we can think of saliency maps as measuring:\n",
    "\n",
    "$$\\frac{\\partial{\\text{ output (for example an output class or regression value)}}}{\\partial{\\text{ input}}}$$\n",
    "\n",
    "More formally we can query our CNN about the **spatial support** of a particular class in a given image. In a [groundbreaking paper from the Visual Geometry group of Oxford in 2014 by Simonyan et al.](https://arxiv.org/abs/1312.6034) given an image $I_0$, a class $c$, and a classification with the class score function $S_c(I)$ we can rank the pixels of the image based on their effect on the score $S_c(I_0)$.\n",
    "\n",
    "For example, given $w_c$ and $b_c$ are class specific weight and bias scores of the model, imagine that the score function were simply\n",
    "\n",
    "$$S_c(I_0) = w_c^T I + b_c$$\n",
    "\n",
    "where  then it is clear that **\"the magnitude of the elements of *$w_c$* defines the importance of the corresponding pixels of $I$ for class $c$ \".**\n",
    "\n",
    "$$ \\frac{\\partial S_c}{\\partial I} \\propto w_c^T$$\n",
    "\n",
    "CNNs in reality have a highly non-linear and complex error function and a much more complex derivative. However, using the first term of a taylor series approximation allows us to describe the error function in the neighborhood of an image $I_0$ simply by writing:\n",
    "\n",
    "$$ S_c (I) \\approx w_c^T I_0 + b$$\n",
    "\n",
    "\n",
    "$$w_c = \\left.\\frac{\\partial S_c}{\\partial I}\\right |_{I_0}$$\n",
    "\n",
    "Notice that $w_c$ now is defined as the derivative of $S_c$ with respect to the image $I$ evaluated at the point $I_0$.\n",
    "\n",
    "Steps to compute saliency map $ \\mathcal{M} \\in \\textrm{IR}^{nxm}$ for input image I with height $n$ and width $m$:\n",
    "1. We find the derivative $w_c$ by backpropagation, we rearrange the vector. \n",
    "\n",
    "2. We then take the absolute value, so the map at pixel at row i and column j can be computed as $M_{ij} = \\left|w_{h(i,j)}\\right|$ where h(i,j) is the index.\n",
    "\n",
    "3. For a color image (3 channels) a common practice is to take the maximum across the channels for each pixel ie $M_{ij} = Max_c\\left|w_{(i,j,c)}\\right|$\n",
    " \n",
    "Some further things to note:\n",
    "\n",
    "$\\bullet$ In normal backpropagation we take the derivative of the error with respect to the weights and use that gradient to adjust the weights. Here instead we take the derivative with respect to the input image.\n",
    "\n",
    "$$\\text{Backpropagation: } \\frac{\\partial{S_c}}{\\partial w_{jk}^{l}} \\text{ vs Vanilla Saliency Map: }\\frac{\\partial S_c}{\\partial I}$$\n",
    "\n",
    "\n",
    "$\\bullet$ ***An interpretation of computing the image-specific class saliency using the class score derivative is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most.***\n",
    "\n",
    "$\\bullet$ The magnitudes of these derivatives can indicate the location of the class and saliency maps have applications in **semantic segmentation**.\n",
    "\n",
    "<img src='https://analyticsindiamag.com/wp-content/uploads/2018/07/sal-1.jpg' width = 700> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-reservoir",
   "metadata": {},
   "source": [
    "<a id='gradcam'></a>\n",
    "## GradCam\n",
    "\n",
    "This explanation was adapted from this useful link: https://fairyonice.github.io/Grad-CAM-with-keras-vis.html\n",
    "\n",
    "Also, attached is the original paper on Grad Cam: https://arxiv.org/pdf/1610.02391.pdf\n",
    "\n",
    "One problem with saliency maps is that they can be very noisy. Other techniques that were developed were cam, guided backpropagation, and finally gradcam.\n",
    "\n",
    "Each feature map in the last convolution layer can be conceptualized as describing a classification feature. Let the final feature map be represented by $A^k \\in \\mathrm{IR}^{n x m}$, then\n",
    "\n",
    "$$y = f(A^1, ..., A^{64})$$\n",
    "\n",
    "Grad Cam assumes that this final feature map regionally reveals how the model discriminates.\n",
    "\n",
    "For an individual class, such as a cathedral, certain features (like a steeple or stained glass windows) might identify that class more than another class like Golden Retriever. So instead of taking a simple linear combination of the final feature maps we take a weighted combination\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Grad-Cam}} = \\sum_{k} a_k^c A^k$$\n",
    "\n",
    "The gradient of a specific class score with respect to the $k^\\text{th}$ final feature map measures \"the linear effect of the $(i,j)^\\text{th}$ pixel on the $c^\\text{th}$ class score\".\n",
    "\n",
    "Grad-Cam assumes an average gradient score for the weights of the feature map:\n",
    "\n",
    "$$a_k^c = \\frac{1}{uv}\\sum_{i = 1}^ n\\sum_{j=1}^m \\frac{\\partial S_c}{\\partial A_{(i,j)}^k}$$\n",
    "\n",
    "Finally to calculate the Grad-Cam score we pass the weighted linear combination of feature maps through a relu because we only care about positive class correlation, not negative. For example want to know which parts of the image positively contributed to the score of zebra (is it looking at the stripes?) vs areas of the image which negatively impacted this score.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Grad-Cam}} = \\text{ReLU} \\left(\\sum_{k} a_k^c A^k\\right) \\in \\mathcal{R}^{m x n}$$\n",
    "\n",
    "$n$ and $m$ represent the height and width of the final feature maps. Finally, we upsample the final feature maps to the size of the input image!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = {\"activation\" : \"relu\", \"strides\" : 2}\n",
    "\n",
    "#lets redefine our model with 2 output nodes instead of one so we can investigate the saliency maps of both classes\n",
    "\n",
    "if not MODELS_TRAINED:\n",
    "    model = Sequential([\n",
    "        Input(shape = (H, W, 3)),\n",
    "        Conv2D(filters = 16, kernel_size = (2,2), strides = 1, activation = 'relu', padding = \"same\"),\n",
    "        Conv2D(filters = 16, kernel_size = (3,3), **act),\n",
    "        Dropout(0.005), #dropout between convolution layers acts as bernoulli noise, \n",
    "                      # which is not like standard fnn dropout\n",
    "        Conv2D(filters = 17, kernel_size = (3,3), **act),\n",
    "        Conv2D(filters = 22, kernel_size = (2,2), **act),\n",
    "        Conv2D(filters = 44, kernel_size = (2,2), **act),\n",
    "        Flatten(),\n",
    "        Dense(25),\n",
    "        Dropout(0.05),\n",
    "        Dense(10),\n",
    "        Dropout(0.05),\n",
    "        Dense(2, activation = 'softmax')])\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 25\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    history = model.fit(ds_train.cache().shuffle(buffer_size = ds_train.cardinality(), seed = 109).batch(BATCH_SIZE).prefetch(AUTOTUNE),#.map(augment, num_parallel_calls=AUTOTUNE),\n",
    "                    validation_data=ds_test.cache().shuffle(buffer_size = ds_test.cardinality(), seed = 109).batch(BATCH_SIZE).prefetch(AUTOTUNE),\n",
    "                    callbacks=[es], epochs=EPOCHS,\n",
    "    )\n",
    "    plot_loss(history)\n",
    "    \n",
    "    #save the model\n",
    "    model.save(DATA_DIR + 'cnn2')\n",
    "else:\n",
    "    #load the model\n",
    "    model = tf.keras.models.load_model(DATA_DIR + 'cnn2')\n",
    "\n",
    "# Check the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-embassy",
   "metadata": {},
   "source": [
    "<a id='BO2'></a>\n",
    "\n",
    "## Break Out Room 2\n",
    "\n",
    "Here you will try to implement a function that creates saliency maps for your desired output.\n",
    "\n",
    "1. Complete the prepare image and model modifier helper functions\n",
    "\n",
    "2. Complete the saliency graphs function\n",
    "\n",
    "3. View the saliency maps\n",
    "\n",
    "4. Discuss what these saliency maps tell us about how the model is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function here to replace \"Binary Cross Entropy\" \n",
    "# of the final dense layer with \"linear\" as we want the class scores, not the class\n",
    "def prepare_image(img):\n",
    "    \n",
    "    #add a fourth dimension to your image to enable it to be passed to model.predict:\n",
    "    \n",
    "    img_expanded_dims = ...\n",
    "    return img_expanded_dims\n",
    "\n",
    "def model_modifier(m):\n",
    "    m.layers[...].activation = ...\n",
    "\n",
    "def saliency_graphs(model, img, positive_gradients = False):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3, figsize = (14,3))\n",
    "    \n",
    "    # Create Saliency object. \n",
    "    saliency = Saliency(model, model_modifier)\n",
    "\n",
    "    # input image that is pre-processed\n",
    "    input_image =  prepare_image(...) \n",
    "\n",
    "    # predict on the input image\n",
    "    y_pred = model.predict(...)\n",
    "    n_classes = 2\n",
    "    ax[0].imshow(img[0])\n",
    "    \n",
    "    cmap_dict = {0 : \"Reds\", 1 : \"Greens\"}\n",
    "    \n",
    "    for i in range(2):\n",
    "        \n",
    "        # Define loss function for the class label.\n",
    "        # The 'output' variable refer to the output of the model. \n",
    "        # This will be different for softmax activations and regression!\n",
    "        \n",
    "        loss = lambda output: tf.keras.backend.mean(...)\n",
    "\n",
    "        # Generate saliency map with smoothing. Smoothing reduces noise in the Saliency map\n",
    "        # smooth_samples is the number of calculating gradients iterations\n",
    "        \n",
    "        saliency_map = saliency(loss, ..., smooth_samples=20)\n",
    "        \n",
    "        #to only see positive gradients:\n",
    "        if positive_gradients:\n",
    "            locs = saliency_map > 0\n",
    "            \n",
    "        #normalize\n",
    "        saliency_map = normalize(saliency_map)\n",
    "        \n",
    "        if positive_gradients:\n",
    "            saliency_map[locs] = 0\n",
    "        \n",
    "        ax[i+1].imshow(saliency_map[0,...], cmap = cmap_dict[i])\n",
    "        if i == 0:\n",
    "            ax[i+1].set_title(\"Horse saliency map\")\n",
    "        else:\n",
    "            ax[i+1].set_title(\"Zebra saliency map\")\n",
    "    #show original image\n",
    "    ax[0].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opponent-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol/bo2_sol.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#run this after you have repaired the function above or loaded and executed the solution cell.\n",
    "for i in range(3):\n",
    "    saliency_graphs(model, zebra_imgs[i].reshape(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this after you have repaired the function above or loaded and executed the solution cell.\n",
    "for i in range(3):\n",
    "    saliency_graphs(model, horse_imgs[i].reshape(128,128,3))\n",
    "\n",
    "#re-enable warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1):\n",
    "#    saliency_graphs(model, horse_imgs[i].reshape(128,128,3), positive_gradients = False)\n",
    "#    saliency_graphs(model, horse_imgs[i].reshape(128,128,3), positive_gradients = True)\n",
    "#    saliency_graphs(model, zebra_imgs[i].reshape(128,128,3), positive_gradients = False)\n",
    "#    saliency_graphs(model, zebra_imgs[i].reshape(128,128,3), positive_gradients = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-cradle",
   "metadata": {},
   "source": [
    "<a id='sota'></a>\n",
    "# Some State of the Art Architectures and associated papers\n",
    "\n",
    "This is a very <a href='https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d'> nice article</a> that walks through 10 modern CNN  architectures.\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*n16lj3lSkz2miMc_5cvkrA.jpeg'>\n",
    "\n",
    "Some important CNN models architecture papers:\n",
    "\n",
    "LeNet (1998) \n",
    "    <a href='http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf'>\n",
    "    Gradient-Based Learning Applied to Document Recognition\n",
    "    </a>\n",
    "    \n",
    "AlexNet (2012) <a href='https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf'>\n",
    "ImageNet Classification with Deep Convolutional Neural Networks</a>\n",
    "\n",
    "VGG (2014) <a href='https://arxiv.org/abs/1409.1556'> Going Deeper with Convolutions</a>\n",
    "\n",
    "Inception (2015) <a href='https://arxiv.org/abs/1409.4842'> Going Deeper with Convolutions</a>\n",
    "\n",
    "ResNet-50 (2015) <a href ='https://arxiv.org/abs/1512.03385'> Deep Residual Learning for Image Recognition </a>\n",
    "\n",
    "MobileNet (2017) <a href='https://arxiv.org/abs/1704.04861'> MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-expert",
   "metadata": {},
   "source": [
    "<a id='deepdream'></a>\n",
    "# Advanced visualization tools and techniques: Deep Dream and Style Transfer  from Google\n",
    "\n",
    "Google DeepDream (created by Alexander Mordvintsev, a google data scientist and engineer) creates images that try to maximally exite a given output nueron. For example certain nuerons in the deeper layers of the network might be responsible for detecting faces or cathedrals.\n",
    "\n",
    "Its visualizations were originally used to help understand the emergent structure of the convolutional neural network.\n",
    "\n",
    "\"Initially it was invented to help scientists and engineers to see what a deep neural network is seeing when it is looking in a given image. Later the algorithm has become a new form of psychedelic and abstract art.\" To see more visit their <a href=\"https://deepdreamgenerator.com/\"> website </a> or view the advanced section\n",
    "\n",
    "<img src='http://i.imgur.com/e5cy8HS.gif' width = 500>\n",
    "\n",
    "<img src='https://b2h3x3f6.stackpathcdn.com/assets/landing/img/blend/horizontal/dd.jpg' width = 500>\n",
    "\n",
    "<img src='https://b2h3x3f6.stackpathcdn.com/assets/landing/img/blend/horizontal/ds_1.jpg' width = 500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-uniform",
   "metadata": {},
   "source": [
    "<a id='tl'></a>\n",
    "# Bonus Material: Gradient Tap\n",
    "\n",
    "`tf.GradientTape` allows us to track calculate gradients with respect to variables of our choice! It is part of the tensorflow automatic differentiation API.\n",
    "\n",
    "The tape will automatically \"watch\" trainable tf.Variables. Otherwise we can use `tape.watch`\n",
    "\n",
    "Here is an additional tutorial: https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22\n",
    "\n",
    "**From the <a href='https://www.tensorflow.org/guide/autodiff'> documentation:** </a>\n",
    "\"TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variables`. TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impliment saliency map with gradient tape\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we generate a saliency map using gradient tape.\n",
    "# first we get a modified model copy:\n",
    "model_ = Model(inputs=[model.input], outputs=[model.layers[-1].output])\n",
    "\n",
    "#specifically we modify the last layer to have a linear activation like before\n",
    "model_.output.activation = tf.keras.activations.linear\n",
    "\n",
    "#now select the input image\n",
    "img_ = zebra_imgs[3]\n",
    "\n",
    "image = tf.expand_dims(img_, axis = 0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    #we tell the tape to watch the four dimensional input image tensor.\n",
    "    tape.watch(image)\n",
    "    \n",
    "    pred = model_(image, training=False) #training = false will prevent our model from using dropout\n",
    "    zebra_prediction = pred[0, 1]\n",
    "    loss = zebra_prediction\n",
    "\n",
    "#calculate the gradient of the loss with respect to the input image\n",
    "dloss_dimage = tape.gradient(loss, image)\n",
    "print(dloss_dimage.shape)\n",
    "\n",
    "def process_gradient(grads_):\n",
    "    \"\"\"\n",
    "    This helper function takes the absolute gradients, finds the maximum gradient across the tree channels,\n",
    "    and then normalizes the result. This is specific to the method of calculating a saliency map.\n",
    "    \"\"\"\n",
    "    abs_grads = tf.math.abs(grads_)[0,...]\n",
    "\n",
    "    grads_max = np.max(abs_grads, axis=2)\n",
    "\n",
    "    norm_grads_2d = normalize(grads_max)\n",
    "    return norm_grads_2d\n",
    "\n",
    "#lets plot the saliency map!\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16,5))\n",
    "\n",
    "ax[0].imshow(process_gradient(dloss_dimage), cmap=\"Greens\", alpha = 1)\n",
    "ax[0].set_title(\"saliency map\")\n",
    "ax[1].imshow(img_)\n",
    "ax[1].set_title(\"input image\")\n",
    "[ax.axis('off') for ax in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tf.GradientTape())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
